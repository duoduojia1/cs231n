{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aaae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# # assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "# FOLDERNAME = None\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # Now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# # This downloads the CIFAR-10 dataset to your Drive\n",
    "# # if it doesn't already exist.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# !bash get_datasets.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3cfa1",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55a72bf",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cs231n.data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcs231n\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_CIFAR10\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n.data_utils'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99594ec",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713b7f4",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6168f67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cs231n.classifiers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# First implement the naive softmax loss function with nested loops.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Open the file cs231n/classifiers/softmax.py and implement the\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# softmax_loss_naive function.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcs231n\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoftmax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax_loss_naive\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate a random softmax weight matrix and use it to compute the loss.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n.classifiers'"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bc604",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bef674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -589.918529 analytic: -589.918561, relative error: 2.762761e-08\n",
      "numerical: -1029.650147 analytic: -1029.650166, relative error: 9.050950e-09\n",
      "numerical: -81.519179 analytic: -81.519211, relative error: 1.950081e-07\n",
      "numerical: 2.067851 analytic: 2.067802, relative error: 1.177407e-05\n",
      "numerical: -674.587053 analytic: -674.587067, relative error: 1.059119e-08\n",
      "numerical: 431.332962 analytic: 431.332900, relative error: 7.219993e-08\n",
      "numerical: -2899.067134 analytic: -2899.067122, relative error: 2.052728e-09\n",
      "numerical: -967.127388 analytic: -967.127491, relative error: 5.335446e-08\n",
      "numerical: -524.594146 analytic: -524.594174, relative error: 2.605252e-08\n",
      "numerical: 1088.959560 analytic: 1088.959483, relative error: 3.505381e-08\n",
      "numerical: 590.991119 analytic: 590.986256, relative error: 4.114361e-06\n",
      "numerical: -254.746356 analytic: -254.748254, relative error: 3.726722e-06\n",
      "numerical: 108.466681 analytic: 108.465620, relative error: 4.891579e-06\n",
      "numerical: -437.828842 analytic: -437.839209, relative error: 1.183933e-05\n",
      "numerical: -145.094085 analytic: -145.094331, relative error: 8.472595e-07\n",
      "numerical: -255.015848 analytic: -255.007260, relative error: 1.683846e-05\n",
      "numerical: 1778.192083 analytic: 1778.196499, relative error: 1.241703e-06\n",
      "numerical: 1742.695848 analytic: 1742.693273, relative error: 7.389888e-07\n",
      "numerical: 743.035859 analytic: 743.028219, relative error: 5.141401e-06\n",
      "numerical: 1072.805248 analytic: 1072.806213, relative error: 4.495339e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3994f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 1.170770e+03 computed in 0.150940s\n",
      "vectorized loss: 2.341541e+00 computed in 0.008005s\n",
      "Loss difference: 1168.428850\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd171311",
   "metadata": {
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 469.423600, accuracy 0.109000\n",
      "iteration 1 / 1500: loss 463.525161, accuracy 0.102000\n",
      "iteration 2 / 1500: loss 457.960625, accuracy 0.122000\n",
      "iteration 3 / 1500: loss 452.370459, accuracy 0.120000\n",
      "iteration 4 / 1500: loss 446.961789, accuracy 0.110000\n",
      "iteration 5 / 1500: loss 441.434261, accuracy 0.136000\n",
      "iteration 6 / 1500: loss 436.349538, accuracy 0.114000\n",
      "iteration 7 / 1500: loss 430.879048, accuracy 0.108000\n",
      "iteration 8 / 1500: loss 425.624107, accuracy 0.133000\n",
      "iteration 9 / 1500: loss 420.692893, accuracy 0.114000\n",
      "iteration 10 / 1500: loss 415.455512, accuracy 0.126000\n",
      "iteration 11 / 1500: loss 410.571540, accuracy 0.123000\n",
      "iteration 12 / 1500: loss 405.529621, accuracy 0.110000\n",
      "iteration 13 / 1500: loss 400.774874, accuracy 0.136000\n",
      "iteration 14 / 1500: loss 395.948880, accuracy 0.104000\n",
      "iteration 15 / 1500: loss 391.142756, accuracy 0.123000\n",
      "iteration 16 / 1500: loss 386.396129, accuracy 0.136000\n",
      "iteration 17 / 1500: loss 381.699460, accuracy 0.132000\n",
      "iteration 18 / 1500: loss 377.260785, accuracy 0.131000\n",
      "iteration 19 / 1500: loss 372.514806, accuracy 0.131000\n",
      "iteration 20 / 1500: loss 368.145264, accuracy 0.123000\n",
      "iteration 21 / 1500: loss 363.798257, accuracy 0.124000\n",
      "iteration 22 / 1500: loss 359.333446, accuracy 0.123000\n",
      "iteration 23 / 1500: loss 355.036268, accuracy 0.116000\n",
      "iteration 24 / 1500: loss 350.709226, accuracy 0.133000\n",
      "iteration 25 / 1500: loss 346.676740, accuracy 0.110000\n",
      "iteration 26 / 1500: loss 342.511634, accuracy 0.135000\n",
      "iteration 27 / 1500: loss 338.254330, accuracy 0.135000\n",
      "iteration 28 / 1500: loss 334.240326, accuracy 0.135000\n",
      "iteration 29 / 1500: loss 330.121174, accuracy 0.139000\n",
      "iteration 30 / 1500: loss 326.380793, accuracy 0.116000\n",
      "iteration 31 / 1500: loss 322.386989, accuracy 0.133000\n",
      "iteration 32 / 1500: loss 318.571523, accuracy 0.129000\n",
      "iteration 33 / 1500: loss 314.714069, accuracy 0.129000\n",
      "iteration 34 / 1500: loss 310.845052, accuracy 0.146000\n",
      "iteration 35 / 1500: loss 307.247898, accuracy 0.132000\n",
      "iteration 36 / 1500: loss 303.504105, accuracy 0.128000\n",
      "iteration 37 / 1500: loss 299.875358, accuracy 0.156000\n",
      "iteration 38 / 1500: loss 296.286432, accuracy 0.149000\n",
      "iteration 39 / 1500: loss 292.802253, accuracy 0.143000\n",
      "iteration 40 / 1500: loss 289.315837, accuracy 0.123000\n",
      "iteration 41 / 1500: loss 285.816263, accuracy 0.145000\n",
      "iteration 42 / 1500: loss 282.307863, accuracy 0.149000\n",
      "iteration 43 / 1500: loss 278.942246, accuracy 0.179000\n",
      "iteration 44 / 1500: loss 275.797088, accuracy 0.135000\n",
      "iteration 45 / 1500: loss 272.402082, accuracy 0.133000\n",
      "iteration 46 / 1500: loss 269.173309, accuracy 0.150000\n",
      "iteration 47 / 1500: loss 265.964370, accuracy 0.155000\n",
      "iteration 48 / 1500: loss 262.736916, accuracy 0.138000\n",
      "iteration 49 / 1500: loss 259.628309, accuracy 0.142000\n",
      "iteration 50 / 1500: loss 256.601958, accuracy 0.151000\n",
      "iteration 51 / 1500: loss 253.479641, accuracy 0.151000\n",
      "iteration 52 / 1500: loss 250.315498, accuracy 0.152000\n",
      "iteration 53 / 1500: loss 247.413835, accuracy 0.153000\n",
      "iteration 54 / 1500: loss 244.559633, accuracy 0.148000\n",
      "iteration 55 / 1500: loss 241.622444, accuracy 0.152000\n",
      "iteration 56 / 1500: loss 238.651190, accuracy 0.170000\n",
      "iteration 57 / 1500: loss 235.809243, accuracy 0.170000\n",
      "iteration 58 / 1500: loss 232.984119, accuracy 0.155000\n",
      "iteration 59 / 1500: loss 230.218887, accuracy 0.154000\n",
      "iteration 60 / 1500: loss 227.478077, accuracy 0.173000\n",
      "iteration 61 / 1500: loss 224.804604, accuracy 0.175000\n",
      "iteration 62 / 1500: loss 222.174176, accuracy 0.162000\n",
      "iteration 63 / 1500: loss 219.455001, accuracy 0.155000\n",
      "iteration 64 / 1500: loss 216.912591, accuracy 0.162000\n",
      "iteration 65 / 1500: loss 214.275025, accuracy 0.174000\n",
      "iteration 66 / 1500: loss 211.746103, accuracy 0.164000\n",
      "iteration 67 / 1500: loss 209.075071, accuracy 0.168000\n",
      "iteration 68 / 1500: loss 206.686902, accuracy 0.178000\n",
      "iteration 69 / 1500: loss 204.254095, accuracy 0.175000\n",
      "iteration 70 / 1500: loss 201.836062, accuracy 0.165000\n",
      "iteration 71 / 1500: loss 199.419690, accuracy 0.163000\n",
      "iteration 72 / 1500: loss 197.062015, accuracy 0.167000\n",
      "iteration 73 / 1500: loss 194.654887, accuracy 0.176000\n",
      "iteration 74 / 1500: loss 192.411419, accuracy 0.163000\n",
      "iteration 75 / 1500: loss 190.116406, accuracy 0.178000\n",
      "iteration 76 / 1500: loss 187.754565, accuracy 0.189000\n",
      "iteration 77 / 1500: loss 185.602436, accuracy 0.168000\n",
      "iteration 78 / 1500: loss 183.379183, accuracy 0.153000\n",
      "iteration 79 / 1500: loss 181.251130, accuracy 0.178000\n",
      "iteration 80 / 1500: loss 179.004203, accuracy 0.177000\n",
      "iteration 81 / 1500: loss 177.037143, accuracy 0.161000\n",
      "iteration 82 / 1500: loss 174.809604, accuracy 0.182000\n",
      "iteration 83 / 1500: loss 172.759946, accuracy 0.182000\n",
      "iteration 84 / 1500: loss 170.699919, accuracy 0.176000\n",
      "iteration 85 / 1500: loss 168.712091, accuracy 0.185000\n",
      "iteration 86 / 1500: loss 166.713092, accuracy 0.170000\n",
      "iteration 87 / 1500: loss 164.582264, accuracy 0.199000\n",
      "iteration 88 / 1500: loss 162.688889, accuracy 0.179000\n",
      "iteration 89 / 1500: loss 160.697658, accuracy 0.178000\n",
      "iteration 90 / 1500: loss 158.884685, accuracy 0.187000\n",
      "iteration 91 / 1500: loss 157.033897, accuracy 0.183000\n",
      "iteration 92 / 1500: loss 155.106845, accuracy 0.196000\n",
      "iteration 93 / 1500: loss 153.356156, accuracy 0.185000\n",
      "iteration 94 / 1500: loss 151.555374, accuracy 0.177000\n",
      "iteration 95 / 1500: loss 149.709474, accuracy 0.187000\n",
      "iteration 96 / 1500: loss 147.908894, accuracy 0.180000\n",
      "iteration 97 / 1500: loss 146.122063, accuracy 0.189000\n",
      "iteration 98 / 1500: loss 144.488798, accuracy 0.179000\n",
      "iteration 99 / 1500: loss 142.665748, accuracy 0.185000\n",
      "iteration 100 / 1500: loss 141.061709, accuracy 0.186000\n",
      "iteration 101 / 1500: loss 139.400799, accuracy 0.170000\n",
      "iteration 102 / 1500: loss 137.841621, accuracy 0.160000\n",
      "iteration 103 / 1500: loss 136.054117, accuracy 0.199000\n",
      "iteration 104 / 1500: loss 134.421754, accuracy 0.201000\n",
      "iteration 105 / 1500: loss 132.862372, accuracy 0.206000\n",
      "iteration 106 / 1500: loss 131.256786, accuracy 0.205000\n",
      "iteration 107 / 1500: loss 129.749163, accuracy 0.205000\n",
      "iteration 108 / 1500: loss 128.282025, accuracy 0.180000\n",
      "iteration 109 / 1500: loss 126.630585, accuracy 0.200000\n",
      "iteration 110 / 1500: loss 125.266443, accuracy 0.170000\n",
      "iteration 111 / 1500: loss 123.676795, accuracy 0.194000\n",
      "iteration 112 / 1500: loss 122.260912, accuracy 0.193000\n",
      "iteration 113 / 1500: loss 120.862975, accuracy 0.195000\n",
      "iteration 114 / 1500: loss 119.391159, accuracy 0.203000\n",
      "iteration 115 / 1500: loss 117.943545, accuracy 0.209000\n",
      "iteration 116 / 1500: loss 116.559637, accuracy 0.195000\n",
      "iteration 117 / 1500: loss 115.091817, accuracy 0.225000\n",
      "iteration 118 / 1500: loss 113.863938, accuracy 0.228000\n",
      "iteration 119 / 1500: loss 112.535022, accuracy 0.201000\n",
      "iteration 120 / 1500: loss 111.179933, accuracy 0.224000\n",
      "iteration 121 / 1500: loss 109.867722, accuracy 0.236000\n",
      "iteration 122 / 1500: loss 108.556935, accuracy 0.216000\n",
      "iteration 123 / 1500: loss 107.324358, accuracy 0.208000\n",
      "iteration 124 / 1500: loss 106.090511, accuracy 0.211000\n",
      "iteration 125 / 1500: loss 104.753471, accuracy 0.214000\n",
      "iteration 126 / 1500: loss 103.535379, accuracy 0.210000\n",
      "iteration 127 / 1500: loss 102.356161, accuracy 0.195000\n",
      "iteration 128 / 1500: loss 101.169920, accuracy 0.188000\n",
      "iteration 129 / 1500: loss 99.876425, accuracy 0.224000\n",
      "iteration 130 / 1500: loss 98.728599, accuracy 0.227000\n",
      "iteration 131 / 1500: loss 97.617258, accuracy 0.210000\n",
      "iteration 132 / 1500: loss 96.515481, accuracy 0.199000\n",
      "iteration 133 / 1500: loss 95.342663, accuracy 0.204000\n",
      "iteration 134 / 1500: loss 94.200487, accuracy 0.201000\n",
      "iteration 135 / 1500: loss 93.067236, accuracy 0.236000\n",
      "iteration 136 / 1500: loss 91.933029, accuracy 0.232000\n",
      "iteration 137 / 1500: loss 90.902531, accuracy 0.208000\n",
      "iteration 138 / 1500: loss 89.865639, accuracy 0.214000\n",
      "iteration 139 / 1500: loss 88.764778, accuracy 0.211000\n",
      "iteration 140 / 1500: loss 87.764339, accuracy 0.215000\n",
      "iteration 141 / 1500: loss 86.733215, accuracy 0.209000\n",
      "iteration 142 / 1500: loss 85.683783, accuracy 0.213000\n",
      "iteration 143 / 1500: loss 84.748492, accuracy 0.188000\n",
      "iteration 144 / 1500: loss 83.696883, accuracy 0.221000\n",
      "iteration 145 / 1500: loss 82.699641, accuracy 0.220000\n",
      "iteration 146 / 1500: loss 81.736896, accuracy 0.240000\n",
      "iteration 147 / 1500: loss 80.815309, accuracy 0.216000\n",
      "iteration 148 / 1500: loss 79.838330, accuracy 0.225000\n",
      "iteration 149 / 1500: loss 78.849352, accuracy 0.244000\n",
      "iteration 150 / 1500: loss 78.020384, accuracy 0.223000\n",
      "iteration 151 / 1500: loss 77.144851, accuracy 0.197000\n",
      "iteration 152 / 1500: loss 76.216820, accuracy 0.215000\n",
      "iteration 153 / 1500: loss 75.315762, accuracy 0.210000\n",
      "iteration 154 / 1500: loss 74.436009, accuracy 0.238000\n",
      "iteration 155 / 1500: loss 73.525407, accuracy 0.231000\n",
      "iteration 156 / 1500: loss 72.721335, accuracy 0.245000\n",
      "iteration 157 / 1500: loss 71.896865, accuracy 0.208000\n",
      "iteration 158 / 1500: loss 71.024427, accuracy 0.220000\n",
      "iteration 159 / 1500: loss 70.185661, accuracy 0.231000\n",
      "iteration 160 / 1500: loss 69.322427, accuracy 0.229000\n",
      "iteration 161 / 1500: loss 68.554378, accuracy 0.226000\n",
      "iteration 162 / 1500: loss 67.732557, accuracy 0.231000\n",
      "iteration 163 / 1500: loss 66.913614, accuracy 0.263000\n",
      "iteration 164 / 1500: loss 66.192023, accuracy 0.227000\n",
      "iteration 165 / 1500: loss 65.423172, accuracy 0.221000\n",
      "iteration 166 / 1500: loss 64.621266, accuracy 0.235000\n",
      "iteration 167 / 1500: loss 63.946661, accuracy 0.231000\n",
      "iteration 168 / 1500: loss 63.141196, accuracy 0.232000\n",
      "iteration 169 / 1500: loss 62.385120, accuracy 0.248000\n",
      "iteration 170 / 1500: loss 61.705015, accuracy 0.231000\n",
      "iteration 171 / 1500: loss 61.007747, accuracy 0.238000\n",
      "iteration 172 / 1500: loss 60.301508, accuracy 0.229000\n",
      "iteration 173 / 1500: loss 59.552546, accuracy 0.243000\n",
      "iteration 174 / 1500: loss 58.883187, accuracy 0.242000\n",
      "iteration 175 / 1500: loss 58.176721, accuracy 0.270000\n",
      "iteration 176 / 1500: loss 57.502647, accuracy 0.250000\n",
      "iteration 177 / 1500: loss 56.843778, accuracy 0.224000\n",
      "iteration 178 / 1500: loss 56.265721, accuracy 0.225000\n",
      "iteration 179 / 1500: loss 55.562035, accuracy 0.234000\n",
      "iteration 180 / 1500: loss 54.879767, accuracy 0.248000\n",
      "iteration 181 / 1500: loss 54.273956, accuracy 0.254000\n",
      "iteration 182 / 1500: loss 53.639771, accuracy 0.251000\n",
      "iteration 183 / 1500: loss 53.028869, accuracy 0.251000\n",
      "iteration 184 / 1500: loss 52.378475, accuracy 0.257000\n",
      "iteration 185 / 1500: loss 51.826574, accuracy 0.231000\n",
      "iteration 186 / 1500: loss 51.192929, accuracy 0.250000\n",
      "iteration 187 / 1500: loss 50.631969, accuracy 0.223000\n",
      "iteration 188 / 1500: loss 50.040770, accuracy 0.250000\n",
      "iteration 189 / 1500: loss 49.459632, accuracy 0.239000\n",
      "iteration 190 / 1500: loss 48.923734, accuracy 0.237000\n",
      "iteration 191 / 1500: loss 48.289805, accuracy 0.269000\n",
      "iteration 192 / 1500: loss 47.730703, accuracy 0.277000\n",
      "iteration 193 / 1500: loss 47.216339, accuracy 0.264000\n",
      "iteration 194 / 1500: loss 46.742145, accuracy 0.239000\n",
      "iteration 195 / 1500: loss 46.139801, accuracy 0.261000\n",
      "iteration 196 / 1500: loss 45.648948, accuracy 0.237000\n",
      "iteration 197 / 1500: loss 45.088995, accuracy 0.251000\n",
      "iteration 198 / 1500: loss 44.603665, accuracy 0.251000\n",
      "iteration 199 / 1500: loss 44.081818, accuracy 0.233000\n",
      "iteration 200 / 1500: loss 43.550747, accuracy 0.250000\n",
      "iteration 201 / 1500: loss 43.073117, accuracy 0.258000\n",
      "iteration 202 / 1500: loss 42.572435, accuracy 0.261000\n",
      "iteration 203 / 1500: loss 42.066734, accuracy 0.265000\n",
      "iteration 204 / 1500: loss 41.593403, accuracy 0.261000\n",
      "iteration 205 / 1500: loss 41.164937, accuracy 0.255000\n",
      "iteration 206 / 1500: loss 40.634796, accuracy 0.272000\n",
      "iteration 207 / 1500: loss 40.198157, accuracy 0.259000\n",
      "iteration 208 / 1500: loss 39.721724, accuracy 0.266000\n",
      "iteration 209 / 1500: loss 39.338315, accuracy 0.246000\n",
      "iteration 210 / 1500: loss 38.840116, accuracy 0.253000\n",
      "iteration 211 / 1500: loss 38.407479, accuracy 0.281000\n",
      "iteration 212 / 1500: loss 37.915223, accuracy 0.278000\n",
      "iteration 213 / 1500: loss 37.515758, accuracy 0.271000\n",
      "iteration 214 / 1500: loss 37.101196, accuracy 0.264000\n",
      "iteration 215 / 1500: loss 36.666962, accuracy 0.256000\n",
      "iteration 216 / 1500: loss 36.297792, accuracy 0.247000\n",
      "iteration 217 / 1500: loss 35.863678, accuracy 0.272000\n",
      "iteration 218 / 1500: loss 35.427792, accuracy 0.276000\n",
      "iteration 219 / 1500: loss 34.987753, accuracy 0.276000\n",
      "iteration 220 / 1500: loss 34.620664, accuracy 0.277000\n",
      "iteration 221 / 1500: loss 34.294669, accuracy 0.247000\n",
      "iteration 222 / 1500: loss 33.849727, accuracy 0.286000\n",
      "iteration 223 / 1500: loss 33.512464, accuracy 0.262000\n",
      "iteration 224 / 1500: loss 33.060562, accuracy 0.299000\n",
      "iteration 225 / 1500: loss 32.716023, accuracy 0.278000\n",
      "iteration 226 / 1500: loss 32.395454, accuracy 0.259000\n",
      "iteration 227 / 1500: loss 32.017478, accuracy 0.251000\n",
      "iteration 228 / 1500: loss 31.694126, accuracy 0.259000\n",
      "iteration 229 / 1500: loss 31.271025, accuracy 0.279000\n",
      "iteration 230 / 1500: loss 30.977473, accuracy 0.268000\n",
      "iteration 231 / 1500: loss 30.592319, accuracy 0.274000\n",
      "iteration 232 / 1500: loss 30.234707, accuracy 0.284000\n",
      "iteration 233 / 1500: loss 29.923597, accuracy 0.265000\n",
      "iteration 234 / 1500: loss 29.603761, accuracy 0.279000\n",
      "iteration 235 / 1500: loss 29.249249, accuracy 0.267000\n",
      "iteration 236 / 1500: loss 28.951879, accuracy 0.246000\n",
      "iteration 237 / 1500: loss 28.605923, accuracy 0.252000\n",
      "iteration 238 / 1500: loss 28.286179, accuracy 0.279000\n",
      "iteration 239 / 1500: loss 27.972793, accuracy 0.278000\n",
      "iteration 240 / 1500: loss 27.640409, accuracy 0.277000\n",
      "iteration 241 / 1500: loss 27.356355, accuracy 0.281000\n",
      "iteration 242 / 1500: loss 27.022403, accuracy 0.296000\n",
      "iteration 243 / 1500: loss 26.751578, accuracy 0.257000\n",
      "iteration 244 / 1500: loss 26.418131, accuracy 0.283000\n",
      "iteration 245 / 1500: loss 26.147414, accuracy 0.281000\n",
      "iteration 246 / 1500: loss 25.820365, accuracy 0.301000\n",
      "iteration 247 / 1500: loss 25.599965, accuracy 0.281000\n",
      "iteration 248 / 1500: loss 25.296416, accuracy 0.275000\n",
      "iteration 249 / 1500: loss 25.022018, accuracy 0.276000\n",
      "iteration 250 / 1500: loss 24.731754, accuracy 0.279000\n",
      "iteration 251 / 1500: loss 24.450276, accuracy 0.294000\n",
      "iteration 252 / 1500: loss 24.208134, accuracy 0.269000\n",
      "iteration 253 / 1500: loss 23.902779, accuracy 0.292000\n",
      "iteration 254 / 1500: loss 23.645704, accuracy 0.286000\n",
      "iteration 255 / 1500: loss 23.407667, accuracy 0.270000\n",
      "iteration 256 / 1500: loss 23.185317, accuracy 0.275000\n",
      "iteration 257 / 1500: loss 22.882760, accuracy 0.309000\n",
      "iteration 258 / 1500: loss 22.643601, accuracy 0.301000\n",
      "iteration 259 / 1500: loss 22.407639, accuracy 0.285000\n",
      "iteration 260 / 1500: loss 22.177486, accuracy 0.268000\n",
      "iteration 261 / 1500: loss 21.885105, accuracy 0.295000\n",
      "iteration 262 / 1500: loss 21.666175, accuracy 0.288000\n",
      "iteration 263 / 1500: loss 21.412840, accuracy 0.294000\n",
      "iteration 264 / 1500: loss 21.210379, accuracy 0.304000\n",
      "iteration 265 / 1500: loss 21.000833, accuracy 0.279000\n",
      "iteration 266 / 1500: loss 20.742857, accuracy 0.283000\n",
      "iteration 267 / 1500: loss 20.514812, accuracy 0.275000\n",
      "iteration 268 / 1500: loss 20.315003, accuracy 0.292000\n",
      "iteration 269 / 1500: loss 20.091328, accuracy 0.291000\n",
      "iteration 270 / 1500: loss 19.875219, accuracy 0.278000\n",
      "iteration 271 / 1500: loss 19.673230, accuracy 0.289000\n",
      "iteration 272 / 1500: loss 19.447595, accuracy 0.301000\n",
      "iteration 273 / 1500: loss 19.254845, accuracy 0.271000\n",
      "iteration 274 / 1500: loss 19.043687, accuracy 0.302000\n",
      "iteration 275 / 1500: loss 18.824444, accuracy 0.288000\n",
      "iteration 276 / 1500: loss 18.640615, accuracy 0.281000\n",
      "iteration 277 / 1500: loss 18.404143, accuracy 0.301000\n",
      "iteration 278 / 1500: loss 18.215010, accuracy 0.311000\n",
      "iteration 279 / 1500: loss 18.054742, accuracy 0.287000\n",
      "iteration 280 / 1500: loss 17.868412, accuracy 0.272000\n",
      "iteration 281 / 1500: loss 17.646418, accuracy 0.298000\n",
      "iteration 282 / 1500: loss 17.460313, accuracy 0.300000\n",
      "iteration 283 / 1500: loss 17.233526, accuracy 0.330000\n",
      "iteration 284 / 1500: loss 17.108192, accuracy 0.286000\n",
      "iteration 285 / 1500: loss 16.923891, accuracy 0.307000\n",
      "iteration 286 / 1500: loss 16.754320, accuracy 0.297000\n",
      "iteration 287 / 1500: loss 16.563448, accuracy 0.279000\n",
      "iteration 288 / 1500: loss 16.350081, accuracy 0.331000\n",
      "iteration 289 / 1500: loss 16.211266, accuracy 0.284000\n",
      "iteration 290 / 1500: loss 16.054719, accuracy 0.288000\n",
      "iteration 291 / 1500: loss 15.876455, accuracy 0.311000\n",
      "iteration 292 / 1500: loss 15.718064, accuracy 0.287000\n",
      "iteration 293 / 1500: loss 15.572200, accuracy 0.297000\n",
      "iteration 294 / 1500: loss 15.375510, accuracy 0.305000\n",
      "iteration 295 / 1500: loss 15.210242, accuracy 0.308000\n",
      "iteration 296 / 1500: loss 15.067790, accuracy 0.301000\n",
      "iteration 297 / 1500: loss 14.911376, accuracy 0.284000\n",
      "iteration 298 / 1500: loss 14.760852, accuracy 0.293000\n",
      "iteration 299 / 1500: loss 14.611661, accuracy 0.318000\n",
      "iteration 300 / 1500: loss 14.448203, accuracy 0.318000\n",
      "iteration 301 / 1500: loss 14.317659, accuracy 0.311000\n",
      "iteration 302 / 1500: loss 14.165409, accuracy 0.296000\n",
      "iteration 303 / 1500: loss 14.003723, accuracy 0.311000\n",
      "iteration 304 / 1500: loss 13.858864, accuracy 0.296000\n",
      "iteration 305 / 1500: loss 13.737759, accuracy 0.292000\n",
      "iteration 306 / 1500: loss 13.567949, accuracy 0.317000\n",
      "iteration 307 / 1500: loss 13.426926, accuracy 0.319000\n",
      "iteration 308 / 1500: loss 13.321843, accuracy 0.301000\n",
      "iteration 309 / 1500: loss 13.182463, accuracy 0.304000\n",
      "iteration 310 / 1500: loss 13.021990, accuracy 0.312000\n",
      "iteration 311 / 1500: loss 12.906128, accuracy 0.318000\n",
      "iteration 312 / 1500: loss 12.797367, accuracy 0.306000\n",
      "iteration 313 / 1500: loss 12.604254, accuracy 0.331000\n",
      "iteration 314 / 1500: loss 12.519239, accuracy 0.320000\n",
      "iteration 315 / 1500: loss 12.406909, accuracy 0.316000\n",
      "iteration 316 / 1500: loss 12.255352, accuracy 0.336000\n",
      "iteration 317 / 1500: loss 12.134326, accuracy 0.304000\n",
      "iteration 318 / 1500: loss 12.028992, accuracy 0.316000\n",
      "iteration 319 / 1500: loss 11.913047, accuracy 0.306000\n",
      "iteration 320 / 1500: loss 11.729312, accuracy 0.337000\n",
      "iteration 321 / 1500: loss 11.668898, accuracy 0.303000\n",
      "iteration 322 / 1500: loss 11.532403, accuracy 0.322000\n",
      "iteration 323 / 1500: loss 11.443891, accuracy 0.319000\n",
      "iteration 324 / 1500: loss 11.337664, accuracy 0.323000\n",
      "iteration 325 / 1500: loss 11.219201, accuracy 0.306000\n",
      "iteration 326 / 1500: loss 11.074236, accuracy 0.317000\n",
      "iteration 327 / 1500: loss 10.968396, accuracy 0.329000\n",
      "iteration 328 / 1500: loss 10.903407, accuracy 0.299000\n",
      "iteration 329 / 1500: loss 10.842937, accuracy 0.295000\n",
      "iteration 330 / 1500: loss 10.654064, accuracy 0.346000\n",
      "iteration 331 / 1500: loss 10.573608, accuracy 0.310000\n",
      "iteration 332 / 1500: loss 10.490694, accuracy 0.305000\n",
      "iteration 333 / 1500: loss 10.406526, accuracy 0.291000\n",
      "iteration 334 / 1500: loss 10.279603, accuracy 0.312000\n",
      "iteration 335 / 1500: loss 10.198387, accuracy 0.307000\n",
      "iteration 336 / 1500: loss 10.096979, accuracy 0.306000\n",
      "iteration 337 / 1500: loss 9.981464, accuracy 0.320000\n",
      "iteration 338 / 1500: loss 9.880877, accuracy 0.325000\n",
      "iteration 339 / 1500: loss 9.800166, accuracy 0.314000\n",
      "iteration 340 / 1500: loss 9.725173, accuracy 0.282000\n",
      "iteration 341 / 1500: loss 9.595076, accuracy 0.328000\n",
      "iteration 342 / 1500: loss 9.499644, accuracy 0.325000\n",
      "iteration 343 / 1500: loss 9.381339, accuracy 0.318000\n",
      "iteration 344 / 1500: loss 9.347935, accuracy 0.316000\n",
      "iteration 345 / 1500: loss 9.249405, accuracy 0.312000\n",
      "iteration 346 / 1500: loss 9.184778, accuracy 0.289000\n",
      "iteration 347 / 1500: loss 9.067602, accuracy 0.323000\n",
      "iteration 348 / 1500: loss 8.979750, accuracy 0.327000\n",
      "iteration 349 / 1500: loss 8.913897, accuracy 0.333000\n",
      "iteration 350 / 1500: loss 8.843347, accuracy 0.301000\n",
      "iteration 351 / 1500: loss 8.732612, accuracy 0.308000\n",
      "iteration 352 / 1500: loss 8.650147, accuracy 0.324000\n",
      "iteration 353 / 1500: loss 8.576441, accuracy 0.319000\n",
      "iteration 354 / 1500: loss 8.500629, accuracy 0.340000\n",
      "iteration 355 / 1500: loss 8.430902, accuracy 0.324000\n",
      "iteration 356 / 1500: loss 8.348491, accuracy 0.322000\n",
      "iteration 357 / 1500: loss 8.293541, accuracy 0.316000\n",
      "iteration 358 / 1500: loss 8.206596, accuracy 0.317000\n",
      "iteration 359 / 1500: loss 8.127326, accuracy 0.319000\n",
      "iteration 360 / 1500: loss 8.078675, accuracy 0.298000\n",
      "iteration 361 / 1500: loss 7.973372, accuracy 0.329000\n",
      "iteration 362 / 1500: loss 7.918333, accuracy 0.324000\n",
      "iteration 363 / 1500: loss 7.857001, accuracy 0.318000\n",
      "iteration 364 / 1500: loss 7.775634, accuracy 0.331000\n",
      "iteration 365 / 1500: loss 7.688351, accuracy 0.313000\n",
      "iteration 366 / 1500: loss 7.645966, accuracy 0.318000\n",
      "iteration 367 / 1500: loss 7.594115, accuracy 0.305000\n",
      "iteration 368 / 1500: loss 7.487803, accuracy 0.325000\n",
      "iteration 369 / 1500: loss 7.427208, accuracy 0.348000\n",
      "iteration 370 / 1500: loss 7.353171, accuracy 0.330000\n",
      "iteration 371 / 1500: loss 7.286779, accuracy 0.332000\n",
      "iteration 372 / 1500: loss 7.229836, accuracy 0.321000\n",
      "iteration 373 / 1500: loss 7.163512, accuracy 0.341000\n",
      "iteration 374 / 1500: loss 7.127272, accuracy 0.322000\n",
      "iteration 375 / 1500: loss 7.056338, accuracy 0.336000\n",
      "iteration 376 / 1500: loss 6.975677, accuracy 0.345000\n",
      "iteration 377 / 1500: loss 6.960819, accuracy 0.310000\n",
      "iteration 378 / 1500: loss 6.871474, accuracy 0.350000\n",
      "iteration 379 / 1500: loss 6.803469, accuracy 0.343000\n",
      "iteration 380 / 1500: loss 6.767422, accuracy 0.322000\n",
      "iteration 381 / 1500: loss 6.718223, accuracy 0.330000\n",
      "iteration 382 / 1500: loss 6.603535, accuracy 0.359000\n",
      "iteration 383 / 1500: loss 6.609256, accuracy 0.321000\n",
      "iteration 384 / 1500: loss 6.534039, accuracy 0.325000\n",
      "iteration 385 / 1500: loss 6.504441, accuracy 0.345000\n",
      "iteration 386 / 1500: loss 6.419695, accuracy 0.325000\n",
      "iteration 387 / 1500: loss 6.367423, accuracy 0.314000\n",
      "iteration 388 / 1500: loss 6.331610, accuracy 0.312000\n",
      "iteration 389 / 1500: loss 6.292762, accuracy 0.322000\n",
      "iteration 390 / 1500: loss 6.209766, accuracy 0.344000\n",
      "iteration 391 / 1500: loss 6.175832, accuracy 0.346000\n",
      "iteration 392 / 1500: loss 6.107709, accuracy 0.323000\n",
      "iteration 393 / 1500: loss 6.064069, accuracy 0.339000\n",
      "iteration 394 / 1500: loss 6.036050, accuracy 0.327000\n",
      "iteration 395 / 1500: loss 5.958435, accuracy 0.309000\n",
      "iteration 396 / 1500: loss 5.933751, accuracy 0.342000\n",
      "iteration 397 / 1500: loss 5.895035, accuracy 0.322000\n",
      "iteration 398 / 1500: loss 5.842465, accuracy 0.333000\n",
      "iteration 399 / 1500: loss 5.804729, accuracy 0.324000\n",
      "iteration 400 / 1500: loss 5.730904, accuracy 0.343000\n",
      "iteration 401 / 1500: loss 5.710961, accuracy 0.321000\n",
      "iteration 402 / 1500: loss 5.636548, accuracy 0.337000\n",
      "iteration 403 / 1500: loss 5.644929, accuracy 0.313000\n",
      "iteration 404 / 1500: loss 5.599635, accuracy 0.306000\n",
      "iteration 405 / 1500: loss 5.531210, accuracy 0.338000\n",
      "iteration 406 / 1500: loss 5.504164, accuracy 0.334000\n",
      "iteration 407 / 1500: loss 5.469114, accuracy 0.315000\n",
      "iteration 408 / 1500: loss 5.396198, accuracy 0.334000\n",
      "iteration 409 / 1500: loss 5.362357, accuracy 0.335000\n",
      "iteration 410 / 1500: loss 5.338053, accuracy 0.323000\n",
      "iteration 411 / 1500: loss 5.247479, accuracy 0.363000\n",
      "iteration 412 / 1500: loss 5.274855, accuracy 0.323000\n",
      "iteration 413 / 1500: loss 5.219693, accuracy 0.334000\n",
      "iteration 414 / 1500: loss 5.147814, accuracy 0.346000\n",
      "iteration 415 / 1500: loss 5.109938, accuracy 0.346000\n",
      "iteration 416 / 1500: loss 5.106232, accuracy 0.342000\n",
      "iteration 417 / 1500: loss 5.083627, accuracy 0.330000\n",
      "iteration 418 / 1500: loss 5.062178, accuracy 0.304000\n",
      "iteration 419 / 1500: loss 4.997972, accuracy 0.307000\n",
      "iteration 420 / 1500: loss 4.929822, accuracy 0.359000\n",
      "iteration 421 / 1500: loss 4.906408, accuracy 0.323000\n",
      "iteration 422 / 1500: loss 4.864194, accuracy 0.335000\n",
      "iteration 423 / 1500: loss 4.845508, accuracy 0.328000\n",
      "iteration 424 / 1500: loss 4.827082, accuracy 0.330000\n",
      "iteration 425 / 1500: loss 4.806783, accuracy 0.314000\n",
      "iteration 426 / 1500: loss 4.712083, accuracy 0.338000\n",
      "iteration 427 / 1500: loss 4.695524, accuracy 0.331000\n",
      "iteration 428 / 1500: loss 4.681533, accuracy 0.334000\n",
      "iteration 429 / 1500: loss 4.647856, accuracy 0.345000\n",
      "iteration 430 / 1500: loss 4.629802, accuracy 0.352000\n",
      "iteration 431 / 1500: loss 4.592313, accuracy 0.316000\n",
      "iteration 432 / 1500: loss 4.544318, accuracy 0.328000\n",
      "iteration 433 / 1500: loss 4.524831, accuracy 0.317000\n",
      "iteration 434 / 1500: loss 4.497913, accuracy 0.354000\n",
      "iteration 435 / 1500: loss 4.476439, accuracy 0.325000\n",
      "iteration 436 / 1500: loss 4.434126, accuracy 0.332000\n",
      "iteration 437 / 1500: loss 4.401296, accuracy 0.352000\n",
      "iteration 438 / 1500: loss 4.372086, accuracy 0.354000\n",
      "iteration 439 / 1500: loss 4.356799, accuracy 0.322000\n",
      "iteration 440 / 1500: loss 4.339465, accuracy 0.344000\n",
      "iteration 441 / 1500: loss 4.277621, accuracy 0.366000\n",
      "iteration 442 / 1500: loss 4.310407, accuracy 0.303000\n",
      "iteration 443 / 1500: loss 4.247764, accuracy 0.346000\n",
      "iteration 444 / 1500: loss 4.258688, accuracy 0.311000\n",
      "iteration 445 / 1500: loss 4.207816, accuracy 0.325000\n",
      "iteration 446 / 1500: loss 4.175552, accuracy 0.320000\n",
      "iteration 447 / 1500: loss 4.152806, accuracy 0.347000\n",
      "iteration 448 / 1500: loss 4.106483, accuracy 0.352000\n",
      "iteration 449 / 1500: loss 4.115523, accuracy 0.318000\n",
      "iteration 450 / 1500: loss 4.051051, accuracy 0.351000\n",
      "iteration 451 / 1500: loss 4.055167, accuracy 0.333000\n",
      "iteration 452 / 1500: loss 4.032221, accuracy 0.341000\n",
      "iteration 453 / 1500: loss 3.972624, accuracy 0.333000\n",
      "iteration 454 / 1500: loss 3.967677, accuracy 0.342000\n",
      "iteration 455 / 1500: loss 3.949496, accuracy 0.326000\n",
      "iteration 456 / 1500: loss 3.927621, accuracy 0.325000\n",
      "iteration 457 / 1500: loss 3.920384, accuracy 0.321000\n",
      "iteration 458 / 1500: loss 3.897754, accuracy 0.341000\n",
      "iteration 459 / 1500: loss 3.878822, accuracy 0.323000\n",
      "iteration 460 / 1500: loss 3.828836, accuracy 0.365000\n",
      "iteration 461 / 1500: loss 3.842288, accuracy 0.338000\n",
      "iteration 462 / 1500: loss 3.775149, accuracy 0.344000\n",
      "iteration 463 / 1500: loss 3.765332, accuracy 0.355000\n",
      "iteration 464 / 1500: loss 3.756949, accuracy 0.334000\n",
      "iteration 465 / 1500: loss 3.728448, accuracy 0.366000\n",
      "iteration 466 / 1500: loss 3.717445, accuracy 0.319000\n",
      "iteration 467 / 1500: loss 3.698558, accuracy 0.337000\n",
      "iteration 468 / 1500: loss 3.700104, accuracy 0.334000\n",
      "iteration 469 / 1500: loss 3.639326, accuracy 0.334000\n",
      "iteration 470 / 1500: loss 3.616734, accuracy 0.345000\n",
      "iteration 471 / 1500: loss 3.628068, accuracy 0.318000\n",
      "iteration 472 / 1500: loss 3.587342, accuracy 0.322000\n",
      "iteration 473 / 1500: loss 3.579926, accuracy 0.342000\n",
      "iteration 474 / 1500: loss 3.569238, accuracy 0.338000\n",
      "iteration 475 / 1500: loss 3.548163, accuracy 0.340000\n",
      "iteration 476 / 1500: loss 3.541961, accuracy 0.341000\n",
      "iteration 477 / 1500: loss 3.502572, accuracy 0.325000\n",
      "iteration 478 / 1500: loss 3.468970, accuracy 0.346000\n",
      "iteration 479 / 1500: loss 3.484130, accuracy 0.346000\n",
      "iteration 480 / 1500: loss 3.438990, accuracy 0.349000\n",
      "iteration 481 / 1500: loss 3.420984, accuracy 0.349000\n",
      "iteration 482 / 1500: loss 3.410272, accuracy 0.349000\n",
      "iteration 483 / 1500: loss 3.440455, accuracy 0.315000\n",
      "iteration 484 / 1500: loss 3.382666, accuracy 0.336000\n",
      "iteration 485 / 1500: loss 3.370483, accuracy 0.348000\n",
      "iteration 486 / 1500: loss 3.321215, accuracy 0.362000\n",
      "iteration 487 / 1500: loss 3.385694, accuracy 0.317000\n",
      "iteration 488 / 1500: loss 3.334001, accuracy 0.336000\n",
      "iteration 489 / 1500: loss 3.305476, accuracy 0.337000\n",
      "iteration 490 / 1500: loss 3.259636, accuracy 0.391000\n",
      "iteration 491 / 1500: loss 3.274572, accuracy 0.342000\n",
      "iteration 492 / 1500: loss 3.282137, accuracy 0.314000\n",
      "iteration 493 / 1500: loss 3.267872, accuracy 0.321000\n",
      "iteration 494 / 1500: loss 3.222777, accuracy 0.334000\n",
      "iteration 495 / 1500: loss 3.224921, accuracy 0.333000\n",
      "iteration 496 / 1500: loss 3.209210, accuracy 0.321000\n",
      "iteration 497 / 1500: loss 3.213500, accuracy 0.315000\n",
      "iteration 498 / 1500: loss 3.186428, accuracy 0.340000\n",
      "iteration 499 / 1500: loss 3.185444, accuracy 0.322000\n",
      "iteration 500 / 1500: loss 3.181195, accuracy 0.315000\n",
      "iteration 501 / 1500: loss 3.150717, accuracy 0.336000\n",
      "iteration 502 / 1500: loss 3.126865, accuracy 0.327000\n",
      "iteration 503 / 1500: loss 3.110149, accuracy 0.342000\n",
      "iteration 504 / 1500: loss 3.087702, accuracy 0.337000\n",
      "iteration 505 / 1500: loss 3.081521, accuracy 0.342000\n",
      "iteration 506 / 1500: loss 3.050441, accuracy 0.359000\n",
      "iteration 507 / 1500: loss 3.098239, accuracy 0.322000\n",
      "iteration 508 / 1500: loss 3.069139, accuracy 0.334000\n",
      "iteration 509 / 1500: loss 3.050047, accuracy 0.330000\n",
      "iteration 510 / 1500: loss 3.024918, accuracy 0.345000\n",
      "iteration 511 / 1500: loss 2.992826, accuracy 0.356000\n",
      "iteration 512 / 1500: loss 2.994810, accuracy 0.369000\n",
      "iteration 513 / 1500: loss 3.000166, accuracy 0.337000\n",
      "iteration 514 / 1500: loss 2.989456, accuracy 0.326000\n",
      "iteration 515 / 1500: loss 2.933967, accuracy 0.366000\n",
      "iteration 516 / 1500: loss 2.938737, accuracy 0.357000\n",
      "iteration 517 / 1500: loss 2.941199, accuracy 0.343000\n",
      "iteration 518 / 1500: loss 2.925742, accuracy 0.352000\n",
      "iteration 519 / 1500: loss 2.917961, accuracy 0.331000\n",
      "iteration 520 / 1500: loss 2.911115, accuracy 0.346000\n",
      "iteration 521 / 1500: loss 2.930189, accuracy 0.307000\n",
      "iteration 522 / 1500: loss 2.901700, accuracy 0.342000\n",
      "iteration 523 / 1500: loss 2.876084, accuracy 0.346000\n",
      "iteration 524 / 1500: loss 2.873707, accuracy 0.336000\n",
      "iteration 525 / 1500: loss 2.873595, accuracy 0.297000\n",
      "iteration 526 / 1500: loss 2.834228, accuracy 0.330000\n",
      "iteration 527 / 1500: loss 2.858180, accuracy 0.348000\n",
      "iteration 528 / 1500: loss 2.817258, accuracy 0.356000\n",
      "iteration 529 / 1500: loss 2.809717, accuracy 0.359000\n",
      "iteration 530 / 1500: loss 2.824566, accuracy 0.342000\n",
      "iteration 531 / 1500: loss 2.815712, accuracy 0.312000\n",
      "iteration 532 / 1500: loss 2.776874, accuracy 0.351000\n",
      "iteration 533 / 1500: loss 2.767122, accuracy 0.343000\n",
      "iteration 534 / 1500: loss 2.772037, accuracy 0.350000\n",
      "iteration 535 / 1500: loss 2.777678, accuracy 0.319000\n",
      "iteration 536 / 1500: loss 2.757131, accuracy 0.335000\n",
      "iteration 537 / 1500: loss 2.780407, accuracy 0.335000\n",
      "iteration 538 / 1500: loss 2.740202, accuracy 0.340000\n",
      "iteration 539 / 1500: loss 2.730591, accuracy 0.347000\n",
      "iteration 540 / 1500: loss 2.733678, accuracy 0.349000\n",
      "iteration 541 / 1500: loss 2.730142, accuracy 0.340000\n",
      "iteration 542 / 1500: loss 2.735637, accuracy 0.311000\n",
      "iteration 543 / 1500: loss 2.714663, accuracy 0.336000\n",
      "iteration 544 / 1500: loss 2.711491, accuracy 0.347000\n",
      "iteration 545 / 1500: loss 2.682274, accuracy 0.350000\n",
      "iteration 546 / 1500: loss 2.653985, accuracy 0.371000\n",
      "iteration 547 / 1500: loss 2.663035, accuracy 0.370000\n",
      "iteration 548 / 1500: loss 2.657524, accuracy 0.345000\n",
      "iteration 549 / 1500: loss 2.647699, accuracy 0.350000\n",
      "iteration 550 / 1500: loss 2.623563, accuracy 0.364000\n",
      "iteration 551 / 1500: loss 2.628580, accuracy 0.332000\n",
      "iteration 552 / 1500: loss 2.634282, accuracy 0.322000\n",
      "iteration 553 / 1500: loss 2.610886, accuracy 0.363000\n",
      "iteration 554 / 1500: loss 2.621293, accuracy 0.352000\n",
      "iteration 555 / 1500: loss 2.588951, accuracy 0.354000\n",
      "iteration 556 / 1500: loss 2.614600, accuracy 0.337000\n",
      "iteration 557 / 1500: loss 2.591931, accuracy 0.329000\n",
      "iteration 558 / 1500: loss 2.586649, accuracy 0.321000\n",
      "iteration 559 / 1500: loss 2.609457, accuracy 0.342000\n",
      "iteration 560 / 1500: loss 2.560464, accuracy 0.360000\n",
      "iteration 561 / 1500: loss 2.551488, accuracy 0.347000\n",
      "iteration 562 / 1500: loss 2.540543, accuracy 0.351000\n",
      "iteration 563 / 1500: loss 2.556960, accuracy 0.363000\n",
      "iteration 564 / 1500: loss 2.569054, accuracy 0.323000\n",
      "iteration 565 / 1500: loss 2.540026, accuracy 0.362000\n",
      "iteration 566 / 1500: loss 2.542086, accuracy 0.325000\n",
      "iteration 567 / 1500: loss 2.528761, accuracy 0.329000\n",
      "iteration 568 / 1500: loss 2.522143, accuracy 0.333000\n",
      "iteration 569 / 1500: loss 2.493705, accuracy 0.357000\n",
      "iteration 570 / 1500: loss 2.482723, accuracy 0.354000\n",
      "iteration 571 / 1500: loss 2.512275, accuracy 0.337000\n",
      "iteration 572 / 1500: loss 2.505235, accuracy 0.330000\n",
      "iteration 573 / 1500: loss 2.484321, accuracy 0.349000\n",
      "iteration 574 / 1500: loss 2.506540, accuracy 0.337000\n",
      "iteration 575 / 1500: loss 2.451451, accuracy 0.358000\n",
      "iteration 576 / 1500: loss 2.473089, accuracy 0.356000\n",
      "iteration 577 / 1500: loss 2.460568, accuracy 0.352000\n",
      "iteration 578 / 1500: loss 2.495274, accuracy 0.338000\n",
      "iteration 579 / 1500: loss 2.455246, accuracy 0.353000\n",
      "iteration 580 / 1500: loss 2.418575, accuracy 0.377000\n",
      "iteration 581 / 1500: loss 2.475486, accuracy 0.328000\n",
      "iteration 582 / 1500: loss 2.438833, accuracy 0.341000\n",
      "iteration 583 / 1500: loss 2.463054, accuracy 0.334000\n",
      "iteration 584 / 1500: loss 2.437666, accuracy 0.366000\n",
      "iteration 585 / 1500: loss 2.438653, accuracy 0.338000\n",
      "iteration 586 / 1500: loss 2.407299, accuracy 0.353000\n",
      "iteration 587 / 1500: loss 2.432535, accuracy 0.331000\n",
      "iteration 588 / 1500: loss 2.405271, accuracy 0.338000\n",
      "iteration 589 / 1500: loss 2.423017, accuracy 0.342000\n",
      "iteration 590 / 1500: loss 2.405509, accuracy 0.354000\n",
      "iteration 591 / 1500: loss 2.401702, accuracy 0.354000\n",
      "iteration 592 / 1500: loss 2.381241, accuracy 0.354000\n",
      "iteration 593 / 1500: loss 2.395106, accuracy 0.321000\n",
      "iteration 594 / 1500: loss 2.398965, accuracy 0.325000\n",
      "iteration 595 / 1500: loss 2.416194, accuracy 0.312000\n",
      "iteration 596 / 1500: loss 2.365028, accuracy 0.369000\n",
      "iteration 597 / 1500: loss 2.393084, accuracy 0.332000\n",
      "iteration 598 / 1500: loss 2.373037, accuracy 0.374000\n",
      "iteration 599 / 1500: loss 2.352119, accuracy 0.357000\n",
      "iteration 600 / 1500: loss 2.381955, accuracy 0.369000\n",
      "iteration 601 / 1500: loss 2.400438, accuracy 0.338000\n",
      "iteration 602 / 1500: loss 2.405042, accuracy 0.329000\n",
      "iteration 603 / 1500: loss 2.376150, accuracy 0.337000\n",
      "iteration 604 / 1500: loss 2.413657, accuracy 0.289000\n",
      "iteration 605 / 1500: loss 2.353768, accuracy 0.337000\n",
      "iteration 606 / 1500: loss 2.330514, accuracy 0.365000\n",
      "iteration 607 / 1500: loss 2.362411, accuracy 0.344000\n",
      "iteration 608 / 1500: loss 2.353641, accuracy 0.336000\n",
      "iteration 609 / 1500: loss 2.333752, accuracy 0.346000\n",
      "iteration 610 / 1500: loss 2.334018, accuracy 0.344000\n",
      "iteration 611 / 1500: loss 2.336302, accuracy 0.364000\n",
      "iteration 612 / 1500: loss 2.354511, accuracy 0.344000\n",
      "iteration 613 / 1500: loss 2.318053, accuracy 0.364000\n",
      "iteration 614 / 1500: loss 2.309409, accuracy 0.347000\n",
      "iteration 615 / 1500: loss 2.325677, accuracy 0.337000\n",
      "iteration 616 / 1500: loss 2.302275, accuracy 0.339000\n",
      "iteration 617 / 1500: loss 2.266330, accuracy 0.354000\n",
      "iteration 618 / 1500: loss 2.280684, accuracy 0.360000\n",
      "iteration 619 / 1500: loss 2.302803, accuracy 0.348000\n",
      "iteration 620 / 1500: loss 2.303006, accuracy 0.339000\n",
      "iteration 621 / 1500: loss 2.299361, accuracy 0.347000\n",
      "iteration 622 / 1500: loss 2.330135, accuracy 0.338000\n",
      "iteration 623 / 1500: loss 2.289796, accuracy 0.342000\n",
      "iteration 624 / 1500: loss 2.280911, accuracy 0.350000\n",
      "iteration 625 / 1500: loss 2.266029, accuracy 0.373000\n",
      "iteration 626 / 1500: loss 2.308258, accuracy 0.337000\n",
      "iteration 627 / 1500: loss 2.251072, accuracy 0.359000\n",
      "iteration 628 / 1500: loss 2.280343, accuracy 0.333000\n",
      "iteration 629 / 1500: loss 2.255848, accuracy 0.364000\n",
      "iteration 630 / 1500: loss 2.283697, accuracy 0.327000\n",
      "iteration 631 / 1500: loss 2.267174, accuracy 0.343000\n",
      "iteration 632 / 1500: loss 2.249892, accuracy 0.377000\n",
      "iteration 633 / 1500: loss 2.249273, accuracy 0.342000\n",
      "iteration 634 / 1500: loss 2.271328, accuracy 0.339000\n",
      "iteration 635 / 1500: loss 2.280445, accuracy 0.339000\n",
      "iteration 636 / 1500: loss 2.240291, accuracy 0.348000\n",
      "iteration 637 / 1500: loss 2.241748, accuracy 0.358000\n",
      "iteration 638 / 1500: loss 2.259858, accuracy 0.348000\n",
      "iteration 639 / 1500: loss 2.232860, accuracy 0.347000\n",
      "iteration 640 / 1500: loss 2.242948, accuracy 0.358000\n",
      "iteration 641 / 1500: loss 2.249383, accuracy 0.337000\n",
      "iteration 642 / 1500: loss 2.206822, accuracy 0.365000\n",
      "iteration 643 / 1500: loss 2.235943, accuracy 0.334000\n",
      "iteration 644 / 1500: loss 2.214876, accuracy 0.350000\n",
      "iteration 645 / 1500: loss 2.248140, accuracy 0.331000\n",
      "iteration 646 / 1500: loss 2.233255, accuracy 0.344000\n",
      "iteration 647 / 1500: loss 2.216610, accuracy 0.369000\n",
      "iteration 648 / 1500: loss 2.223913, accuracy 0.343000\n",
      "iteration 649 / 1500: loss 2.245392, accuracy 0.335000\n",
      "iteration 650 / 1500: loss 2.201070, accuracy 0.345000\n",
      "iteration 651 / 1500: loss 2.227561, accuracy 0.353000\n",
      "iteration 652 / 1500: loss 2.209714, accuracy 0.344000\n",
      "iteration 653 / 1500: loss 2.206265, accuracy 0.346000\n",
      "iteration 654 / 1500: loss 2.228131, accuracy 0.332000\n",
      "iteration 655 / 1500: loss 2.207178, accuracy 0.340000\n",
      "iteration 656 / 1500: loss 2.212129, accuracy 0.359000\n",
      "iteration 657 / 1500: loss 2.196612, accuracy 0.342000\n",
      "iteration 658 / 1500: loss 2.189915, accuracy 0.368000\n",
      "iteration 659 / 1500: loss 2.196304, accuracy 0.340000\n",
      "iteration 660 / 1500: loss 2.232555, accuracy 0.330000\n",
      "iteration 661 / 1500: loss 2.184118, accuracy 0.348000\n",
      "iteration 662 / 1500: loss 2.185248, accuracy 0.355000\n",
      "iteration 663 / 1500: loss 2.188775, accuracy 0.355000\n",
      "iteration 664 / 1500: loss 2.164261, accuracy 0.375000\n",
      "iteration 665 / 1500: loss 2.207429, accuracy 0.327000\n",
      "iteration 666 / 1500: loss 2.164449, accuracy 0.357000\n",
      "iteration 667 / 1500: loss 2.200043, accuracy 0.344000\n",
      "iteration 668 / 1500: loss 2.185166, accuracy 0.342000\n",
      "iteration 669 / 1500: loss 2.200895, accuracy 0.334000\n",
      "iteration 670 / 1500: loss 2.184831, accuracy 0.350000\n",
      "iteration 671 / 1500: loss 2.181514, accuracy 0.346000\n",
      "iteration 672 / 1500: loss 2.196446, accuracy 0.348000\n",
      "iteration 673 / 1500: loss 2.180275, accuracy 0.355000\n",
      "iteration 674 / 1500: loss 2.160760, accuracy 0.354000\n",
      "iteration 675 / 1500: loss 2.157685, accuracy 0.360000\n",
      "iteration 676 / 1500: loss 2.203959, accuracy 0.329000\n",
      "iteration 677 / 1500: loss 2.175723, accuracy 0.349000\n",
      "iteration 678 / 1500: loss 2.165953, accuracy 0.356000\n",
      "iteration 679 / 1500: loss 2.172717, accuracy 0.322000\n",
      "iteration 680 / 1500: loss 2.157648, accuracy 0.370000\n",
      "iteration 681 / 1500: loss 2.155022, accuracy 0.364000\n",
      "iteration 682 / 1500: loss 2.151643, accuracy 0.353000\n",
      "iteration 683 / 1500: loss 2.155306, accuracy 0.363000\n",
      "iteration 684 / 1500: loss 2.161206, accuracy 0.349000\n",
      "iteration 685 / 1500: loss 2.200160, accuracy 0.316000\n",
      "iteration 686 / 1500: loss 2.143101, accuracy 0.340000\n",
      "iteration 687 / 1500: loss 2.141896, accuracy 0.346000\n",
      "iteration 688 / 1500: loss 2.186258, accuracy 0.336000\n",
      "iteration 689 / 1500: loss 2.161256, accuracy 0.354000\n",
      "iteration 690 / 1500: loss 2.155479, accuracy 0.353000\n",
      "iteration 691 / 1500: loss 2.171848, accuracy 0.337000\n",
      "iteration 692 / 1500: loss 2.133731, accuracy 0.343000\n",
      "iteration 693 / 1500: loss 2.150147, accuracy 0.360000\n",
      "iteration 694 / 1500: loss 2.150128, accuracy 0.327000\n",
      "iteration 695 / 1500: loss 2.150122, accuracy 0.335000\n",
      "iteration 696 / 1500: loss 2.157870, accuracy 0.326000\n",
      "iteration 697 / 1500: loss 2.128619, accuracy 0.357000\n",
      "iteration 698 / 1500: loss 2.133940, accuracy 0.327000\n",
      "iteration 699 / 1500: loss 2.162710, accuracy 0.343000\n",
      "iteration 700 / 1500: loss 2.124473, accuracy 0.373000\n",
      "iteration 701 / 1500: loss 2.114710, accuracy 0.357000\n",
      "iteration 702 / 1500: loss 2.135851, accuracy 0.334000\n",
      "iteration 703 / 1500: loss 2.127316, accuracy 0.339000\n",
      "iteration 704 / 1500: loss 2.127085, accuracy 0.341000\n",
      "iteration 705 / 1500: loss 2.167041, accuracy 0.346000\n",
      "iteration 706 / 1500: loss 2.135827, accuracy 0.338000\n",
      "iteration 707 / 1500: loss 2.139442, accuracy 0.334000\n",
      "iteration 708 / 1500: loss 2.127959, accuracy 0.351000\n",
      "iteration 709 / 1500: loss 2.136682, accuracy 0.339000\n",
      "iteration 710 / 1500: loss 2.133005, accuracy 0.330000\n",
      "iteration 711 / 1500: loss 2.105232, accuracy 0.351000\n",
      "iteration 712 / 1500: loss 2.151653, accuracy 0.319000\n",
      "iteration 713 / 1500: loss 2.125711, accuracy 0.342000\n",
      "iteration 714 / 1500: loss 2.117835, accuracy 0.345000\n",
      "iteration 715 / 1500: loss 2.103425, accuracy 0.368000\n",
      "iteration 716 / 1500: loss 2.134457, accuracy 0.334000\n",
      "iteration 717 / 1500: loss 2.124340, accuracy 0.339000\n",
      "iteration 718 / 1500: loss 2.135028, accuracy 0.326000\n",
      "iteration 719 / 1500: loss 2.144923, accuracy 0.323000\n",
      "iteration 720 / 1500: loss 2.106533, accuracy 0.325000\n",
      "iteration 721 / 1500: loss 2.109208, accuracy 0.357000\n",
      "iteration 722 / 1500: loss 2.104116, accuracy 0.378000\n",
      "iteration 723 / 1500: loss 2.127212, accuracy 0.345000\n",
      "iteration 724 / 1500: loss 2.125407, accuracy 0.348000\n",
      "iteration 725 / 1500: loss 2.100948, accuracy 0.354000\n",
      "iteration 726 / 1500: loss 2.152571, accuracy 0.330000\n",
      "iteration 727 / 1500: loss 2.118050, accuracy 0.347000\n",
      "iteration 728 / 1500: loss 2.088313, accuracy 0.355000\n",
      "iteration 729 / 1500: loss 2.133069, accuracy 0.342000\n",
      "iteration 730 / 1500: loss 2.079843, accuracy 0.343000\n",
      "iteration 731 / 1500: loss 2.107508, accuracy 0.351000\n",
      "iteration 732 / 1500: loss 2.081593, accuracy 0.356000\n",
      "iteration 733 / 1500: loss 2.107032, accuracy 0.353000\n",
      "iteration 734 / 1500: loss 2.113873, accuracy 0.336000\n",
      "iteration 735 / 1500: loss 2.109864, accuracy 0.345000\n",
      "iteration 736 / 1500: loss 2.074350, accuracy 0.349000\n",
      "iteration 737 / 1500: loss 2.119528, accuracy 0.322000\n",
      "iteration 738 / 1500: loss 2.121421, accuracy 0.348000\n",
      "iteration 739 / 1500: loss 2.088241, accuracy 0.351000\n",
      "iteration 740 / 1500: loss 2.076414, accuracy 0.344000\n",
      "iteration 741 / 1500: loss 2.075708, accuracy 0.363000\n",
      "iteration 742 / 1500: loss 2.096452, accuracy 0.367000\n",
      "iteration 743 / 1500: loss 2.092131, accuracy 0.351000\n",
      "iteration 744 / 1500: loss 2.082508, accuracy 0.352000\n",
      "iteration 745 / 1500: loss 2.112601, accuracy 0.347000\n",
      "iteration 746 / 1500: loss 2.129421, accuracy 0.313000\n",
      "iteration 747 / 1500: loss 2.097515, accuracy 0.349000\n",
      "iteration 748 / 1500: loss 2.106266, accuracy 0.333000\n",
      "iteration 749 / 1500: loss 2.088273, accuracy 0.348000\n",
      "iteration 750 / 1500: loss 2.085959, accuracy 0.350000\n",
      "iteration 751 / 1500: loss 2.077226, accuracy 0.324000\n",
      "iteration 752 / 1500: loss 2.052041, accuracy 0.396000\n",
      "iteration 753 / 1500: loss 2.098291, accuracy 0.343000\n",
      "iteration 754 / 1500: loss 2.120667, accuracy 0.332000\n",
      "iteration 755 / 1500: loss 2.102957, accuracy 0.373000\n",
      "iteration 756 / 1500: loss 2.100539, accuracy 0.365000\n",
      "iteration 757 / 1500: loss 2.102448, accuracy 0.350000\n",
      "iteration 758 / 1500: loss 2.101044, accuracy 0.345000\n",
      "iteration 759 / 1500: loss 2.057239, accuracy 0.328000\n",
      "iteration 760 / 1500: loss 2.058307, accuracy 0.355000\n",
      "iteration 761 / 1500: loss 2.066983, accuracy 0.338000\n",
      "iteration 762 / 1500: loss 2.090186, accuracy 0.354000\n",
      "iteration 763 / 1500: loss 2.053913, accuracy 0.380000\n",
      "iteration 764 / 1500: loss 2.107453, accuracy 0.330000\n",
      "iteration 765 / 1500: loss 2.090184, accuracy 0.338000\n",
      "iteration 766 / 1500: loss 2.057062, accuracy 0.368000\n",
      "iteration 767 / 1500: loss 2.046250, accuracy 0.367000\n",
      "iteration 768 / 1500: loss 2.091477, accuracy 0.358000\n",
      "iteration 769 / 1500: loss 2.082590, accuracy 0.337000\n",
      "iteration 770 / 1500: loss 2.050595, accuracy 0.363000\n",
      "iteration 771 / 1500: loss 2.066059, accuracy 0.371000\n",
      "iteration 772 / 1500: loss 2.075862, accuracy 0.348000\n",
      "iteration 773 / 1500: loss 2.090048, accuracy 0.325000\n",
      "iteration 774 / 1500: loss 2.084729, accuracy 0.339000\n",
      "iteration 775 / 1500: loss 2.064114, accuracy 0.338000\n",
      "iteration 776 / 1500: loss 2.066482, accuracy 0.329000\n",
      "iteration 777 / 1500: loss 2.088478, accuracy 0.328000\n",
      "iteration 778 / 1500: loss 2.044702, accuracy 0.377000\n",
      "iteration 779 / 1500: loss 2.049424, accuracy 0.353000\n",
      "iteration 780 / 1500: loss 2.093674, accuracy 0.335000\n",
      "iteration 781 / 1500: loss 2.095982, accuracy 0.339000\n",
      "iteration 782 / 1500: loss 2.087224, accuracy 0.352000\n",
      "iteration 783 / 1500: loss 2.064900, accuracy 0.363000\n",
      "iteration 784 / 1500: loss 2.078618, accuracy 0.340000\n",
      "iteration 785 / 1500: loss 2.085879, accuracy 0.355000\n",
      "iteration 786 / 1500: loss 2.067750, accuracy 0.346000\n",
      "iteration 787 / 1500: loss 2.075073, accuracy 0.332000\n",
      "iteration 788 / 1500: loss 2.073481, accuracy 0.316000\n",
      "iteration 789 / 1500: loss 2.112937, accuracy 0.315000\n",
      "iteration 790 / 1500: loss 2.079086, accuracy 0.346000\n",
      "iteration 791 / 1500: loss 2.075666, accuracy 0.346000\n",
      "iteration 792 / 1500: loss 2.059968, accuracy 0.349000\n",
      "iteration 793 / 1500: loss 2.039560, accuracy 0.368000\n",
      "iteration 794 / 1500: loss 2.079707, accuracy 0.346000\n",
      "iteration 795 / 1500: loss 2.088721, accuracy 0.359000\n",
      "iteration 796 / 1500: loss 2.095695, accuracy 0.318000\n",
      "iteration 797 / 1500: loss 2.059994, accuracy 0.363000\n",
      "iteration 798 / 1500: loss 2.070165, accuracy 0.351000\n",
      "iteration 799 / 1500: loss 2.047806, accuracy 0.346000\n",
      "iteration 800 / 1500: loss 2.051195, accuracy 0.342000\n",
      "iteration 801 / 1500: loss 2.066698, accuracy 0.327000\n",
      "iteration 802 / 1500: loss 2.078071, accuracy 0.331000\n",
      "iteration 803 / 1500: loss 2.080116, accuracy 0.336000\n",
      "iteration 804 / 1500: loss 2.059061, accuracy 0.362000\n",
      "iteration 805 / 1500: loss 2.076550, accuracy 0.324000\n",
      "iteration 806 / 1500: loss 2.070169, accuracy 0.345000\n",
      "iteration 807 / 1500: loss 2.041635, accuracy 0.354000\n",
      "iteration 808 / 1500: loss 2.085285, accuracy 0.342000\n",
      "iteration 809 / 1500: loss 2.052489, accuracy 0.357000\n",
      "iteration 810 / 1500: loss 2.090049, accuracy 0.335000\n",
      "iteration 811 / 1500: loss 2.030492, accuracy 0.367000\n",
      "iteration 812 / 1500: loss 2.062469, accuracy 0.352000\n",
      "iteration 813 / 1500: loss 2.047652, accuracy 0.361000\n",
      "iteration 814 / 1500: loss 2.030376, accuracy 0.367000\n",
      "iteration 815 / 1500: loss 2.070153, accuracy 0.365000\n",
      "iteration 816 / 1500: loss 2.075153, accuracy 0.332000\n",
      "iteration 817 / 1500: loss 2.059433, accuracy 0.332000\n",
      "iteration 818 / 1500: loss 2.064819, accuracy 0.355000\n",
      "iteration 819 / 1500: loss 2.078659, accuracy 0.329000\n",
      "iteration 820 / 1500: loss 2.081130, accuracy 0.324000\n",
      "iteration 821 / 1500: loss 2.052681, accuracy 0.347000\n",
      "iteration 822 / 1500: loss 2.066475, accuracy 0.368000\n",
      "iteration 823 / 1500: loss 2.057413, accuracy 0.348000\n",
      "iteration 824 / 1500: loss 2.047575, accuracy 0.349000\n",
      "iteration 825 / 1500: loss 2.055303, accuracy 0.365000\n",
      "iteration 826 / 1500: loss 2.070818, accuracy 0.334000\n",
      "iteration 827 / 1500: loss 2.059276, accuracy 0.345000\n",
      "iteration 828 / 1500: loss 2.030333, accuracy 0.361000\n",
      "iteration 829 / 1500: loss 2.086751, accuracy 0.318000\n",
      "iteration 830 / 1500: loss 2.065595, accuracy 0.359000\n",
      "iteration 831 / 1500: loss 2.065622, accuracy 0.334000\n",
      "iteration 832 / 1500: loss 2.047969, accuracy 0.373000\n",
      "iteration 833 / 1500: loss 2.036077, accuracy 0.355000\n",
      "iteration 834 / 1500: loss 2.051675, accuracy 0.354000\n",
      "iteration 835 / 1500: loss 2.070747, accuracy 0.338000\n",
      "iteration 836 / 1500: loss 2.091884, accuracy 0.337000\n",
      "iteration 837 / 1500: loss 2.041583, accuracy 0.355000\n",
      "iteration 838 / 1500: loss 2.056036, accuracy 0.362000\n",
      "iteration 839 / 1500: loss 2.056908, accuracy 0.350000\n",
      "iteration 840 / 1500: loss 2.054590, accuracy 0.341000\n",
      "iteration 841 / 1500: loss 2.022762, accuracy 0.387000\n",
      "iteration 842 / 1500: loss 2.076745, accuracy 0.338000\n",
      "iteration 843 / 1500: loss 2.040090, accuracy 0.375000\n",
      "iteration 844 / 1500: loss 2.063732, accuracy 0.341000\n",
      "iteration 845 / 1500: loss 2.025243, accuracy 0.370000\n",
      "iteration 846 / 1500: loss 2.072101, accuracy 0.336000\n",
      "iteration 847 / 1500: loss 2.061869, accuracy 0.336000\n",
      "iteration 848 / 1500: loss 2.054455, accuracy 0.356000\n",
      "iteration 849 / 1500: loss 2.073452, accuracy 0.331000\n",
      "iteration 850 / 1500: loss 2.045632, accuracy 0.359000\n",
      "iteration 851 / 1500: loss 2.043611, accuracy 0.345000\n",
      "iteration 852 / 1500: loss 2.068352, accuracy 0.339000\n",
      "iteration 853 / 1500: loss 2.073832, accuracy 0.351000\n",
      "iteration 854 / 1500: loss 2.076242, accuracy 0.338000\n",
      "iteration 855 / 1500: loss 2.041803, accuracy 0.363000\n",
      "iteration 856 / 1500: loss 2.052302, accuracy 0.354000\n",
      "iteration 857 / 1500: loss 2.091082, accuracy 0.352000\n",
      "iteration 858 / 1500: loss 2.030762, accuracy 0.332000\n",
      "iteration 859 / 1500: loss 2.028159, accuracy 0.387000\n",
      "iteration 860 / 1500: loss 2.071613, accuracy 0.336000\n",
      "iteration 861 / 1500: loss 2.031607, accuracy 0.355000\n",
      "iteration 862 / 1500: loss 2.030102, accuracy 0.370000\n",
      "iteration 863 / 1500: loss 2.032497, accuracy 0.336000\n",
      "iteration 864 / 1500: loss 2.085163, accuracy 0.322000\n",
      "iteration 865 / 1500: loss 2.033418, accuracy 0.354000\n",
      "iteration 866 / 1500: loss 2.046437, accuracy 0.342000\n",
      "iteration 867 / 1500: loss 2.041190, accuracy 0.364000\n",
      "iteration 868 / 1500: loss 2.029589, accuracy 0.376000\n",
      "iteration 869 / 1500: loss 2.028642, accuracy 0.386000\n",
      "iteration 870 / 1500: loss 2.075342, accuracy 0.336000\n",
      "iteration 871 / 1500: loss 2.048749, accuracy 0.330000\n",
      "iteration 872 / 1500: loss 2.046523, accuracy 0.340000\n",
      "iteration 873 / 1500: loss 2.065985, accuracy 0.336000\n",
      "iteration 874 / 1500: loss 2.051720, accuracy 0.351000\n",
      "iteration 875 / 1500: loss 2.039201, accuracy 0.356000\n",
      "iteration 876 / 1500: loss 2.061355, accuracy 0.348000\n",
      "iteration 877 / 1500: loss 2.048553, accuracy 0.352000\n",
      "iteration 878 / 1500: loss 2.051616, accuracy 0.347000\n",
      "iteration 879 / 1500: loss 2.061034, accuracy 0.342000\n",
      "iteration 880 / 1500: loss 2.038378, accuracy 0.362000\n",
      "iteration 881 / 1500: loss 2.058782, accuracy 0.354000\n",
      "iteration 882 / 1500: loss 2.056054, accuracy 0.334000\n",
      "iteration 883 / 1500: loss 2.055139, accuracy 0.348000\n",
      "iteration 884 / 1500: loss 2.037361, accuracy 0.373000\n",
      "iteration 885 / 1500: loss 2.038386, accuracy 0.351000\n",
      "iteration 886 / 1500: loss 2.060322, accuracy 0.330000\n",
      "iteration 887 / 1500: loss 2.074016, accuracy 0.315000\n",
      "iteration 888 / 1500: loss 2.064435, accuracy 0.349000\n",
      "iteration 889 / 1500: loss 2.046610, accuracy 0.348000\n",
      "iteration 890 / 1500: loss 2.023961, accuracy 0.359000\n",
      "iteration 891 / 1500: loss 2.054741, accuracy 0.349000\n",
      "iteration 892 / 1500: loss 2.077848, accuracy 0.343000\n",
      "iteration 893 / 1500: loss 2.056293, accuracy 0.347000\n",
      "iteration 894 / 1500: loss 2.060120, accuracy 0.342000\n",
      "iteration 895 / 1500: loss 2.061429, accuracy 0.323000\n",
      "iteration 896 / 1500: loss 2.045639, accuracy 0.364000\n",
      "iteration 897 / 1500: loss 2.065569, accuracy 0.343000\n",
      "iteration 898 / 1500: loss 2.048164, accuracy 0.359000\n",
      "iteration 899 / 1500: loss 2.017853, accuracy 0.387000\n",
      "iteration 900 / 1500: loss 2.039868, accuracy 0.364000\n",
      "iteration 901 / 1500: loss 2.051736, accuracy 0.351000\n",
      "iteration 902 / 1500: loss 2.040405, accuracy 0.355000\n",
      "iteration 903 / 1500: loss 2.058037, accuracy 0.335000\n",
      "iteration 904 / 1500: loss 2.029822, accuracy 0.379000\n",
      "iteration 905 / 1500: loss 2.040480, accuracy 0.372000\n",
      "iteration 906 / 1500: loss 2.031251, accuracy 0.355000\n",
      "iteration 907 / 1500: loss 2.030819, accuracy 0.349000\n",
      "iteration 908 / 1500: loss 2.078571, accuracy 0.308000\n",
      "iteration 909 / 1500: loss 2.049329, accuracy 0.337000\n",
      "iteration 910 / 1500: loss 2.021281, accuracy 0.374000\n",
      "iteration 911 / 1500: loss 2.038631, accuracy 0.354000\n",
      "iteration 912 / 1500: loss 2.027086, accuracy 0.354000\n",
      "iteration 913 / 1500: loss 2.071604, accuracy 0.333000\n",
      "iteration 914 / 1500: loss 2.056293, accuracy 0.348000\n",
      "iteration 915 / 1500: loss 2.048334, accuracy 0.342000\n",
      "iteration 916 / 1500: loss 2.037813, accuracy 0.342000\n",
      "iteration 917 / 1500: loss 2.035239, accuracy 0.340000\n",
      "iteration 918 / 1500: loss 2.035701, accuracy 0.350000\n",
      "iteration 919 / 1500: loss 2.042423, accuracy 0.332000\n",
      "iteration 920 / 1500: loss 2.047093, accuracy 0.324000\n",
      "iteration 921 / 1500: loss 2.040941, accuracy 0.339000\n",
      "iteration 922 / 1500: loss 2.055443, accuracy 0.334000\n",
      "iteration 923 / 1500: loss 2.026305, accuracy 0.341000\n",
      "iteration 924 / 1500: loss 2.034206, accuracy 0.337000\n",
      "iteration 925 / 1500: loss 2.057418, accuracy 0.336000\n",
      "iteration 926 / 1500: loss 2.035422, accuracy 0.341000\n",
      "iteration 927 / 1500: loss 2.071468, accuracy 0.351000\n",
      "iteration 928 / 1500: loss 2.060160, accuracy 0.345000\n",
      "iteration 929 / 1500: loss 2.051159, accuracy 0.332000\n",
      "iteration 930 / 1500: loss 2.050774, accuracy 0.327000\n",
      "iteration 931 / 1500: loss 2.040323, accuracy 0.352000\n",
      "iteration 932 / 1500: loss 2.047460, accuracy 0.350000\n",
      "iteration 933 / 1500: loss 2.053778, accuracy 0.340000\n",
      "iteration 934 / 1500: loss 2.063330, accuracy 0.328000\n",
      "iteration 935 / 1500: loss 2.049353, accuracy 0.339000\n",
      "iteration 936 / 1500: loss 2.027140, accuracy 0.333000\n",
      "iteration 937 / 1500: loss 2.047031, accuracy 0.357000\n",
      "iteration 938 / 1500: loss 2.037227, accuracy 0.344000\n",
      "iteration 939 / 1500: loss 2.015205, accuracy 0.373000\n",
      "iteration 940 / 1500: loss 2.027162, accuracy 0.371000\n",
      "iteration 941 / 1500: loss 2.058663, accuracy 0.332000\n",
      "iteration 942 / 1500: loss 2.061903, accuracy 0.331000\n",
      "iteration 943 / 1500: loss 2.057880, accuracy 0.328000\n",
      "iteration 944 / 1500: loss 2.056747, accuracy 0.335000\n",
      "iteration 945 / 1500: loss 2.036773, accuracy 0.361000\n",
      "iteration 946 / 1500: loss 2.063306, accuracy 0.342000\n",
      "iteration 947 / 1500: loss 2.063189, accuracy 0.305000\n",
      "iteration 948 / 1500: loss 2.053852, accuracy 0.342000\n",
      "iteration 949 / 1500: loss 2.031183, accuracy 0.353000\n",
      "iteration 950 / 1500: loss 2.032811, accuracy 0.358000\n",
      "iteration 951 / 1500: loss 2.087057, accuracy 0.304000\n",
      "iteration 952 / 1500: loss 2.033599, accuracy 0.353000\n",
      "iteration 953 / 1500: loss 2.055874, accuracy 0.337000\n",
      "iteration 954 / 1500: loss 2.049019, accuracy 0.344000\n",
      "iteration 955 / 1500: loss 2.057097, accuracy 0.319000\n",
      "iteration 956 / 1500: loss 2.055068, accuracy 0.338000\n",
      "iteration 957 / 1500: loss 2.023959, accuracy 0.361000\n",
      "iteration 958 / 1500: loss 2.048189, accuracy 0.340000\n",
      "iteration 959 / 1500: loss 2.035238, accuracy 0.351000\n",
      "iteration 960 / 1500: loss 2.058687, accuracy 0.333000\n",
      "iteration 961 / 1500: loss 1.995950, accuracy 0.374000\n",
      "iteration 962 / 1500: loss 2.034052, accuracy 0.346000\n",
      "iteration 963 / 1500: loss 2.051309, accuracy 0.346000\n",
      "iteration 964 / 1500: loss 2.030636, accuracy 0.370000\n",
      "iteration 965 / 1500: loss 2.077389, accuracy 0.326000\n",
      "iteration 966 / 1500: loss 2.034481, accuracy 0.365000\n",
      "iteration 967 / 1500: loss 2.072205, accuracy 0.314000\n",
      "iteration 968 / 1500: loss 2.028704, accuracy 0.352000\n",
      "iteration 969 / 1500: loss 2.014930, accuracy 0.362000\n",
      "iteration 970 / 1500: loss 2.029096, accuracy 0.334000\n",
      "iteration 971 / 1500: loss 2.042750, accuracy 0.343000\n",
      "iteration 972 / 1500: loss 2.079466, accuracy 0.315000\n",
      "iteration 973 / 1500: loss 2.052898, accuracy 0.350000\n",
      "iteration 974 / 1500: loss 2.055860, accuracy 0.339000\n",
      "iteration 975 / 1500: loss 2.045943, accuracy 0.349000\n",
      "iteration 976 / 1500: loss 2.043680, accuracy 0.345000\n",
      "iteration 977 / 1500: loss 2.049796, accuracy 0.350000\n",
      "iteration 978 / 1500: loss 2.050946, accuracy 0.326000\n",
      "iteration 979 / 1500: loss 2.066102, accuracy 0.338000\n",
      "iteration 980 / 1500: loss 2.039658, accuracy 0.343000\n",
      "iteration 981 / 1500: loss 2.031944, accuracy 0.341000\n",
      "iteration 982 / 1500: loss 2.045056, accuracy 0.347000\n",
      "iteration 983 / 1500: loss 2.041231, accuracy 0.365000\n",
      "iteration 984 / 1500: loss 2.025972, accuracy 0.343000\n",
      "iteration 985 / 1500: loss 2.035788, accuracy 0.358000\n",
      "iteration 986 / 1500: loss 2.054314, accuracy 0.320000\n",
      "iteration 987 / 1500: loss 2.061119, accuracy 0.325000\n",
      "iteration 988 / 1500: loss 2.048293, accuracy 0.329000\n",
      "iteration 989 / 1500: loss 2.041779, accuracy 0.347000\n",
      "iteration 990 / 1500: loss 2.032483, accuracy 0.340000\n",
      "iteration 991 / 1500: loss 2.046564, accuracy 0.328000\n",
      "iteration 992 / 1500: loss 2.018680, accuracy 0.357000\n",
      "iteration 993 / 1500: loss 2.009360, accuracy 0.365000\n",
      "iteration 994 / 1500: loss 2.043005, accuracy 0.345000\n",
      "iteration 995 / 1500: loss 2.065539, accuracy 0.334000\n",
      "iteration 996 / 1500: loss 2.007903, accuracy 0.367000\n",
      "iteration 997 / 1500: loss 2.046362, accuracy 0.336000\n",
      "iteration 998 / 1500: loss 2.022536, accuracy 0.338000\n",
      "iteration 999 / 1500: loss 2.073756, accuracy 0.312000\n",
      "iteration 1000 / 1500: loss 2.023150, accuracy 0.378000\n",
      "iteration 1001 / 1500: loss 2.062951, accuracy 0.315000\n",
      "iteration 1002 / 1500: loss 2.039174, accuracy 0.337000\n",
      "iteration 1003 / 1500: loss 2.038377, accuracy 0.345000\n",
      "iteration 1004 / 1500: loss 2.035522, accuracy 0.373000\n",
      "iteration 1005 / 1500: loss 2.059967, accuracy 0.327000\n",
      "iteration 1006 / 1500: loss 2.101478, accuracy 0.302000\n",
      "iteration 1007 / 1500: loss 2.050563, accuracy 0.319000\n",
      "iteration 1008 / 1500: loss 2.031966, accuracy 0.339000\n",
      "iteration 1009 / 1500: loss 2.053898, accuracy 0.352000\n",
      "iteration 1010 / 1500: loss 2.062117, accuracy 0.346000\n",
      "iteration 1011 / 1500: loss 2.051576, accuracy 0.339000\n",
      "iteration 1012 / 1500: loss 2.043971, accuracy 0.345000\n",
      "iteration 1013 / 1500: loss 2.042986, accuracy 0.337000\n",
      "iteration 1014 / 1500: loss 2.050399, accuracy 0.343000\n",
      "iteration 1015 / 1500: loss 2.042873, accuracy 0.353000\n",
      "iteration 1016 / 1500: loss 2.050923, accuracy 0.341000\n",
      "iteration 1017 / 1500: loss 2.027405, accuracy 0.339000\n",
      "iteration 1018 / 1500: loss 2.035701, accuracy 0.360000\n",
      "iteration 1019 / 1500: loss 2.022939, accuracy 0.348000\n",
      "iteration 1020 / 1500: loss 2.020510, accuracy 0.358000\n",
      "iteration 1021 / 1500: loss 2.041628, accuracy 0.338000\n",
      "iteration 1022 / 1500: loss 2.059926, accuracy 0.339000\n",
      "iteration 1023 / 1500: loss 2.025542, accuracy 0.357000\n",
      "iteration 1024 / 1500: loss 2.022830, accuracy 0.352000\n",
      "iteration 1025 / 1500: loss 2.051516, accuracy 0.341000\n",
      "iteration 1026 / 1500: loss 2.069183, accuracy 0.338000\n",
      "iteration 1027 / 1500: loss 2.029859, accuracy 0.354000\n",
      "iteration 1028 / 1500: loss 2.033578, accuracy 0.373000\n",
      "iteration 1029 / 1500: loss 2.052687, accuracy 0.332000\n",
      "iteration 1030 / 1500: loss 2.050533, accuracy 0.336000\n",
      "iteration 1031 / 1500: loss 2.029183, accuracy 0.334000\n",
      "iteration 1032 / 1500: loss 2.039760, accuracy 0.370000\n",
      "iteration 1033 / 1500: loss 2.017334, accuracy 0.348000\n",
      "iteration 1034 / 1500: loss 2.072816, accuracy 0.353000\n",
      "iteration 1035 / 1500: loss 2.050055, accuracy 0.334000\n",
      "iteration 1036 / 1500: loss 2.026559, accuracy 0.357000\n",
      "iteration 1037 / 1500: loss 2.035061, accuracy 0.351000\n",
      "iteration 1038 / 1500: loss 2.062166, accuracy 0.340000\n",
      "iteration 1039 / 1500: loss 2.039748, accuracy 0.366000\n",
      "iteration 1040 / 1500: loss 2.034276, accuracy 0.348000\n",
      "iteration 1041 / 1500: loss 2.039424, accuracy 0.341000\n",
      "iteration 1042 / 1500: loss 2.006438, accuracy 0.364000\n",
      "iteration 1043 / 1500: loss 2.014267, accuracy 0.363000\n",
      "iteration 1044 / 1500: loss 2.034568, accuracy 0.352000\n",
      "iteration 1045 / 1500: loss 2.031883, accuracy 0.360000\n",
      "iteration 1046 / 1500: loss 2.038379, accuracy 0.342000\n",
      "iteration 1047 / 1500: loss 2.053852, accuracy 0.363000\n",
      "iteration 1048 / 1500: loss 2.024452, accuracy 0.362000\n",
      "iteration 1049 / 1500: loss 2.033906, accuracy 0.368000\n",
      "iteration 1050 / 1500: loss 2.059239, accuracy 0.337000\n",
      "iteration 1051 / 1500: loss 2.012956, accuracy 0.369000\n",
      "iteration 1052 / 1500: loss 2.020503, accuracy 0.356000\n",
      "iteration 1053 / 1500: loss 2.019925, accuracy 0.366000\n",
      "iteration 1054 / 1500: loss 2.015145, accuracy 0.356000\n",
      "iteration 1055 / 1500: loss 2.040641, accuracy 0.351000\n",
      "iteration 1056 / 1500: loss 2.006862, accuracy 0.370000\n",
      "iteration 1057 / 1500: loss 2.029752, accuracy 0.356000\n",
      "iteration 1058 / 1500: loss 2.036687, accuracy 0.334000\n",
      "iteration 1059 / 1500: loss 2.019941, accuracy 0.370000\n",
      "iteration 1060 / 1500: loss 2.039327, accuracy 0.336000\n",
      "iteration 1061 / 1500: loss 2.029535, accuracy 0.350000\n",
      "iteration 1062 / 1500: loss 2.027066, accuracy 0.352000\n",
      "iteration 1063 / 1500: loss 2.035786, accuracy 0.348000\n",
      "iteration 1064 / 1500: loss 2.058372, accuracy 0.346000\n",
      "iteration 1065 / 1500: loss 2.000062, accuracy 0.366000\n",
      "iteration 1066 / 1500: loss 2.036530, accuracy 0.330000\n",
      "iteration 1067 / 1500: loss 2.033079, accuracy 0.352000\n",
      "iteration 1068 / 1500: loss 2.044028, accuracy 0.340000\n",
      "iteration 1069 / 1500: loss 1.997613, accuracy 0.357000\n",
      "iteration 1070 / 1500: loss 2.020341, accuracy 0.365000\n",
      "iteration 1071 / 1500: loss 2.033622, accuracy 0.339000\n",
      "iteration 1072 / 1500: loss 2.048098, accuracy 0.354000\n",
      "iteration 1073 / 1500: loss 2.036854, accuracy 0.352000\n",
      "iteration 1074 / 1500: loss 2.058897, accuracy 0.333000\n",
      "iteration 1075 / 1500: loss 2.038191, accuracy 0.356000\n",
      "iteration 1076 / 1500: loss 2.025536, accuracy 0.359000\n",
      "iteration 1077 / 1500: loss 2.061335, accuracy 0.328000\n",
      "iteration 1078 / 1500: loss 2.018352, accuracy 0.357000\n",
      "iteration 1079 / 1500: loss 2.041532, accuracy 0.361000\n",
      "iteration 1080 / 1500: loss 2.017262, accuracy 0.349000\n",
      "iteration 1081 / 1500: loss 2.041152, accuracy 0.345000\n",
      "iteration 1082 / 1500: loss 2.049701, accuracy 0.349000\n",
      "iteration 1083 / 1500: loss 2.029056, accuracy 0.350000\n",
      "iteration 1084 / 1500: loss 2.032537, accuracy 0.356000\n",
      "iteration 1085 / 1500: loss 2.027152, accuracy 0.340000\n",
      "iteration 1086 / 1500: loss 2.003930, accuracy 0.370000\n",
      "iteration 1087 / 1500: loss 2.028939, accuracy 0.342000\n",
      "iteration 1088 / 1500: loss 2.041565, accuracy 0.355000\n",
      "iteration 1089 / 1500: loss 2.043937, accuracy 0.345000\n",
      "iteration 1090 / 1500: loss 2.029420, accuracy 0.347000\n",
      "iteration 1091 / 1500: loss 2.052091, accuracy 0.312000\n",
      "iteration 1092 / 1500: loss 2.024728, accuracy 0.366000\n",
      "iteration 1093 / 1500: loss 2.045052, accuracy 0.369000\n",
      "iteration 1094 / 1500: loss 2.052244, accuracy 0.339000\n",
      "iteration 1095 / 1500: loss 2.065638, accuracy 0.349000\n",
      "iteration 1096 / 1500: loss 2.030871, accuracy 0.354000\n",
      "iteration 1097 / 1500: loss 2.021421, accuracy 0.352000\n",
      "iteration 1098 / 1500: loss 2.049575, accuracy 0.340000\n",
      "iteration 1099 / 1500: loss 2.021371, accuracy 0.339000\n",
      "iteration 1100 / 1500: loss 2.047825, accuracy 0.353000\n",
      "iteration 1101 / 1500: loss 2.030668, accuracy 0.352000\n",
      "iteration 1102 / 1500: loss 2.014122, accuracy 0.336000\n",
      "iteration 1103 / 1500: loss 2.040541, accuracy 0.342000\n",
      "iteration 1104 / 1500: loss 2.008885, accuracy 0.367000\n",
      "iteration 1105 / 1500: loss 2.050934, accuracy 0.329000\n",
      "iteration 1106 / 1500: loss 2.005966, accuracy 0.362000\n",
      "iteration 1107 / 1500: loss 2.032336, accuracy 0.348000\n",
      "iteration 1108 / 1500: loss 2.033941, accuracy 0.339000\n",
      "iteration 1109 / 1500: loss 2.034823, accuracy 0.346000\n",
      "iteration 1110 / 1500: loss 2.072226, accuracy 0.325000\n",
      "iteration 1111 / 1500: loss 2.002963, accuracy 0.365000\n",
      "iteration 1112 / 1500: loss 2.012703, accuracy 0.362000\n",
      "iteration 1113 / 1500: loss 2.045004, accuracy 0.338000\n",
      "iteration 1114 / 1500: loss 2.073560, accuracy 0.323000\n",
      "iteration 1115 / 1500: loss 2.026199, accuracy 0.347000\n",
      "iteration 1116 / 1500: loss 2.016411, accuracy 0.368000\n",
      "iteration 1117 / 1500: loss 2.014648, accuracy 0.348000\n",
      "iteration 1118 / 1500: loss 2.027508, accuracy 0.353000\n",
      "iteration 1119 / 1500: loss 2.042578, accuracy 0.361000\n",
      "iteration 1120 / 1500: loss 2.027407, accuracy 0.347000\n",
      "iteration 1121 / 1500: loss 2.040039, accuracy 0.349000\n",
      "iteration 1122 / 1500: loss 2.028901, accuracy 0.335000\n",
      "iteration 1123 / 1500: loss 2.066682, accuracy 0.326000\n",
      "iteration 1124 / 1500: loss 2.010583, accuracy 0.364000\n",
      "iteration 1125 / 1500: loss 2.042799, accuracy 0.321000\n",
      "iteration 1126 / 1500: loss 2.053992, accuracy 0.338000\n",
      "iteration 1127 / 1500: loss 2.055441, accuracy 0.327000\n",
      "iteration 1128 / 1500: loss 2.074152, accuracy 0.308000\n",
      "iteration 1129 / 1500: loss 2.026874, accuracy 0.329000\n",
      "iteration 1130 / 1500: loss 2.036403, accuracy 0.346000\n",
      "iteration 1131 / 1500: loss 2.017275, accuracy 0.376000\n",
      "iteration 1132 / 1500: loss 2.038580, accuracy 0.347000\n",
      "iteration 1133 / 1500: loss 2.063551, accuracy 0.340000\n",
      "iteration 1134 / 1500: loss 2.080237, accuracy 0.325000\n",
      "iteration 1135 / 1500: loss 2.037641, accuracy 0.358000\n",
      "iteration 1136 / 1500: loss 2.051117, accuracy 0.363000\n",
      "iteration 1137 / 1500: loss 2.040731, accuracy 0.345000\n",
      "iteration 1138 / 1500: loss 2.023468, accuracy 0.364000\n",
      "iteration 1139 / 1500: loss 2.038645, accuracy 0.343000\n",
      "iteration 1140 / 1500: loss 2.066107, accuracy 0.341000\n",
      "iteration 1141 / 1500: loss 2.032986, accuracy 0.367000\n",
      "iteration 1142 / 1500: loss 2.052541, accuracy 0.349000\n",
      "iteration 1143 / 1500: loss 2.043189, accuracy 0.353000\n",
      "iteration 1144 / 1500: loss 2.072916, accuracy 0.318000\n",
      "iteration 1145 / 1500: loss 2.037591, accuracy 0.349000\n",
      "iteration 1146 / 1500: loss 2.026609, accuracy 0.356000\n",
      "iteration 1147 / 1500: loss 2.039430, accuracy 0.337000\n",
      "iteration 1148 / 1500: loss 2.018694, accuracy 0.385000\n",
      "iteration 1149 / 1500: loss 2.026153, accuracy 0.345000\n",
      "iteration 1150 / 1500: loss 2.041334, accuracy 0.359000\n",
      "iteration 1151 / 1500: loss 2.060165, accuracy 0.333000\n",
      "iteration 1152 / 1500: loss 2.103596, accuracy 0.304000\n",
      "iteration 1153 / 1500: loss 2.020841, accuracy 0.371000\n",
      "iteration 1154 / 1500: loss 2.048711, accuracy 0.357000\n",
      "iteration 1155 / 1500: loss 2.035936, accuracy 0.351000\n",
      "iteration 1156 / 1500: loss 2.041131, accuracy 0.340000\n",
      "iteration 1157 / 1500: loss 2.041920, accuracy 0.348000\n",
      "iteration 1158 / 1500: loss 2.037091, accuracy 0.331000\n",
      "iteration 1159 / 1500: loss 2.029946, accuracy 0.358000\n",
      "iteration 1160 / 1500: loss 2.052521, accuracy 0.362000\n",
      "iteration 1161 / 1500: loss 2.067147, accuracy 0.330000\n",
      "iteration 1162 / 1500: loss 2.065538, accuracy 0.323000\n",
      "iteration 1163 / 1500: loss 2.048361, accuracy 0.346000\n",
      "iteration 1164 / 1500: loss 2.042951, accuracy 0.351000\n",
      "iteration 1165 / 1500: loss 2.064694, accuracy 0.324000\n",
      "iteration 1166 / 1500: loss 2.041310, accuracy 0.362000\n",
      "iteration 1167 / 1500: loss 2.037044, accuracy 0.367000\n",
      "iteration 1168 / 1500: loss 2.038817, accuracy 0.364000\n",
      "iteration 1169 / 1500: loss 2.072287, accuracy 0.308000\n",
      "iteration 1170 / 1500: loss 2.078719, accuracy 0.337000\n",
      "iteration 1171 / 1500: loss 2.064463, accuracy 0.346000\n",
      "iteration 1172 / 1500: loss 2.045210, accuracy 0.337000\n",
      "iteration 1173 / 1500: loss 2.038587, accuracy 0.332000\n",
      "iteration 1174 / 1500: loss 2.016126, accuracy 0.346000\n",
      "iteration 1175 / 1500: loss 2.030844, accuracy 0.366000\n",
      "iteration 1176 / 1500: loss 2.029433, accuracy 0.369000\n",
      "iteration 1177 / 1500: loss 2.031917, accuracy 0.366000\n",
      "iteration 1178 / 1500: loss 2.067586, accuracy 0.330000\n",
      "iteration 1179 / 1500: loss 2.064954, accuracy 0.335000\n",
      "iteration 1180 / 1500: loss 2.066990, accuracy 0.320000\n",
      "iteration 1181 / 1500: loss 2.004151, accuracy 0.366000\n",
      "iteration 1182 / 1500: loss 2.019437, accuracy 0.347000\n",
      "iteration 1183 / 1500: loss 2.041701, accuracy 0.367000\n",
      "iteration 1184 / 1500: loss 2.040708, accuracy 0.344000\n",
      "iteration 1185 / 1500: loss 2.025232, accuracy 0.366000\n",
      "iteration 1186 / 1500: loss 2.015753, accuracy 0.355000\n",
      "iteration 1187 / 1500: loss 2.046545, accuracy 0.351000\n",
      "iteration 1188 / 1500: loss 2.054090, accuracy 0.356000\n",
      "iteration 1189 / 1500: loss 2.033615, accuracy 0.364000\n",
      "iteration 1190 / 1500: loss 2.062283, accuracy 0.334000\n",
      "iteration 1191 / 1500: loss 2.052219, accuracy 0.331000\n",
      "iteration 1192 / 1500: loss 2.029283, accuracy 0.357000\n",
      "iteration 1193 / 1500: loss 2.031490, accuracy 0.337000\n",
      "iteration 1194 / 1500: loss 2.039240, accuracy 0.348000\n",
      "iteration 1195 / 1500: loss 2.058265, accuracy 0.340000\n",
      "iteration 1196 / 1500: loss 2.035898, accuracy 0.353000\n",
      "iteration 1197 / 1500: loss 2.033147, accuracy 0.320000\n",
      "iteration 1198 / 1500: loss 2.039762, accuracy 0.337000\n",
      "iteration 1199 / 1500: loss 2.046041, accuracy 0.339000\n",
      "iteration 1200 / 1500: loss 2.073773, accuracy 0.329000\n",
      "iteration 1201 / 1500: loss 2.040431, accuracy 0.353000\n",
      "iteration 1202 / 1500: loss 2.038254, accuracy 0.348000\n",
      "iteration 1203 / 1500: loss 2.044007, accuracy 0.351000\n",
      "iteration 1204 / 1500: loss 2.035010, accuracy 0.360000\n",
      "iteration 1205 / 1500: loss 2.028462, accuracy 0.361000\n",
      "iteration 1206 / 1500: loss 2.046722, accuracy 0.356000\n",
      "iteration 1207 / 1500: loss 2.062705, accuracy 0.310000\n",
      "iteration 1208 / 1500: loss 2.017532, accuracy 0.355000\n",
      "iteration 1209 / 1500: loss 2.042133, accuracy 0.351000\n",
      "iteration 1210 / 1500: loss 2.069130, accuracy 0.332000\n",
      "iteration 1211 / 1500: loss 2.016843, accuracy 0.367000\n",
      "iteration 1212 / 1500: loss 2.020970, accuracy 0.352000\n",
      "iteration 1213 / 1500: loss 2.041423, accuracy 0.360000\n",
      "iteration 1214 / 1500: loss 2.028690, accuracy 0.358000\n",
      "iteration 1215 / 1500: loss 2.024879, accuracy 0.351000\n",
      "iteration 1216 / 1500: loss 2.033689, accuracy 0.349000\n",
      "iteration 1217 / 1500: loss 2.043110, accuracy 0.357000\n",
      "iteration 1218 / 1500: loss 2.014342, accuracy 0.376000\n",
      "iteration 1219 / 1500: loss 2.040463, accuracy 0.337000\n",
      "iteration 1220 / 1500: loss 2.026671, accuracy 0.346000\n",
      "iteration 1221 / 1500: loss 2.016602, accuracy 0.370000\n",
      "iteration 1222 / 1500: loss 2.036965, accuracy 0.351000\n",
      "iteration 1223 / 1500: loss 2.066782, accuracy 0.335000\n",
      "iteration 1224 / 1500: loss 2.074944, accuracy 0.324000\n",
      "iteration 1225 / 1500: loss 2.026738, accuracy 0.352000\n",
      "iteration 1226 / 1500: loss 2.043560, accuracy 0.329000\n",
      "iteration 1227 / 1500: loss 2.023117, accuracy 0.357000\n",
      "iteration 1228 / 1500: loss 2.062031, accuracy 0.330000\n",
      "iteration 1229 / 1500: loss 2.043146, accuracy 0.372000\n",
      "iteration 1230 / 1500: loss 2.031378, accuracy 0.353000\n",
      "iteration 1231 / 1500: loss 2.043049, accuracy 0.331000\n",
      "iteration 1232 / 1500: loss 2.032096, accuracy 0.353000\n",
      "iteration 1233 / 1500: loss 2.036582, accuracy 0.327000\n",
      "iteration 1234 / 1500: loss 2.018429, accuracy 0.376000\n",
      "iteration 1235 / 1500: loss 2.042240, accuracy 0.337000\n",
      "iteration 1236 / 1500: loss 2.015262, accuracy 0.379000\n",
      "iteration 1237 / 1500: loss 2.001337, accuracy 0.377000\n",
      "iteration 1238 / 1500: loss 2.044717, accuracy 0.358000\n",
      "iteration 1239 / 1500: loss 2.032961, accuracy 0.354000\n",
      "iteration 1240 / 1500: loss 2.062123, accuracy 0.317000\n",
      "iteration 1241 / 1500: loss 2.052543, accuracy 0.327000\n",
      "iteration 1242 / 1500: loss 2.038282, accuracy 0.340000\n",
      "iteration 1243 / 1500: loss 2.058628, accuracy 0.356000\n",
      "iteration 1244 / 1500: loss 2.026732, accuracy 0.342000\n",
      "iteration 1245 / 1500: loss 2.014005, accuracy 0.366000\n",
      "iteration 1246 / 1500: loss 2.032897, accuracy 0.369000\n",
      "iteration 1247 / 1500: loss 2.047859, accuracy 0.343000\n",
      "iteration 1248 / 1500: loss 2.006730, accuracy 0.357000\n",
      "iteration 1249 / 1500: loss 2.023139, accuracy 0.379000\n",
      "iteration 1250 / 1500: loss 2.052204, accuracy 0.336000\n",
      "iteration 1251 / 1500: loss 2.055565, accuracy 0.324000\n",
      "iteration 1252 / 1500: loss 2.023741, accuracy 0.369000\n",
      "iteration 1253 / 1500: loss 2.040974, accuracy 0.355000\n",
      "iteration 1254 / 1500: loss 2.046462, accuracy 0.343000\n",
      "iteration 1255 / 1500: loss 2.016178, accuracy 0.366000\n",
      "iteration 1256 / 1500: loss 2.059795, accuracy 0.340000\n",
      "iteration 1257 / 1500: loss 2.005515, accuracy 0.333000\n",
      "iteration 1258 / 1500: loss 2.033190, accuracy 0.348000\n",
      "iteration 1259 / 1500: loss 2.055655, accuracy 0.322000\n",
      "iteration 1260 / 1500: loss 2.063173, accuracy 0.322000\n",
      "iteration 1261 / 1500: loss 2.077929, accuracy 0.318000\n",
      "iteration 1262 / 1500: loss 2.033045, accuracy 0.345000\n",
      "iteration 1263 / 1500: loss 2.032890, accuracy 0.346000\n",
      "iteration 1264 / 1500: loss 1.993640, accuracy 0.366000\n",
      "iteration 1265 / 1500: loss 2.051587, accuracy 0.331000\n",
      "iteration 1266 / 1500: loss 2.040626, accuracy 0.335000\n",
      "iteration 1267 / 1500: loss 2.069354, accuracy 0.332000\n",
      "iteration 1268 / 1500: loss 2.033920, accuracy 0.353000\n",
      "iteration 1269 / 1500: loss 2.032605, accuracy 0.353000\n",
      "iteration 1270 / 1500: loss 2.051421, accuracy 0.343000\n",
      "iteration 1271 / 1500: loss 2.037932, accuracy 0.360000\n",
      "iteration 1272 / 1500: loss 2.041495, accuracy 0.344000\n",
      "iteration 1273 / 1500: loss 2.019990, accuracy 0.348000\n",
      "iteration 1274 / 1500: loss 2.034908, accuracy 0.334000\n",
      "iteration 1275 / 1500: loss 2.045463, accuracy 0.325000\n",
      "iteration 1276 / 1500: loss 2.026417, accuracy 0.370000\n",
      "iteration 1277 / 1500: loss 2.090829, accuracy 0.308000\n",
      "iteration 1278 / 1500: loss 2.047575, accuracy 0.329000\n",
      "iteration 1279 / 1500: loss 2.021417, accuracy 0.391000\n",
      "iteration 1280 / 1500: loss 2.032464, accuracy 0.352000\n",
      "iteration 1281 / 1500: loss 2.020466, accuracy 0.370000\n",
      "iteration 1282 / 1500: loss 2.017686, accuracy 0.371000\n",
      "iteration 1283 / 1500: loss 2.050039, accuracy 0.369000\n",
      "iteration 1284 / 1500: loss 2.047814, accuracy 0.333000\n",
      "iteration 1285 / 1500: loss 2.057119, accuracy 0.338000\n",
      "iteration 1286 / 1500: loss 2.040436, accuracy 0.358000\n",
      "iteration 1287 / 1500: loss 2.034021, accuracy 0.350000\n",
      "iteration 1288 / 1500: loss 2.065485, accuracy 0.324000\n",
      "iteration 1289 / 1500: loss 2.045646, accuracy 0.323000\n",
      "iteration 1290 / 1500: loss 2.050999, accuracy 0.330000\n",
      "iteration 1291 / 1500: loss 2.016889, accuracy 0.366000\n",
      "iteration 1292 / 1500: loss 2.018289, accuracy 0.368000\n",
      "iteration 1293 / 1500: loss 2.044969, accuracy 0.344000\n",
      "iteration 1294 / 1500: loss 2.074970, accuracy 0.327000\n",
      "iteration 1295 / 1500: loss 2.025248, accuracy 0.349000\n",
      "iteration 1296 / 1500: loss 2.023110, accuracy 0.354000\n",
      "iteration 1297 / 1500: loss 2.045673, accuracy 0.346000\n",
      "iteration 1298 / 1500: loss 2.015440, accuracy 0.366000\n",
      "iteration 1299 / 1500: loss 2.048927, accuracy 0.337000\n",
      "iteration 1300 / 1500: loss 2.007524, accuracy 0.377000\n",
      "iteration 1301 / 1500: loss 2.068471, accuracy 0.343000\n",
      "iteration 1302 / 1500: loss 2.046063, accuracy 0.355000\n",
      "iteration 1303 / 1500: loss 2.038088, accuracy 0.332000\n",
      "iteration 1304 / 1500: loss 2.036586, accuracy 0.349000\n",
      "iteration 1305 / 1500: loss 2.008686, accuracy 0.374000\n",
      "iteration 1306 / 1500: loss 2.050395, accuracy 0.344000\n",
      "iteration 1307 / 1500: loss 2.020401, accuracy 0.353000\n",
      "iteration 1308 / 1500: loss 2.033731, accuracy 0.340000\n",
      "iteration 1309 / 1500: loss 2.016348, accuracy 0.353000\n",
      "iteration 1310 / 1500: loss 2.074132, accuracy 0.344000\n",
      "iteration 1311 / 1500: loss 2.033121, accuracy 0.367000\n",
      "iteration 1312 / 1500: loss 2.027208, accuracy 0.382000\n",
      "iteration 1313 / 1500: loss 2.033911, accuracy 0.353000\n",
      "iteration 1314 / 1500: loss 2.037757, accuracy 0.342000\n",
      "iteration 1315 / 1500: loss 1.978109, accuracy 0.388000\n",
      "iteration 1316 / 1500: loss 2.041574, accuracy 0.341000\n",
      "iteration 1317 / 1500: loss 2.033065, accuracy 0.348000\n",
      "iteration 1318 / 1500: loss 2.053161, accuracy 0.347000\n",
      "iteration 1319 / 1500: loss 2.066151, accuracy 0.323000\n",
      "iteration 1320 / 1500: loss 2.051334, accuracy 0.350000\n",
      "iteration 1321 / 1500: loss 2.026980, accuracy 0.358000\n",
      "iteration 1322 / 1500: loss 2.037288, accuracy 0.355000\n",
      "iteration 1323 / 1500: loss 2.045247, accuracy 0.348000\n",
      "iteration 1324 / 1500: loss 2.044480, accuracy 0.332000\n",
      "iteration 1325 / 1500: loss 2.054599, accuracy 0.331000\n",
      "iteration 1326 / 1500: loss 2.038800, accuracy 0.339000\n",
      "iteration 1327 / 1500: loss 2.049238, accuracy 0.363000\n",
      "iteration 1328 / 1500: loss 2.055343, accuracy 0.325000\n",
      "iteration 1329 / 1500: loss 2.041439, accuracy 0.350000\n",
      "iteration 1330 / 1500: loss 2.039515, accuracy 0.343000\n",
      "iteration 1331 / 1500: loss 2.051299, accuracy 0.343000\n",
      "iteration 1332 / 1500: loss 2.034281, accuracy 0.339000\n",
      "iteration 1333 / 1500: loss 1.995220, accuracy 0.355000\n",
      "iteration 1334 / 1500: loss 2.037544, accuracy 0.336000\n",
      "iteration 1335 / 1500: loss 2.060330, accuracy 0.340000\n",
      "iteration 1336 / 1500: loss 2.031905, accuracy 0.336000\n",
      "iteration 1337 / 1500: loss 2.073851, accuracy 0.328000\n",
      "iteration 1338 / 1500: loss 1.988316, accuracy 0.352000\n",
      "iteration 1339 / 1500: loss 2.020607, accuracy 0.337000\n",
      "iteration 1340 / 1500: loss 2.044752, accuracy 0.325000\n",
      "iteration 1341 / 1500: loss 2.019434, accuracy 0.366000\n",
      "iteration 1342 / 1500: loss 2.045776, accuracy 0.328000\n",
      "iteration 1343 / 1500: loss 2.031571, accuracy 0.335000\n",
      "iteration 1344 / 1500: loss 2.041360, accuracy 0.342000\n",
      "iteration 1345 / 1500: loss 2.023823, accuracy 0.356000\n",
      "iteration 1346 / 1500: loss 2.027600, accuracy 0.361000\n",
      "iteration 1347 / 1500: loss 2.041250, accuracy 0.340000\n",
      "iteration 1348 / 1500: loss 2.069297, accuracy 0.328000\n",
      "iteration 1349 / 1500: loss 2.019217, accuracy 0.346000\n",
      "iteration 1350 / 1500: loss 2.044018, accuracy 0.344000\n",
      "iteration 1351 / 1500: loss 2.021102, accuracy 0.351000\n",
      "iteration 1352 / 1500: loss 2.042464, accuracy 0.358000\n",
      "iteration 1353 / 1500: loss 2.044557, accuracy 0.344000\n",
      "iteration 1354 / 1500: loss 2.015512, accuracy 0.359000\n",
      "iteration 1355 / 1500: loss 2.034885, accuracy 0.370000\n",
      "iteration 1356 / 1500: loss 2.056866, accuracy 0.342000\n",
      "iteration 1357 / 1500: loss 2.030023, accuracy 0.355000\n",
      "iteration 1358 / 1500: loss 2.055008, accuracy 0.342000\n",
      "iteration 1359 / 1500: loss 2.018656, accuracy 0.356000\n",
      "iteration 1360 / 1500: loss 2.016031, accuracy 0.352000\n",
      "iteration 1361 / 1500: loss 2.018860, accuracy 0.364000\n",
      "iteration 1362 / 1500: loss 2.062758, accuracy 0.325000\n",
      "iteration 1363 / 1500: loss 2.057596, accuracy 0.350000\n",
      "iteration 1364 / 1500: loss 2.011060, accuracy 0.350000\n",
      "iteration 1365 / 1500: loss 2.043458, accuracy 0.315000\n",
      "iteration 1366 / 1500: loss 2.038360, accuracy 0.366000\n",
      "iteration 1367 / 1500: loss 2.061355, accuracy 0.346000\n",
      "iteration 1368 / 1500: loss 2.061073, accuracy 0.328000\n",
      "iteration 1369 / 1500: loss 2.032498, accuracy 0.341000\n",
      "iteration 1370 / 1500: loss 2.044358, accuracy 0.337000\n",
      "iteration 1371 / 1500: loss 2.032184, accuracy 0.364000\n",
      "iteration 1372 / 1500: loss 2.070725, accuracy 0.315000\n",
      "iteration 1373 / 1500: loss 2.035549, accuracy 0.354000\n",
      "iteration 1374 / 1500: loss 2.063657, accuracy 0.332000\n",
      "iteration 1375 / 1500: loss 2.021696, accuracy 0.342000\n",
      "iteration 1376 / 1500: loss 2.017307, accuracy 0.371000\n",
      "iteration 1377 / 1500: loss 2.031307, accuracy 0.360000\n",
      "iteration 1378 / 1500: loss 2.033536, accuracy 0.364000\n",
      "iteration 1379 / 1500: loss 2.024273, accuracy 0.375000\n",
      "iteration 1380 / 1500: loss 2.055587, accuracy 0.326000\n",
      "iteration 1381 / 1500: loss 2.048923, accuracy 0.324000\n",
      "iteration 1382 / 1500: loss 2.056059, accuracy 0.331000\n",
      "iteration 1383 / 1500: loss 2.054934, accuracy 0.324000\n",
      "iteration 1384 / 1500: loss 2.054811, accuracy 0.346000\n",
      "iteration 1385 / 1500: loss 2.022956, accuracy 0.346000\n",
      "iteration 1386 / 1500: loss 2.071211, accuracy 0.335000\n",
      "iteration 1387 / 1500: loss 2.025510, accuracy 0.356000\n",
      "iteration 1388 / 1500: loss 2.028030, accuracy 0.361000\n",
      "iteration 1389 / 1500: loss 2.058701, accuracy 0.330000\n",
      "iteration 1390 / 1500: loss 2.036113, accuracy 0.336000\n",
      "iteration 1391 / 1500: loss 2.010154, accuracy 0.361000\n",
      "iteration 1392 / 1500: loss 2.007336, accuracy 0.360000\n",
      "iteration 1393 / 1500: loss 2.025123, accuracy 0.357000\n",
      "iteration 1394 / 1500: loss 2.051099, accuracy 0.348000\n",
      "iteration 1395 / 1500: loss 2.025104, accuracy 0.364000\n",
      "iteration 1396 / 1500: loss 2.039110, accuracy 0.357000\n",
      "iteration 1397 / 1500: loss 2.013004, accuracy 0.358000\n",
      "iteration 1398 / 1500: loss 2.054535, accuracy 0.345000\n",
      "iteration 1399 / 1500: loss 2.046741, accuracy 0.350000\n",
      "iteration 1400 / 1500: loss 2.063025, accuracy 0.327000\n",
      "iteration 1401 / 1500: loss 2.057720, accuracy 0.342000\n",
      "iteration 1402 / 1500: loss 2.026778, accuracy 0.359000\n",
      "iteration 1403 / 1500: loss 2.038052, accuracy 0.337000\n",
      "iteration 1404 / 1500: loss 2.033604, accuracy 0.354000\n",
      "iteration 1405 / 1500: loss 2.016629, accuracy 0.355000\n",
      "iteration 1406 / 1500: loss 2.040512, accuracy 0.336000\n",
      "iteration 1407 / 1500: loss 2.017456, accuracy 0.354000\n",
      "iteration 1408 / 1500: loss 2.054123, accuracy 0.349000\n",
      "iteration 1409 / 1500: loss 2.073318, accuracy 0.313000\n",
      "iteration 1410 / 1500: loss 2.071013, accuracy 0.329000\n",
      "iteration 1411 / 1500: loss 2.033543, accuracy 0.337000\n",
      "iteration 1412 / 1500: loss 2.022988, accuracy 0.363000\n",
      "iteration 1413 / 1500: loss 2.026859, accuracy 0.347000\n",
      "iteration 1414 / 1500: loss 2.055446, accuracy 0.342000\n",
      "iteration 1415 / 1500: loss 2.023779, accuracy 0.357000\n",
      "iteration 1416 / 1500: loss 2.046688, accuracy 0.334000\n",
      "iteration 1417 / 1500: loss 2.047436, accuracy 0.349000\n",
      "iteration 1418 / 1500: loss 2.042599, accuracy 0.341000\n",
      "iteration 1419 / 1500: loss 2.045726, accuracy 0.345000\n",
      "iteration 1420 / 1500: loss 2.059137, accuracy 0.326000\n",
      "iteration 1421 / 1500: loss 2.059450, accuracy 0.330000\n",
      "iteration 1422 / 1500: loss 2.014069, accuracy 0.368000\n",
      "iteration 1423 / 1500: loss 2.052492, accuracy 0.338000\n",
      "iteration 1424 / 1500: loss 2.089002, accuracy 0.294000\n",
      "iteration 1425 / 1500: loss 2.030920, accuracy 0.349000\n",
      "iteration 1426 / 1500: loss 2.050938, accuracy 0.337000\n",
      "iteration 1427 / 1500: loss 2.027984, accuracy 0.351000\n",
      "iteration 1428 / 1500: loss 2.057241, accuracy 0.320000\n",
      "iteration 1429 / 1500: loss 2.022884, accuracy 0.360000\n",
      "iteration 1430 / 1500: loss 2.035445, accuracy 0.354000\n",
      "iteration 1431 / 1500: loss 2.063052, accuracy 0.344000\n",
      "iteration 1432 / 1500: loss 2.014693, accuracy 0.359000\n",
      "iteration 1433 / 1500: loss 2.056490, accuracy 0.339000\n",
      "iteration 1434 / 1500: loss 2.031292, accuracy 0.367000\n",
      "iteration 1435 / 1500: loss 2.041744, accuracy 0.343000\n",
      "iteration 1436 / 1500: loss 2.074081, accuracy 0.326000\n",
      "iteration 1437 / 1500: loss 2.043163, accuracy 0.359000\n",
      "iteration 1438 / 1500: loss 2.070458, accuracy 0.335000\n",
      "iteration 1439 / 1500: loss 2.047674, accuracy 0.340000\n",
      "iteration 1440 / 1500: loss 2.034994, accuracy 0.367000\n",
      "iteration 1441 / 1500: loss 2.029954, accuracy 0.356000\n",
      "iteration 1442 / 1500: loss 2.028671, accuracy 0.369000\n",
      "iteration 1443 / 1500: loss 2.025056, accuracy 0.357000\n",
      "iteration 1444 / 1500: loss 2.051906, accuracy 0.345000\n",
      "iteration 1445 / 1500: loss 2.013559, accuracy 0.352000\n",
      "iteration 1446 / 1500: loss 2.014047, accuracy 0.364000\n",
      "iteration 1447 / 1500: loss 2.010460, accuracy 0.342000\n",
      "iteration 1448 / 1500: loss 2.037032, accuracy 0.323000\n",
      "iteration 1449 / 1500: loss 2.059609, accuracy 0.340000\n",
      "iteration 1450 / 1500: loss 2.062135, accuracy 0.328000\n",
      "iteration 1451 / 1500: loss 2.022328, accuracy 0.340000\n",
      "iteration 1452 / 1500: loss 2.024137, accuracy 0.358000\n",
      "iteration 1453 / 1500: loss 2.038049, accuracy 0.342000\n",
      "iteration 1454 / 1500: loss 2.053484, accuracy 0.349000\n",
      "iteration 1455 / 1500: loss 2.026526, accuracy 0.360000\n",
      "iteration 1456 / 1500: loss 2.017535, accuracy 0.365000\n",
      "iteration 1457 / 1500: loss 2.040886, accuracy 0.346000\n",
      "iteration 1458 / 1500: loss 2.014459, accuracy 0.374000\n",
      "iteration 1459 / 1500: loss 2.069688, accuracy 0.321000\n",
      "iteration 1460 / 1500: loss 2.037222, accuracy 0.341000\n",
      "iteration 1461 / 1500: loss 2.030705, accuracy 0.340000\n",
      "iteration 1462 / 1500: loss 2.042321, accuracy 0.351000\n",
      "iteration 1463 / 1500: loss 2.047467, accuracy 0.332000\n",
      "iteration 1464 / 1500: loss 2.023804, accuracy 0.341000\n",
      "iteration 1465 / 1500: loss 2.034008, accuracy 0.356000\n",
      "iteration 1466 / 1500: loss 2.036587, accuracy 0.338000\n",
      "iteration 1467 / 1500: loss 2.030927, accuracy 0.338000\n",
      "iteration 1468 / 1500: loss 2.049845, accuracy 0.341000\n",
      "iteration 1469 / 1500: loss 2.023959, accuracy 0.365000\n",
      "iteration 1470 / 1500: loss 2.030855, accuracy 0.381000\n",
      "iteration 1471 / 1500: loss 2.044917, accuracy 0.347000\n",
      "iteration 1472 / 1500: loss 2.061417, accuracy 0.321000\n",
      "iteration 1473 / 1500: loss 2.057164, accuracy 0.345000\n",
      "iteration 1474 / 1500: loss 2.057889, accuracy 0.353000\n",
      "iteration 1475 / 1500: loss 2.048100, accuracy 0.363000\n",
      "iteration 1476 / 1500: loss 2.028364, accuracy 0.332000\n",
      "iteration 1477 / 1500: loss 2.067376, accuracy 0.339000\n",
      "iteration 1478 / 1500: loss 2.041409, accuracy 0.346000\n",
      "iteration 1479 / 1500: loss 2.035136, accuracy 0.349000\n",
      "iteration 1480 / 1500: loss 2.062847, accuracy 0.354000\n",
      "iteration 1481 / 1500: loss 2.035176, accuracy 0.346000\n",
      "iteration 1482 / 1500: loss 2.060178, accuracy 0.330000\n",
      "iteration 1483 / 1500: loss 2.044120, accuracy 0.358000\n",
      "iteration 1484 / 1500: loss 2.053346, accuracy 0.347000\n",
      "iteration 1485 / 1500: loss 2.035573, accuracy 0.353000\n",
      "iteration 1486 / 1500: loss 2.036410, accuracy 0.366000\n",
      "iteration 1487 / 1500: loss 2.034507, accuracy 0.354000\n",
      "iteration 1488 / 1500: loss 2.025799, accuracy 0.372000\n",
      "iteration 1489 / 1500: loss 2.044200, accuracy 0.340000\n",
      "iteration 1490 / 1500: loss 2.023065, accuracy 0.357000\n",
      "iteration 1491 / 1500: loss 2.028368, accuracy 0.339000\n",
      "iteration 1492 / 1500: loss 2.054696, accuracy 0.353000\n",
      "iteration 1493 / 1500: loss 2.032215, accuracy 0.355000\n",
      "iteration 1494 / 1500: loss 2.027578, accuracy 0.351000\n",
      "iteration 1495 / 1500: loss 2.049794, accuracy 0.359000\n",
      "iteration 1496 / 1500: loss 2.020334, accuracy 0.365000\n",
      "iteration 1497 / 1500: loss 2.069185, accuracy 0.325000\n",
      "iteration 1498 / 1500: loss 2.067767, accuracy 0.331000\n",
      "iteration 1499 / 1500: loss 2.033401, accuracy 0.354000\n",
      "iteration 0 / 1500: loss 507.274110, accuracy 0.119000\n",
      "iteration 1 / 1500: loss 504.802520, accuracy 0.132000\n",
      "iteration 2 / 1500: loss 502.335216, accuracy 0.115000\n",
      "iteration 3 / 1500: loss 499.955401, accuracy 0.133000\n",
      "iteration 4 / 1500: loss 497.483705, accuracy 0.131000\n",
      "iteration 5 / 1500: loss 494.987333, accuracy 0.135000\n",
      "iteration 6 / 1500: loss 492.453180, accuracy 0.130000\n",
      "iteration 7 / 1500: loss 490.425487, accuracy 0.101000\n",
      "iteration 8 / 1500: loss 487.688586, accuracy 0.128000\n",
      "iteration 9 / 1500: loss 485.277173, accuracy 0.148000\n",
      "iteration 10 / 1500: loss 482.986071, accuracy 0.147000\n",
      "iteration 11 / 1500: loss 480.522744, accuracy 0.130000\n",
      "iteration 12 / 1500: loss 478.478128, accuracy 0.110000\n",
      "iteration 13 / 1500: loss 475.971487, accuracy 0.128000\n",
      "iteration 14 / 1500: loss 473.475716, accuracy 0.117000\n",
      "iteration 15 / 1500: loss 471.345623, accuracy 0.123000\n",
      "iteration 16 / 1500: loss 468.947608, accuracy 0.150000\n",
      "iteration 17 / 1500: loss 466.744318, accuracy 0.125000\n",
      "iteration 18 / 1500: loss 464.232962, accuracy 0.149000\n",
      "iteration 19 / 1500: loss 462.075136, accuracy 0.142000\n",
      "iteration 20 / 1500: loss 459.935774, accuracy 0.144000\n",
      "iteration 21 / 1500: loss 457.644131, accuracy 0.137000\n",
      "iteration 22 / 1500: loss 455.473322, accuracy 0.128000\n",
      "iteration 23 / 1500: loss 453.066917, accuracy 0.138000\n",
      "iteration 24 / 1500: loss 450.908120, accuracy 0.132000\n",
      "iteration 25 / 1500: loss 448.767861, accuracy 0.126000\n",
      "iteration 26 / 1500: loss 446.694894, accuracy 0.115000\n",
      "iteration 27 / 1500: loss 444.361784, accuracy 0.123000\n",
      "iteration 28 / 1500: loss 442.073708, accuracy 0.141000\n",
      "iteration 29 / 1500: loss 440.091442, accuracy 0.129000\n",
      "iteration 30 / 1500: loss 437.867252, accuracy 0.141000\n",
      "iteration 31 / 1500: loss 435.731423, accuracy 0.122000\n",
      "iteration 32 / 1500: loss 433.762062, accuracy 0.123000\n",
      "iteration 33 / 1500: loss 431.627405, accuracy 0.105000\n",
      "iteration 34 / 1500: loss 429.289747, accuracy 0.166000\n",
      "iteration 35 / 1500: loss 427.288342, accuracy 0.132000\n",
      "iteration 36 / 1500: loss 425.251617, accuracy 0.140000\n",
      "iteration 37 / 1500: loss 423.180061, accuracy 0.132000\n",
      "iteration 38 / 1500: loss 421.140880, accuracy 0.138000\n",
      "iteration 39 / 1500: loss 418.856990, accuracy 0.157000\n",
      "iteration 40 / 1500: loss 417.000028, accuracy 0.148000\n",
      "iteration 41 / 1500: loss 414.899060, accuracy 0.130000\n",
      "iteration 42 / 1500: loss 412.866564, accuracy 0.138000\n",
      "iteration 43 / 1500: loss 410.977708, accuracy 0.132000\n",
      "iteration 44 / 1500: loss 409.042434, accuracy 0.128000\n",
      "iteration 45 / 1500: loss 406.803492, accuracy 0.126000\n",
      "iteration 46 / 1500: loss 404.752382, accuracy 0.155000\n",
      "iteration 47 / 1500: loss 402.855812, accuracy 0.128000\n",
      "iteration 48 / 1500: loss 401.041242, accuracy 0.134000\n",
      "iteration 49 / 1500: loss 398.983090, accuracy 0.149000\n",
      "iteration 50 / 1500: loss 397.189143, accuracy 0.138000\n",
      "iteration 51 / 1500: loss 395.075993, accuracy 0.139000\n",
      "iteration 52 / 1500: loss 393.167485, accuracy 0.141000\n",
      "iteration 53 / 1500: loss 391.250898, accuracy 0.148000\n",
      "iteration 54 / 1500: loss 389.372809, accuracy 0.142000\n",
      "iteration 55 / 1500: loss 387.523671, accuracy 0.140000\n",
      "iteration 56 / 1500: loss 385.625281, accuracy 0.134000\n",
      "iteration 57 / 1500: loss 383.689377, accuracy 0.136000\n",
      "iteration 58 / 1500: loss 381.956531, accuracy 0.149000\n",
      "iteration 59 / 1500: loss 380.032924, accuracy 0.141000\n",
      "iteration 60 / 1500: loss 378.174638, accuracy 0.150000\n",
      "iteration 61 / 1500: loss 376.441457, accuracy 0.131000\n",
      "iteration 62 / 1500: loss 374.400399, accuracy 0.140000\n",
      "iteration 63 / 1500: loss 372.631477, accuracy 0.150000\n",
      "iteration 64 / 1500: loss 370.744726, accuracy 0.164000\n",
      "iteration 65 / 1500: loss 368.984390, accuracy 0.137000\n",
      "iteration 66 / 1500: loss 367.235904, accuracy 0.138000\n",
      "iteration 67 / 1500: loss 365.399749, accuracy 0.149000\n",
      "iteration 68 / 1500: loss 363.547831, accuracy 0.140000\n",
      "iteration 69 / 1500: loss 361.874253, accuracy 0.143000\n",
      "iteration 70 / 1500: loss 360.216736, accuracy 0.136000\n",
      "iteration 71 / 1500: loss 358.328936, accuracy 0.145000\n",
      "iteration 72 / 1500: loss 356.722340, accuracy 0.138000\n",
      "iteration 73 / 1500: loss 354.916615, accuracy 0.154000\n",
      "iteration 74 / 1500: loss 353.169600, accuracy 0.144000\n",
      "iteration 75 / 1500: loss 351.417569, accuracy 0.138000\n",
      "iteration 76 / 1500: loss 349.719784, accuracy 0.127000\n",
      "iteration 77 / 1500: loss 348.036058, accuracy 0.148000\n",
      "iteration 78 / 1500: loss 346.291038, accuracy 0.170000\n",
      "iteration 79 / 1500: loss 344.581857, accuracy 0.171000\n",
      "iteration 80 / 1500: loss 343.110852, accuracy 0.140000\n",
      "iteration 81 / 1500: loss 341.406962, accuracy 0.144000\n",
      "iteration 82 / 1500: loss 339.527561, accuracy 0.153000\n",
      "iteration 83 / 1500: loss 337.997492, accuracy 0.160000\n",
      "iteration 84 / 1500: loss 336.387617, accuracy 0.140000\n",
      "iteration 85 / 1500: loss 334.762366, accuracy 0.142000\n",
      "iteration 86 / 1500: loss 333.094738, accuracy 0.172000\n",
      "iteration 87 / 1500: loss 331.539710, accuracy 0.144000\n",
      "iteration 88 / 1500: loss 329.829103, accuracy 0.152000\n",
      "iteration 89 / 1500: loss 328.238911, accuracy 0.152000\n",
      "iteration 90 / 1500: loss 326.708582, accuracy 0.159000\n",
      "iteration 91 / 1500: loss 325.121001, accuracy 0.157000\n",
      "iteration 92 / 1500: loss 323.401766, accuracy 0.182000\n",
      "iteration 93 / 1500: loss 321.891805, accuracy 0.158000\n",
      "iteration 94 / 1500: loss 320.432525, accuracy 0.148000\n",
      "iteration 95 / 1500: loss 318.732285, accuracy 0.160000\n",
      "iteration 96 / 1500: loss 317.314901, accuracy 0.143000\n",
      "iteration 97 / 1500: loss 315.825456, accuracy 0.156000\n",
      "iteration 98 / 1500: loss 314.108298, accuracy 0.149000\n",
      "iteration 99 / 1500: loss 312.781500, accuracy 0.163000\n",
      "iteration 100 / 1500: loss 311.080124, accuracy 0.154000\n",
      "iteration 101 / 1500: loss 309.636688, accuracy 0.169000\n",
      "iteration 102 / 1500: loss 308.083064, accuracy 0.151000\n",
      "iteration 103 / 1500: loss 306.610924, accuracy 0.159000\n",
      "iteration 104 / 1500: loss 305.207933, accuracy 0.178000\n",
      "iteration 105 / 1500: loss 303.634668, accuracy 0.164000\n",
      "iteration 106 / 1500: loss 302.194883, accuracy 0.154000\n",
      "iteration 107 / 1500: loss 300.704602, accuracy 0.151000\n",
      "iteration 108 / 1500: loss 299.208181, accuracy 0.153000\n",
      "iteration 109 / 1500: loss 297.851575, accuracy 0.142000\n",
      "iteration 110 / 1500: loss 296.444390, accuracy 0.136000\n",
      "iteration 111 / 1500: loss 294.882719, accuracy 0.167000\n",
      "iteration 112 / 1500: loss 293.440191, accuracy 0.152000\n",
      "iteration 113 / 1500: loss 292.078682, accuracy 0.163000\n",
      "iteration 114 / 1500: loss 290.564091, accuracy 0.167000\n",
      "iteration 115 / 1500: loss 289.231304, accuracy 0.160000\n",
      "iteration 116 / 1500: loss 287.773848, accuracy 0.183000\n",
      "iteration 117 / 1500: loss 286.407433, accuracy 0.151000\n",
      "iteration 118 / 1500: loss 285.069073, accuracy 0.142000\n",
      "iteration 119 / 1500: loss 283.523960, accuracy 0.191000\n",
      "iteration 120 / 1500: loss 282.361274, accuracy 0.145000\n",
      "iteration 121 / 1500: loss 280.880701, accuracy 0.161000\n",
      "iteration 122 / 1500: loss 279.522588, accuracy 0.140000\n",
      "iteration 123 / 1500: loss 278.215213, accuracy 0.170000\n",
      "iteration 124 / 1500: loss 276.862720, accuracy 0.147000\n",
      "iteration 125 / 1500: loss 275.476012, accuracy 0.175000\n",
      "iteration 126 / 1500: loss 274.061479, accuracy 0.164000\n",
      "iteration 127 / 1500: loss 272.903727, accuracy 0.169000\n",
      "iteration 128 / 1500: loss 271.591294, accuracy 0.150000\n",
      "iteration 129 / 1500: loss 270.226750, accuracy 0.161000\n",
      "iteration 130 / 1500: loss 268.801669, accuracy 0.179000\n",
      "iteration 131 / 1500: loss 267.503400, accuracy 0.169000\n",
      "iteration 132 / 1500: loss 266.309437, accuracy 0.159000\n",
      "iteration 133 / 1500: loss 264.999693, accuracy 0.161000\n",
      "iteration 134 / 1500: loss 263.694794, accuracy 0.142000\n",
      "iteration 135 / 1500: loss 262.444159, accuracy 0.155000\n",
      "iteration 136 / 1500: loss 261.191216, accuracy 0.163000\n",
      "iteration 137 / 1500: loss 259.905635, accuracy 0.148000\n",
      "iteration 138 / 1500: loss 258.656980, accuracy 0.154000\n",
      "iteration 139 / 1500: loss 257.322643, accuracy 0.160000\n",
      "iteration 140 / 1500: loss 256.157361, accuracy 0.134000\n",
      "iteration 141 / 1500: loss 254.889097, accuracy 0.176000\n",
      "iteration 142 / 1500: loss 253.560231, accuracy 0.168000\n",
      "iteration 143 / 1500: loss 252.290571, accuracy 0.196000\n",
      "iteration 144 / 1500: loss 251.190933, accuracy 0.169000\n",
      "iteration 145 / 1500: loss 249.882183, accuracy 0.174000\n",
      "iteration 146 / 1500: loss 248.804639, accuracy 0.165000\n",
      "iteration 147 / 1500: loss 247.451276, accuracy 0.170000\n",
      "iteration 148 / 1500: loss 246.382345, accuracy 0.165000\n",
      "iteration 149 / 1500: loss 245.159490, accuracy 0.160000\n",
      "iteration 150 / 1500: loss 243.880801, accuracy 0.172000\n",
      "iteration 151 / 1500: loss 242.774795, accuracy 0.180000\n",
      "iteration 152 / 1500: loss 241.546677, accuracy 0.157000\n",
      "iteration 153 / 1500: loss 240.485535, accuracy 0.161000\n",
      "iteration 154 / 1500: loss 239.268775, accuracy 0.173000\n",
      "iteration 155 / 1500: loss 238.065074, accuracy 0.157000\n",
      "iteration 156 / 1500: loss 237.021376, accuracy 0.167000\n",
      "iteration 157 / 1500: loss 235.864892, accuracy 0.145000\n",
      "iteration 158 / 1500: loss 234.738355, accuracy 0.155000\n",
      "iteration 159 / 1500: loss 233.547537, accuracy 0.152000\n",
      "iteration 160 / 1500: loss 232.362957, accuracy 0.182000\n",
      "iteration 161 / 1500: loss 231.317876, accuracy 0.171000\n",
      "iteration 162 / 1500: loss 230.174385, accuracy 0.159000\n",
      "iteration 163 / 1500: loss 229.077178, accuracy 0.157000\n",
      "iteration 164 / 1500: loss 227.901558, accuracy 0.184000\n",
      "iteration 165 / 1500: loss 226.860570, accuracy 0.172000\n",
      "iteration 166 / 1500: loss 225.692149, accuracy 0.174000\n",
      "iteration 167 / 1500: loss 224.650816, accuracy 0.171000\n",
      "iteration 168 / 1500: loss 223.574064, accuracy 0.175000\n",
      "iteration 169 / 1500: loss 222.481594, accuracy 0.167000\n",
      "iteration 170 / 1500: loss 221.352690, accuracy 0.172000\n",
      "iteration 171 / 1500: loss 220.277202, accuracy 0.154000\n",
      "iteration 172 / 1500: loss 219.119054, accuracy 0.162000\n",
      "iteration 173 / 1500: loss 218.144685, accuracy 0.188000\n",
      "iteration 174 / 1500: loss 217.106398, accuracy 0.181000\n",
      "iteration 175 / 1500: loss 216.108400, accuracy 0.171000\n",
      "iteration 176 / 1500: loss 214.983422, accuracy 0.184000\n",
      "iteration 177 / 1500: loss 214.018499, accuracy 0.167000\n",
      "iteration 178 / 1500: loss 212.838469, accuracy 0.192000\n",
      "iteration 179 / 1500: loss 211.947914, accuracy 0.173000\n",
      "iteration 180 / 1500: loss 210.836542, accuracy 0.183000\n",
      "iteration 181 / 1500: loss 209.851352, accuracy 0.187000\n",
      "iteration 182 / 1500: loss 208.835844, accuracy 0.162000\n",
      "iteration 183 / 1500: loss 207.882491, accuracy 0.164000\n",
      "iteration 184 / 1500: loss 206.784565, accuracy 0.176000\n",
      "iteration 185 / 1500: loss 205.799981, accuracy 0.167000\n",
      "iteration 186 / 1500: loss 204.761621, accuracy 0.202000\n",
      "iteration 187 / 1500: loss 203.770363, accuracy 0.190000\n",
      "iteration 188 / 1500: loss 202.876018, accuracy 0.172000\n",
      "iteration 189 / 1500: loss 201.951836, accuracy 0.161000\n",
      "iteration 190 / 1500: loss 200.916504, accuracy 0.161000\n",
      "iteration 191 / 1500: loss 199.902291, accuracy 0.188000\n",
      "iteration 192 / 1500: loss 199.003631, accuracy 0.154000\n",
      "iteration 193 / 1500: loss 197.913872, accuracy 0.179000\n",
      "iteration 194 / 1500: loss 197.051040, accuracy 0.177000\n",
      "iteration 195 / 1500: loss 196.095724, accuracy 0.176000\n",
      "iteration 196 / 1500: loss 195.147507, accuracy 0.170000\n",
      "iteration 197 / 1500: loss 194.210214, accuracy 0.161000\n",
      "iteration 198 / 1500: loss 193.317417, accuracy 0.160000\n",
      "iteration 199 / 1500: loss 192.398255, accuracy 0.169000\n",
      "iteration 200 / 1500: loss 191.385121, accuracy 0.181000\n",
      "iteration 201 / 1500: loss 190.518636, accuracy 0.182000\n",
      "iteration 202 / 1500: loss 189.510047, accuracy 0.191000\n",
      "iteration 203 / 1500: loss 188.561346, accuracy 0.176000\n",
      "iteration 204 / 1500: loss 187.695958, accuracy 0.194000\n",
      "iteration 205 / 1500: loss 186.805931, accuracy 0.188000\n",
      "iteration 206 / 1500: loss 185.907813, accuracy 0.173000\n",
      "iteration 207 / 1500: loss 185.080487, accuracy 0.173000\n",
      "iteration 208 / 1500: loss 184.053229, accuracy 0.175000\n",
      "iteration 209 / 1500: loss 183.316707, accuracy 0.162000\n",
      "iteration 210 / 1500: loss 182.393071, accuracy 0.182000\n",
      "iteration 211 / 1500: loss 181.491058, accuracy 0.191000\n",
      "iteration 212 / 1500: loss 180.564914, accuracy 0.172000\n",
      "iteration 213 / 1500: loss 179.637874, accuracy 0.211000\n",
      "iteration 214 / 1500: loss 178.847379, accuracy 0.160000\n",
      "iteration 215 / 1500: loss 177.945958, accuracy 0.181000\n",
      "iteration 216 / 1500: loss 177.022558, accuracy 0.212000\n",
      "iteration 217 / 1500: loss 176.260952, accuracy 0.188000\n",
      "iteration 218 / 1500: loss 175.454780, accuracy 0.172000\n",
      "iteration 219 / 1500: loss 174.603403, accuracy 0.173000\n",
      "iteration 220 / 1500: loss 173.701478, accuracy 0.179000\n",
      "iteration 221 / 1500: loss 172.859527, accuracy 0.177000\n",
      "iteration 222 / 1500: loss 172.054260, accuracy 0.186000\n",
      "iteration 223 / 1500: loss 171.185084, accuracy 0.176000\n",
      "iteration 224 / 1500: loss 170.367286, accuracy 0.191000\n",
      "iteration 225 / 1500: loss 169.547831, accuracy 0.196000\n",
      "iteration 226 / 1500: loss 168.754666, accuracy 0.189000\n",
      "iteration 227 / 1500: loss 167.927524, accuracy 0.178000\n",
      "iteration 228 / 1500: loss 167.175316, accuracy 0.177000\n",
      "iteration 229 / 1500: loss 166.363233, accuracy 0.185000\n",
      "iteration 230 / 1500: loss 165.428629, accuracy 0.198000\n",
      "iteration 231 / 1500: loss 164.693456, accuracy 0.183000\n",
      "iteration 232 / 1500: loss 163.907638, accuracy 0.186000\n",
      "iteration 233 / 1500: loss 163.098718, accuracy 0.180000\n",
      "iteration 234 / 1500: loss 162.285969, accuracy 0.195000\n",
      "iteration 235 / 1500: loss 161.541059, accuracy 0.197000\n",
      "iteration 236 / 1500: loss 160.752672, accuracy 0.172000\n",
      "iteration 237 / 1500: loss 159.956731, accuracy 0.201000\n",
      "iteration 238 / 1500: loss 159.196861, accuracy 0.200000\n",
      "iteration 239 / 1500: loss 158.431364, accuracy 0.189000\n",
      "iteration 240 / 1500: loss 157.745890, accuracy 0.183000\n",
      "iteration 241 / 1500: loss 156.925756, accuracy 0.172000\n",
      "iteration 242 / 1500: loss 156.191813, accuracy 0.199000\n",
      "iteration 243 / 1500: loss 155.451457, accuracy 0.184000\n",
      "iteration 244 / 1500: loss 154.719513, accuracy 0.181000\n",
      "iteration 245 / 1500: loss 153.912601, accuracy 0.175000\n",
      "iteration 246 / 1500: loss 153.177814, accuracy 0.185000\n",
      "iteration 247 / 1500: loss 152.411883, accuracy 0.214000\n",
      "iteration 248 / 1500: loss 151.789152, accuracy 0.174000\n",
      "iteration 249 / 1500: loss 151.083184, accuracy 0.163000\n",
      "iteration 250 / 1500: loss 150.233157, accuracy 0.220000\n",
      "iteration 251 / 1500: loss 149.477847, accuracy 0.208000\n",
      "iteration 252 / 1500: loss 148.758218, accuracy 0.216000\n",
      "iteration 253 / 1500: loss 148.125480, accuracy 0.184000\n",
      "iteration 254 / 1500: loss 147.355643, accuracy 0.200000\n",
      "iteration 255 / 1500: loss 146.696947, accuracy 0.193000\n",
      "iteration 256 / 1500: loss 145.967099, accuracy 0.191000\n",
      "iteration 257 / 1500: loss 145.186254, accuracy 0.202000\n",
      "iteration 258 / 1500: loss 144.617912, accuracy 0.175000\n",
      "iteration 259 / 1500: loss 143.908739, accuracy 0.197000\n",
      "iteration 260 / 1500: loss 143.078468, accuracy 0.198000\n",
      "iteration 261 / 1500: loss 142.436472, accuracy 0.189000\n",
      "iteration 262 / 1500: loss 141.724819, accuracy 0.201000\n",
      "iteration 263 / 1500: loss 141.108100, accuracy 0.191000\n",
      "iteration 264 / 1500: loss 140.433478, accuracy 0.199000\n",
      "iteration 265 / 1500: loss 139.855329, accuracy 0.195000\n",
      "iteration 266 / 1500: loss 139.086072, accuracy 0.190000\n",
      "iteration 267 / 1500: loss 138.386842, accuracy 0.203000\n",
      "iteration 268 / 1500: loss 137.746647, accuracy 0.201000\n",
      "iteration 269 / 1500: loss 137.045530, accuracy 0.187000\n",
      "iteration 270 / 1500: loss 136.433851, accuracy 0.189000\n",
      "iteration 271 / 1500: loss 135.764147, accuracy 0.198000\n",
      "iteration 272 / 1500: loss 135.113816, accuracy 0.194000\n",
      "iteration 273 / 1500: loss 134.458193, accuracy 0.187000\n",
      "iteration 274 / 1500: loss 133.840561, accuracy 0.193000\n",
      "iteration 275 / 1500: loss 133.125840, accuracy 0.207000\n",
      "iteration 276 / 1500: loss 132.499046, accuracy 0.196000\n",
      "iteration 277 / 1500: loss 131.889407, accuracy 0.187000\n",
      "iteration 278 / 1500: loss 131.247604, accuracy 0.201000\n",
      "iteration 279 / 1500: loss 130.598862, accuracy 0.200000\n",
      "iteration 280 / 1500: loss 129.946090, accuracy 0.179000\n",
      "iteration 281 / 1500: loss 129.441353, accuracy 0.182000\n",
      "iteration 282 / 1500: loss 128.770201, accuracy 0.183000\n",
      "iteration 283 / 1500: loss 128.086897, accuracy 0.206000\n",
      "iteration 284 / 1500: loss 127.518475, accuracy 0.204000\n",
      "iteration 285 / 1500: loss 126.894999, accuracy 0.210000\n",
      "iteration 286 / 1500: loss 126.319812, accuracy 0.171000\n",
      "iteration 287 / 1500: loss 125.637369, accuracy 0.216000\n",
      "iteration 288 / 1500: loss 125.048766, accuracy 0.209000\n",
      "iteration 289 / 1500: loss 124.513004, accuracy 0.184000\n",
      "iteration 290 / 1500: loss 123.922813, accuracy 0.190000\n",
      "iteration 291 / 1500: loss 123.235529, accuracy 0.213000\n",
      "iteration 292 / 1500: loss 122.612988, accuracy 0.209000\n",
      "iteration 293 / 1500: loss 122.171055, accuracy 0.186000\n",
      "iteration 294 / 1500: loss 121.542055, accuracy 0.198000\n",
      "iteration 295 / 1500: loss 120.918639, accuracy 0.201000\n",
      "iteration 296 / 1500: loss 120.352294, accuracy 0.187000\n",
      "iteration 297 / 1500: loss 119.806072, accuracy 0.189000\n",
      "iteration 298 / 1500: loss 119.171957, accuracy 0.206000\n",
      "iteration 299 / 1500: loss 118.586475, accuracy 0.199000\n",
      "iteration 300 / 1500: loss 118.050598, accuracy 0.229000\n",
      "iteration 301 / 1500: loss 117.444603, accuracy 0.199000\n",
      "iteration 302 / 1500: loss 116.884877, accuracy 0.230000\n",
      "iteration 303 / 1500: loss 116.336812, accuracy 0.188000\n",
      "iteration 304 / 1500: loss 115.808346, accuracy 0.181000\n",
      "iteration 305 / 1500: loss 115.265137, accuracy 0.184000\n",
      "iteration 306 / 1500: loss 114.691371, accuracy 0.193000\n",
      "iteration 307 / 1500: loss 114.110414, accuracy 0.217000\n",
      "iteration 308 / 1500: loss 113.547736, accuracy 0.207000\n",
      "iteration 309 / 1500: loss 113.013839, accuracy 0.212000\n",
      "iteration 310 / 1500: loss 112.503378, accuracy 0.203000\n",
      "iteration 311 / 1500: loss 111.931746, accuracy 0.230000\n",
      "iteration 312 / 1500: loss 111.449029, accuracy 0.196000\n",
      "iteration 313 / 1500: loss 110.898137, accuracy 0.202000\n",
      "iteration 314 / 1500: loss 110.361709, accuracy 0.232000\n",
      "iteration 315 / 1500: loss 109.797465, accuracy 0.213000\n",
      "iteration 316 / 1500: loss 109.336257, accuracy 0.194000\n",
      "iteration 317 / 1500: loss 108.832001, accuracy 0.174000\n",
      "iteration 318 / 1500: loss 108.281041, accuracy 0.219000\n",
      "iteration 319 / 1500: loss 107.696431, accuracy 0.213000\n",
      "iteration 320 / 1500: loss 107.206166, accuracy 0.207000\n",
      "iteration 321 / 1500: loss 106.702096, accuracy 0.215000\n",
      "iteration 322 / 1500: loss 106.162036, accuracy 0.210000\n",
      "iteration 323 / 1500: loss 105.729191, accuracy 0.187000\n",
      "iteration 324 / 1500: loss 105.166227, accuracy 0.202000\n",
      "iteration 325 / 1500: loss 104.649164, accuracy 0.243000\n",
      "iteration 326 / 1500: loss 104.133637, accuracy 0.236000\n",
      "iteration 327 / 1500: loss 103.699742, accuracy 0.215000\n",
      "iteration 328 / 1500: loss 103.158147, accuracy 0.206000\n",
      "iteration 329 / 1500: loss 102.668547, accuracy 0.214000\n",
      "iteration 330 / 1500: loss 102.183068, accuracy 0.224000\n",
      "iteration 331 / 1500: loss 101.708867, accuracy 0.213000\n",
      "iteration 332 / 1500: loss 101.202003, accuracy 0.203000\n",
      "iteration 333 / 1500: loss 100.735731, accuracy 0.199000\n",
      "iteration 334 / 1500: loss 100.313425, accuracy 0.204000\n",
      "iteration 335 / 1500: loss 99.799612, accuracy 0.197000\n",
      "iteration 336 / 1500: loss 99.318680, accuracy 0.207000\n",
      "iteration 337 / 1500: loss 98.782127, accuracy 0.213000\n",
      "iteration 338 / 1500: loss 98.370710, accuracy 0.209000\n",
      "iteration 339 / 1500: loss 97.926197, accuracy 0.213000\n",
      "iteration 340 / 1500: loss 97.457411, accuracy 0.211000\n",
      "iteration 341 / 1500: loss 96.967169, accuracy 0.207000\n",
      "iteration 342 / 1500: loss 96.450379, accuracy 0.229000\n",
      "iteration 343 / 1500: loss 95.996173, accuracy 0.223000\n",
      "iteration 344 / 1500: loss 95.530667, accuracy 0.226000\n",
      "iteration 345 / 1500: loss 95.101596, accuracy 0.233000\n",
      "iteration 346 / 1500: loss 94.627275, accuracy 0.224000\n",
      "iteration 347 / 1500: loss 94.206425, accuracy 0.208000\n",
      "iteration 348 / 1500: loss 93.762165, accuracy 0.196000\n",
      "iteration 349 / 1500: loss 93.306375, accuracy 0.198000\n",
      "iteration 350 / 1500: loss 92.854634, accuracy 0.212000\n",
      "iteration 351 / 1500: loss 92.422645, accuracy 0.212000\n",
      "iteration 352 / 1500: loss 91.922054, accuracy 0.220000\n",
      "iteration 353 / 1500: loss 91.535456, accuracy 0.225000\n",
      "iteration 354 / 1500: loss 91.085068, accuracy 0.212000\n",
      "iteration 355 / 1500: loss 90.681367, accuracy 0.218000\n",
      "iteration 356 / 1500: loss 90.273772, accuracy 0.226000\n",
      "iteration 357 / 1500: loss 89.767304, accuracy 0.221000\n",
      "iteration 358 / 1500: loss 89.391794, accuracy 0.219000\n",
      "iteration 359 / 1500: loss 88.950860, accuracy 0.210000\n",
      "iteration 360 / 1500: loss 88.557458, accuracy 0.200000\n",
      "iteration 361 / 1500: loss 88.058190, accuracy 0.217000\n",
      "iteration 362 / 1500: loss 87.680695, accuracy 0.196000\n",
      "iteration 363 / 1500: loss 87.192453, accuracy 0.236000\n",
      "iteration 364 / 1500: loss 86.828980, accuracy 0.204000\n",
      "iteration 365 / 1500: loss 86.412869, accuracy 0.201000\n",
      "iteration 366 / 1500: loss 86.012664, accuracy 0.217000\n",
      "iteration 367 / 1500: loss 85.619073, accuracy 0.224000\n",
      "iteration 368 / 1500: loss 85.200377, accuracy 0.214000\n",
      "iteration 369 / 1500: loss 84.807590, accuracy 0.219000\n",
      "iteration 370 / 1500: loss 84.363804, accuracy 0.227000\n",
      "iteration 371 / 1500: loss 83.968877, accuracy 0.209000\n",
      "iteration 372 / 1500: loss 83.589632, accuracy 0.217000\n",
      "iteration 373 / 1500: loss 83.170606, accuracy 0.231000\n",
      "iteration 374 / 1500: loss 82.724402, accuracy 0.226000\n",
      "iteration 375 / 1500: loss 82.400343, accuracy 0.205000\n",
      "iteration 376 / 1500: loss 81.981157, accuracy 0.202000\n",
      "iteration 377 / 1500: loss 81.496602, accuracy 0.242000\n",
      "iteration 378 / 1500: loss 81.250852, accuracy 0.206000\n",
      "iteration 379 / 1500: loss 80.850392, accuracy 0.216000\n",
      "iteration 380 / 1500: loss 80.422020, accuracy 0.212000\n",
      "iteration 381 / 1500: loss 80.081432, accuracy 0.203000\n",
      "iteration 382 / 1500: loss 79.669537, accuracy 0.219000\n",
      "iteration 383 / 1500: loss 79.316095, accuracy 0.208000\n",
      "iteration 384 / 1500: loss 78.922359, accuracy 0.208000\n",
      "iteration 385 / 1500: loss 78.596754, accuracy 0.207000\n",
      "iteration 386 / 1500: loss 78.165059, accuracy 0.235000\n",
      "iteration 387 / 1500: loss 77.838806, accuracy 0.222000\n",
      "iteration 388 / 1500: loss 77.450766, accuracy 0.199000\n",
      "iteration 389 / 1500: loss 77.054845, accuracy 0.239000\n",
      "iteration 390 / 1500: loss 76.697367, accuracy 0.211000\n",
      "iteration 391 / 1500: loss 76.377900, accuracy 0.202000\n",
      "iteration 392 / 1500: loss 75.942603, accuracy 0.248000\n",
      "iteration 393 / 1500: loss 75.582425, accuracy 0.226000\n",
      "iteration 394 / 1500: loss 75.214036, accuracy 0.221000\n",
      "iteration 395 / 1500: loss 74.897926, accuracy 0.223000\n",
      "iteration 396 / 1500: loss 74.504765, accuracy 0.223000\n",
      "iteration 397 / 1500: loss 74.171449, accuracy 0.240000\n",
      "iteration 398 / 1500: loss 73.846354, accuracy 0.212000\n",
      "iteration 399 / 1500: loss 73.486584, accuracy 0.222000\n",
      "iteration 400 / 1500: loss 73.158022, accuracy 0.204000\n",
      "iteration 401 / 1500: loss 72.781667, accuracy 0.217000\n",
      "iteration 402 / 1500: loss 72.416843, accuracy 0.225000\n",
      "iteration 403 / 1500: loss 72.108589, accuracy 0.221000\n",
      "iteration 404 / 1500: loss 71.683090, accuracy 0.258000\n",
      "iteration 405 / 1500: loss 71.417578, accuracy 0.211000\n",
      "iteration 406 / 1500: loss 71.067516, accuracy 0.229000\n",
      "iteration 407 / 1500: loss 70.684924, accuracy 0.233000\n",
      "iteration 408 / 1500: loss 70.367511, accuracy 0.247000\n",
      "iteration 409 / 1500: loss 70.084447, accuracy 0.228000\n",
      "iteration 410 / 1500: loss 69.709728, accuracy 0.231000\n",
      "iteration 411 / 1500: loss 69.402089, accuracy 0.226000\n",
      "iteration 412 / 1500: loss 69.046365, accuracy 0.235000\n",
      "iteration 413 / 1500: loss 68.731548, accuracy 0.246000\n",
      "iteration 414 / 1500: loss 68.389333, accuracy 0.231000\n",
      "iteration 415 / 1500: loss 68.078155, accuracy 0.221000\n",
      "iteration 416 / 1500: loss 67.731119, accuracy 0.256000\n",
      "iteration 417 / 1500: loss 67.398997, accuracy 0.248000\n",
      "iteration 418 / 1500: loss 67.094391, accuracy 0.235000\n",
      "iteration 419 / 1500: loss 66.807562, accuracy 0.234000\n",
      "iteration 420 / 1500: loss 66.454356, accuracy 0.231000\n",
      "iteration 421 / 1500: loss 66.199347, accuracy 0.240000\n",
      "iteration 422 / 1500: loss 65.847449, accuracy 0.235000\n",
      "iteration 423 / 1500: loss 65.577359, accuracy 0.217000\n",
      "iteration 424 / 1500: loss 65.250877, accuracy 0.232000\n",
      "iteration 425 / 1500: loss 64.908365, accuracy 0.230000\n",
      "iteration 426 / 1500: loss 64.667345, accuracy 0.246000\n",
      "iteration 427 / 1500: loss 64.282121, accuracy 0.237000\n",
      "iteration 428 / 1500: loss 63.994097, accuracy 0.250000\n",
      "iteration 429 / 1500: loss 63.702981, accuracy 0.230000\n",
      "iteration 430 / 1500: loss 63.367433, accuracy 0.272000\n",
      "iteration 431 / 1500: loss 63.119082, accuracy 0.242000\n",
      "iteration 432 / 1500: loss 62.850586, accuracy 0.241000\n",
      "iteration 433 / 1500: loss 62.552952, accuracy 0.214000\n",
      "iteration 434 / 1500: loss 62.221128, accuracy 0.230000\n",
      "iteration 435 / 1500: loss 61.913039, accuracy 0.258000\n",
      "iteration 436 / 1500: loss 61.642262, accuracy 0.222000\n",
      "iteration 437 / 1500: loss 61.365592, accuracy 0.245000\n",
      "iteration 438 / 1500: loss 61.021655, accuracy 0.244000\n",
      "iteration 439 / 1500: loss 60.725736, accuracy 0.265000\n",
      "iteration 440 / 1500: loss 60.461676, accuracy 0.249000\n",
      "iteration 441 / 1500: loss 60.236838, accuracy 0.219000\n",
      "iteration 442 / 1500: loss 59.896196, accuracy 0.243000\n",
      "iteration 443 / 1500: loss 59.600256, accuracy 0.228000\n",
      "iteration 444 / 1500: loss 59.337141, accuracy 0.249000\n",
      "iteration 445 / 1500: loss 59.088262, accuracy 0.235000\n",
      "iteration 446 / 1500: loss 58.780343, accuracy 0.240000\n",
      "iteration 447 / 1500: loss 58.483562, accuracy 0.236000\n",
      "iteration 448 / 1500: loss 58.255530, accuracy 0.232000\n",
      "iteration 449 / 1500: loss 57.961951, accuracy 0.228000\n",
      "iteration 450 / 1500: loss 57.665953, accuracy 0.233000\n",
      "iteration 451 / 1500: loss 57.429105, accuracy 0.229000\n",
      "iteration 452 / 1500: loss 57.165030, accuracy 0.240000\n",
      "iteration 453 / 1500: loss 56.864317, accuracy 0.245000\n",
      "iteration 454 / 1500: loss 56.632270, accuracy 0.221000\n",
      "iteration 455 / 1500: loss 56.317342, accuracy 0.268000\n",
      "iteration 456 / 1500: loss 56.089178, accuracy 0.209000\n",
      "iteration 457 / 1500: loss 55.788597, accuracy 0.261000\n",
      "iteration 458 / 1500: loss 55.546319, accuracy 0.238000\n",
      "iteration 459 / 1500: loss 55.289564, accuracy 0.248000\n",
      "iteration 460 / 1500: loss 55.039177, accuracy 0.229000\n",
      "iteration 461 / 1500: loss 54.748836, accuracy 0.249000\n",
      "iteration 462 / 1500: loss 54.466419, accuracy 0.264000\n",
      "iteration 463 / 1500: loss 54.253111, accuracy 0.258000\n",
      "iteration 464 / 1500: loss 54.012639, accuracy 0.242000\n",
      "iteration 465 / 1500: loss 53.733614, accuracy 0.235000\n",
      "iteration 466 / 1500: loss 53.506948, accuracy 0.222000\n",
      "iteration 467 / 1500: loss 53.252985, accuracy 0.235000\n",
      "iteration 468 / 1500: loss 52.954802, accuracy 0.233000\n",
      "iteration 469 / 1500: loss 52.729073, accuracy 0.242000\n",
      "iteration 470 / 1500: loss 52.501790, accuracy 0.233000\n",
      "iteration 471 / 1500: loss 52.183211, accuracy 0.286000\n",
      "iteration 472 / 1500: loss 52.018688, accuracy 0.233000\n",
      "iteration 473 / 1500: loss 51.735688, accuracy 0.243000\n",
      "iteration 474 / 1500: loss 51.507613, accuracy 0.263000\n",
      "iteration 475 / 1500: loss 51.239955, accuracy 0.269000\n",
      "iteration 476 / 1500: loss 51.024786, accuracy 0.245000\n",
      "iteration 477 / 1500: loss 50.779423, accuracy 0.248000\n",
      "iteration 478 / 1500: loss 50.533130, accuracy 0.250000\n",
      "iteration 479 / 1500: loss 50.303040, accuracy 0.249000\n",
      "iteration 480 / 1500: loss 50.045130, accuracy 0.254000\n",
      "iteration 481 / 1500: loss 49.824819, accuracy 0.254000\n",
      "iteration 482 / 1500: loss 49.583550, accuracy 0.265000\n",
      "iteration 483 / 1500: loss 49.361773, accuracy 0.263000\n",
      "iteration 484 / 1500: loss 49.163991, accuracy 0.218000\n",
      "iteration 485 / 1500: loss 48.894002, accuracy 0.261000\n",
      "iteration 486 / 1500: loss 48.721505, accuracy 0.218000\n",
      "iteration 487 / 1500: loss 48.449415, accuracy 0.252000\n",
      "iteration 488 / 1500: loss 48.247608, accuracy 0.226000\n",
      "iteration 489 / 1500: loss 47.986125, accuracy 0.243000\n",
      "iteration 490 / 1500: loss 47.802330, accuracy 0.231000\n",
      "iteration 491 / 1500: loss 47.574791, accuracy 0.241000\n",
      "iteration 492 / 1500: loss 47.378774, accuracy 0.260000\n",
      "iteration 493 / 1500: loss 47.126139, accuracy 0.251000\n",
      "iteration 494 / 1500: loss 46.862258, accuracy 0.269000\n",
      "iteration 495 / 1500: loss 46.662542, accuracy 0.265000\n",
      "iteration 496 / 1500: loss 46.402449, accuracy 0.280000\n",
      "iteration 497 / 1500: loss 46.242951, accuracy 0.241000\n",
      "iteration 498 / 1500: loss 46.003277, accuracy 0.249000\n",
      "iteration 499 / 1500: loss 45.808388, accuracy 0.240000\n",
      "iteration 500 / 1500: loss 45.606702, accuracy 0.241000\n",
      "iteration 501 / 1500: loss 45.417520, accuracy 0.256000\n",
      "iteration 502 / 1500: loss 45.166750, accuracy 0.247000\n",
      "iteration 503 / 1500: loss 44.975571, accuracy 0.238000\n",
      "iteration 504 / 1500: loss 44.773846, accuracy 0.219000\n",
      "iteration 505 / 1500: loss 44.520230, accuracy 0.261000\n",
      "iteration 506 / 1500: loss 44.364795, accuracy 0.238000\n",
      "iteration 507 / 1500: loss 44.125947, accuracy 0.253000\n",
      "iteration 508 / 1500: loss 43.893386, accuracy 0.266000\n",
      "iteration 509 / 1500: loss 43.714105, accuracy 0.251000\n",
      "iteration 510 / 1500: loss 43.549003, accuracy 0.239000\n",
      "iteration 511 / 1500: loss 43.326754, accuracy 0.229000\n",
      "iteration 512 / 1500: loss 43.145933, accuracy 0.240000\n",
      "iteration 513 / 1500: loss 42.906391, accuracy 0.251000\n",
      "iteration 514 / 1500: loss 42.724458, accuracy 0.257000\n",
      "iteration 515 / 1500: loss 42.519606, accuracy 0.257000\n",
      "iteration 516 / 1500: loss 42.299888, accuracy 0.258000\n",
      "iteration 517 / 1500: loss 42.121660, accuracy 0.251000\n",
      "iteration 518 / 1500: loss 41.889811, accuracy 0.250000\n",
      "iteration 519 / 1500: loss 41.724621, accuracy 0.240000\n",
      "iteration 520 / 1500: loss 41.495242, accuracy 0.261000\n",
      "iteration 521 / 1500: loss 41.329559, accuracy 0.283000\n",
      "iteration 522 / 1500: loss 41.144895, accuracy 0.277000\n",
      "iteration 523 / 1500: loss 40.956806, accuracy 0.254000\n",
      "iteration 524 / 1500: loss 40.824591, accuracy 0.244000\n",
      "iteration 525 / 1500: loss 40.545936, accuracy 0.264000\n",
      "iteration 526 / 1500: loss 40.412350, accuracy 0.253000\n",
      "iteration 527 / 1500: loss 40.210354, accuracy 0.263000\n",
      "iteration 528 / 1500: loss 39.996009, accuracy 0.265000\n",
      "iteration 529 / 1500: loss 39.829541, accuracy 0.250000\n",
      "iteration 530 / 1500: loss 39.656246, accuracy 0.242000\n",
      "iteration 531 / 1500: loss 39.499856, accuracy 0.242000\n",
      "iteration 532 / 1500: loss 39.280419, accuracy 0.245000\n",
      "iteration 533 / 1500: loss 39.097393, accuracy 0.255000\n",
      "iteration 534 / 1500: loss 38.915795, accuracy 0.260000\n",
      "iteration 535 / 1500: loss 38.753478, accuracy 0.256000\n",
      "iteration 536 / 1500: loss 38.580880, accuracy 0.242000\n",
      "iteration 537 / 1500: loss 38.387195, accuracy 0.232000\n",
      "iteration 538 / 1500: loss 38.195617, accuracy 0.263000\n",
      "iteration 539 / 1500: loss 38.058130, accuracy 0.242000\n",
      "iteration 540 / 1500: loss 37.808412, accuracy 0.295000\n",
      "iteration 541 / 1500: loss 37.658117, accuracy 0.276000\n",
      "iteration 542 / 1500: loss 37.479513, accuracy 0.260000\n",
      "iteration 543 / 1500: loss 37.321340, accuracy 0.275000\n",
      "iteration 544 / 1500: loss 37.148721, accuracy 0.265000\n",
      "iteration 545 / 1500: loss 37.008824, accuracy 0.264000\n",
      "iteration 546 / 1500: loss 36.841735, accuracy 0.256000\n",
      "iteration 547 / 1500: loss 36.683314, accuracy 0.255000\n",
      "iteration 548 / 1500: loss 36.521419, accuracy 0.252000\n",
      "iteration 549 / 1500: loss 36.323119, accuracy 0.249000\n",
      "iteration 550 / 1500: loss 36.145710, accuracy 0.260000\n",
      "iteration 551 / 1500: loss 35.986915, accuracy 0.257000\n",
      "iteration 552 / 1500: loss 35.799326, accuracy 0.273000\n",
      "iteration 553 / 1500: loss 35.685430, accuracy 0.256000\n",
      "iteration 554 / 1500: loss 35.447570, accuracy 0.281000\n",
      "iteration 555 / 1500: loss 35.319014, accuracy 0.244000\n",
      "iteration 556 / 1500: loss 35.154049, accuracy 0.261000\n",
      "iteration 557 / 1500: loss 35.032761, accuracy 0.233000\n",
      "iteration 558 / 1500: loss 34.850843, accuracy 0.271000\n",
      "iteration 559 / 1500: loss 34.683270, accuracy 0.266000\n",
      "iteration 560 / 1500: loss 34.468076, accuracy 0.282000\n",
      "iteration 561 / 1500: loss 34.337035, accuracy 0.265000\n",
      "iteration 562 / 1500: loss 34.189180, accuracy 0.253000\n",
      "iteration 563 / 1500: loss 34.047633, accuracy 0.267000\n",
      "iteration 564 / 1500: loss 33.915799, accuracy 0.255000\n",
      "iteration 565 / 1500: loss 33.760988, accuracy 0.246000\n",
      "iteration 566 / 1500: loss 33.562380, accuracy 0.257000\n",
      "iteration 567 / 1500: loss 33.445094, accuracy 0.260000\n",
      "iteration 568 / 1500: loss 33.334059, accuracy 0.221000\n",
      "iteration 569 / 1500: loss 33.110864, accuracy 0.272000\n",
      "iteration 570 / 1500: loss 32.933818, accuracy 0.295000\n",
      "iteration 571 / 1500: loss 32.787944, accuracy 0.290000\n",
      "iteration 572 / 1500: loss 32.727520, accuracy 0.250000\n",
      "iteration 573 / 1500: loss 32.533295, accuracy 0.256000\n",
      "iteration 574 / 1500: loss 32.368790, accuracy 0.263000\n",
      "iteration 575 / 1500: loss 32.205687, accuracy 0.281000\n",
      "iteration 576 / 1500: loss 32.039200, accuracy 0.266000\n",
      "iteration 577 / 1500: loss 31.926788, accuracy 0.270000\n",
      "iteration 578 / 1500: loss 31.837187, accuracy 0.244000\n",
      "iteration 579 / 1500: loss 31.616990, accuracy 0.267000\n",
      "iteration 580 / 1500: loss 31.458476, accuracy 0.291000\n",
      "iteration 581 / 1500: loss 31.367115, accuracy 0.270000\n",
      "iteration 582 / 1500: loss 31.202724, accuracy 0.256000\n",
      "iteration 583 / 1500: loss 31.030565, accuracy 0.275000\n",
      "iteration 584 / 1500: loss 30.902266, accuracy 0.264000\n",
      "iteration 585 / 1500: loss 30.799771, accuracy 0.255000\n",
      "iteration 586 / 1500: loss 30.636788, accuracy 0.266000\n",
      "iteration 587 / 1500: loss 30.494821, accuracy 0.254000\n",
      "iteration 588 / 1500: loss 30.359024, accuracy 0.254000\n",
      "iteration 589 / 1500: loss 30.240409, accuracy 0.245000\n",
      "iteration 590 / 1500: loss 30.085278, accuracy 0.268000\n",
      "iteration 591 / 1500: loss 29.915485, accuracy 0.275000\n",
      "iteration 592 / 1500: loss 29.814304, accuracy 0.285000\n",
      "iteration 593 / 1500: loss 29.684731, accuracy 0.246000\n",
      "iteration 594 / 1500: loss 29.525592, accuracy 0.276000\n",
      "iteration 595 / 1500: loss 29.409485, accuracy 0.258000\n",
      "iteration 596 / 1500: loss 29.277598, accuracy 0.244000\n",
      "iteration 597 / 1500: loss 29.101740, accuracy 0.294000\n",
      "iteration 598 / 1500: loss 28.987090, accuracy 0.288000\n",
      "iteration 599 / 1500: loss 28.851949, accuracy 0.299000\n",
      "iteration 600 / 1500: loss 28.748833, accuracy 0.286000\n",
      "iteration 601 / 1500: loss 28.623733, accuracy 0.270000\n",
      "iteration 602 / 1500: loss 28.507002, accuracy 0.259000\n",
      "iteration 603 / 1500: loss 28.361420, accuracy 0.277000\n",
      "iteration 604 / 1500: loss 28.208088, accuracy 0.277000\n",
      "iteration 605 / 1500: loss 28.063216, accuracy 0.276000\n",
      "iteration 606 / 1500: loss 27.923442, accuracy 0.307000\n",
      "iteration 607 / 1500: loss 27.855390, accuracy 0.263000\n",
      "iteration 608 / 1500: loss 27.727209, accuracy 0.257000\n",
      "iteration 609 / 1500: loss 27.593679, accuracy 0.266000\n",
      "iteration 610 / 1500: loss 27.489977, accuracy 0.252000\n",
      "iteration 611 / 1500: loss 27.317841, accuracy 0.306000\n",
      "iteration 612 / 1500: loss 27.225468, accuracy 0.266000\n",
      "iteration 613 / 1500: loss 27.081287, accuracy 0.296000\n",
      "iteration 614 / 1500: loss 26.979807, accuracy 0.261000\n",
      "iteration 615 / 1500: loss 26.893603, accuracy 0.241000\n",
      "iteration 616 / 1500: loss 26.700170, accuracy 0.292000\n",
      "iteration 617 / 1500: loss 26.634795, accuracy 0.269000\n",
      "iteration 618 / 1500: loss 26.467190, accuracy 0.292000\n",
      "iteration 619 / 1500: loss 26.374414, accuracy 0.256000\n",
      "iteration 620 / 1500: loss 26.251021, accuracy 0.290000\n",
      "iteration 621 / 1500: loss 26.153547, accuracy 0.243000\n",
      "iteration 622 / 1500: loss 26.020027, accuracy 0.259000\n",
      "iteration 623 / 1500: loss 25.892894, accuracy 0.258000\n",
      "iteration 624 / 1500: loss 25.787968, accuracy 0.285000\n",
      "iteration 625 / 1500: loss 25.665387, accuracy 0.302000\n",
      "iteration 626 / 1500: loss 25.560753, accuracy 0.273000\n",
      "iteration 627 / 1500: loss 25.444332, accuracy 0.281000\n",
      "iteration 628 / 1500: loss 25.294429, accuracy 0.288000\n",
      "iteration 629 / 1500: loss 25.229568, accuracy 0.267000\n",
      "iteration 630 / 1500: loss 25.060838, accuracy 0.286000\n",
      "iteration 631 / 1500: loss 24.948945, accuracy 0.293000\n",
      "iteration 632 / 1500: loss 24.886708, accuracy 0.255000\n",
      "iteration 633 / 1500: loss 24.776264, accuracy 0.252000\n",
      "iteration 634 / 1500: loss 24.629102, accuracy 0.297000\n",
      "iteration 635 / 1500: loss 24.548620, accuracy 0.273000\n",
      "iteration 636 / 1500: loss 24.428681, accuracy 0.291000\n",
      "iteration 637 / 1500: loss 24.326096, accuracy 0.272000\n",
      "iteration 638 / 1500: loss 24.170595, accuracy 0.288000\n",
      "iteration 639 / 1500: loss 24.118964, accuracy 0.282000\n",
      "iteration 640 / 1500: loss 24.012440, accuracy 0.281000\n",
      "iteration 641 / 1500: loss 23.853571, accuracy 0.297000\n",
      "iteration 642 / 1500: loss 23.761887, accuracy 0.281000\n",
      "iteration 643 / 1500: loss 23.659899, accuracy 0.283000\n",
      "iteration 644 / 1500: loss 23.545929, accuracy 0.305000\n",
      "iteration 645 / 1500: loss 23.459949, accuracy 0.285000\n",
      "iteration 646 / 1500: loss 23.331858, accuracy 0.284000\n",
      "iteration 647 / 1500: loss 23.281871, accuracy 0.261000\n",
      "iteration 648 / 1500: loss 23.166152, accuracy 0.284000\n",
      "iteration 649 / 1500: loss 23.042192, accuracy 0.286000\n",
      "iteration 650 / 1500: loss 22.943303, accuracy 0.298000\n",
      "iteration 651 / 1500: loss 22.882949, accuracy 0.279000\n",
      "iteration 652 / 1500: loss 22.753429, accuracy 0.285000\n",
      "iteration 653 / 1500: loss 22.625703, accuracy 0.284000\n",
      "iteration 654 / 1500: loss 22.557126, accuracy 0.277000\n",
      "iteration 655 / 1500: loss 22.437476, accuracy 0.306000\n",
      "iteration 656 / 1500: loss 22.317587, accuracy 0.299000\n",
      "iteration 657 / 1500: loss 22.276165, accuracy 0.270000\n",
      "iteration 658 / 1500: loss 22.127444, accuracy 0.293000\n",
      "iteration 659 / 1500: loss 22.081041, accuracy 0.260000\n",
      "iteration 660 / 1500: loss 21.905877, accuracy 0.307000\n",
      "iteration 661 / 1500: loss 21.842343, accuracy 0.296000\n",
      "iteration 662 / 1500: loss 21.761010, accuracy 0.287000\n",
      "iteration 663 / 1500: loss 21.692571, accuracy 0.284000\n",
      "iteration 664 / 1500: loss 21.554280, accuracy 0.296000\n",
      "iteration 665 / 1500: loss 21.507152, accuracy 0.264000\n",
      "iteration 666 / 1500: loss 21.389051, accuracy 0.263000\n",
      "iteration 667 / 1500: loss 21.275940, accuracy 0.293000\n",
      "iteration 668 / 1500: loss 21.164712, accuracy 0.292000\n",
      "iteration 669 / 1500: loss 21.114148, accuracy 0.271000\n",
      "iteration 670 / 1500: loss 21.015194, accuracy 0.288000\n",
      "iteration 671 / 1500: loss 20.912540, accuracy 0.274000\n",
      "iteration 672 / 1500: loss 20.841523, accuracy 0.255000\n",
      "iteration 673 / 1500: loss 20.732279, accuracy 0.258000\n",
      "iteration 674 / 1500: loss 20.636685, accuracy 0.285000\n",
      "iteration 675 / 1500: loss 20.555824, accuracy 0.283000\n",
      "iteration 676 / 1500: loss 20.435684, accuracy 0.287000\n",
      "iteration 677 / 1500: loss 20.364829, accuracy 0.290000\n",
      "iteration 678 / 1500: loss 20.229253, accuracy 0.292000\n",
      "iteration 679 / 1500: loss 20.219899, accuracy 0.252000\n",
      "iteration 680 / 1500: loss 20.075307, accuracy 0.296000\n",
      "iteration 681 / 1500: loss 19.987003, accuracy 0.304000\n",
      "iteration 682 / 1500: loss 19.917288, accuracy 0.285000\n",
      "iteration 683 / 1500: loss 19.833397, accuracy 0.305000\n",
      "iteration 684 / 1500: loss 19.747500, accuracy 0.270000\n",
      "iteration 685 / 1500: loss 19.613729, accuracy 0.300000\n",
      "iteration 686 / 1500: loss 19.531906, accuracy 0.300000\n",
      "iteration 687 / 1500: loss 19.498513, accuracy 0.268000\n",
      "iteration 688 / 1500: loss 19.350692, accuracy 0.327000\n",
      "iteration 689 / 1500: loss 19.282292, accuracy 0.301000\n",
      "iteration 690 / 1500: loss 19.189479, accuracy 0.312000\n",
      "iteration 691 / 1500: loss 19.144255, accuracy 0.288000\n",
      "iteration 692 / 1500: loss 19.053047, accuracy 0.295000\n",
      "iteration 693 / 1500: loss 18.984645, accuracy 0.293000\n",
      "iteration 694 / 1500: loss 18.910240, accuracy 0.294000\n",
      "iteration 695 / 1500: loss 18.821070, accuracy 0.279000\n",
      "iteration 696 / 1500: loss 18.740522, accuracy 0.280000\n",
      "iteration 697 / 1500: loss 18.703051, accuracy 0.270000\n",
      "iteration 698 / 1500: loss 18.587066, accuracy 0.280000\n",
      "iteration 699 / 1500: loss 18.475261, accuracy 0.303000\n",
      "iteration 700 / 1500: loss 18.422121, accuracy 0.275000\n",
      "iteration 701 / 1500: loss 18.347395, accuracy 0.275000\n",
      "iteration 702 / 1500: loss 18.217786, accuracy 0.308000\n",
      "iteration 703 / 1500: loss 18.159725, accuracy 0.284000\n",
      "iteration 704 / 1500: loss 18.080451, accuracy 0.280000\n",
      "iteration 705 / 1500: loss 18.018847, accuracy 0.300000\n",
      "iteration 706 / 1500: loss 17.915228, accuracy 0.287000\n",
      "iteration 707 / 1500: loss 17.858957, accuracy 0.290000\n",
      "iteration 708 / 1500: loss 17.752031, accuracy 0.323000\n",
      "iteration 709 / 1500: loss 17.663445, accuracy 0.298000\n",
      "iteration 710 / 1500: loss 17.604676, accuracy 0.302000\n",
      "iteration 711 / 1500: loss 17.561653, accuracy 0.283000\n",
      "iteration 712 / 1500: loss 17.471405, accuracy 0.281000\n",
      "iteration 713 / 1500: loss 17.384507, accuracy 0.287000\n",
      "iteration 714 / 1500: loss 17.363802, accuracy 0.279000\n",
      "iteration 715 / 1500: loss 17.283121, accuracy 0.266000\n",
      "iteration 716 / 1500: loss 17.147126, accuracy 0.305000\n",
      "iteration 717 / 1500: loss 17.106607, accuracy 0.286000\n",
      "iteration 718 / 1500: loss 17.048420, accuracy 0.288000\n",
      "iteration 719 / 1500: loss 16.931868, accuracy 0.326000\n",
      "iteration 720 / 1500: loss 16.900518, accuracy 0.279000\n",
      "iteration 721 / 1500: loss 16.781939, accuracy 0.301000\n",
      "iteration 722 / 1500: loss 16.740094, accuracy 0.286000\n",
      "iteration 723 / 1500: loss 16.627614, accuracy 0.309000\n",
      "iteration 724 / 1500: loss 16.562139, accuracy 0.311000\n",
      "iteration 725 / 1500: loss 16.481196, accuracy 0.308000\n",
      "iteration 726 / 1500: loss 16.461744, accuracy 0.296000\n",
      "iteration 727 / 1500: loss 16.401759, accuracy 0.281000\n",
      "iteration 728 / 1500: loss 16.295748, accuracy 0.317000\n",
      "iteration 729 / 1500: loss 16.239824, accuracy 0.286000\n",
      "iteration 730 / 1500: loss 16.182268, accuracy 0.286000\n",
      "iteration 731 / 1500: loss 16.101809, accuracy 0.292000\n",
      "iteration 732 / 1500: loss 16.056911, accuracy 0.269000\n",
      "iteration 733 / 1500: loss 15.974812, accuracy 0.319000\n",
      "iteration 734 / 1500: loss 15.922134, accuracy 0.278000\n",
      "iteration 735 / 1500: loss 15.832820, accuracy 0.308000\n",
      "iteration 736 / 1500: loss 15.780463, accuracy 0.284000\n",
      "iteration 737 / 1500: loss 15.701229, accuracy 0.299000\n",
      "iteration 738 / 1500: loss 15.628696, accuracy 0.286000\n",
      "iteration 739 / 1500: loss 15.530456, accuracy 0.317000\n",
      "iteration 740 / 1500: loss 15.470763, accuracy 0.308000\n",
      "iteration 741 / 1500: loss 15.418792, accuracy 0.310000\n",
      "iteration 742 / 1500: loss 15.342666, accuracy 0.322000\n",
      "iteration 743 / 1500: loss 15.278851, accuracy 0.306000\n",
      "iteration 744 / 1500: loss 15.248956, accuracy 0.305000\n",
      "iteration 745 / 1500: loss 15.167396, accuracy 0.306000\n",
      "iteration 746 / 1500: loss 15.069056, accuracy 0.330000\n",
      "iteration 747 / 1500: loss 15.030919, accuracy 0.295000\n",
      "iteration 748 / 1500: loss 14.972296, accuracy 0.306000\n",
      "iteration 749 / 1500: loss 14.915617, accuracy 0.295000\n",
      "iteration 750 / 1500: loss 14.861944, accuracy 0.289000\n",
      "iteration 751 / 1500: loss 14.765152, accuracy 0.298000\n",
      "iteration 752 / 1500: loss 14.738166, accuracy 0.297000\n",
      "iteration 753 / 1500: loss 14.645768, accuracy 0.311000\n",
      "iteration 754 / 1500: loss 14.612655, accuracy 0.298000\n",
      "iteration 755 / 1500: loss 14.536619, accuracy 0.291000\n",
      "iteration 756 / 1500: loss 14.485087, accuracy 0.296000\n",
      "iteration 757 / 1500: loss 14.424184, accuracy 0.316000\n",
      "iteration 758 / 1500: loss 14.337671, accuracy 0.321000\n",
      "iteration 759 / 1500: loss 14.270676, accuracy 0.321000\n",
      "iteration 760 / 1500: loss 14.237445, accuracy 0.297000\n",
      "iteration 761 / 1500: loss 14.184366, accuracy 0.295000\n",
      "iteration 762 / 1500: loss 14.113370, accuracy 0.312000\n",
      "iteration 763 / 1500: loss 14.052273, accuracy 0.289000\n",
      "iteration 764 / 1500: loss 13.988837, accuracy 0.317000\n",
      "iteration 765 / 1500: loss 13.989553, accuracy 0.273000\n",
      "iteration 766 / 1500: loss 13.868057, accuracy 0.313000\n",
      "iteration 767 / 1500: loss 13.799494, accuracy 0.324000\n",
      "iteration 768 / 1500: loss 13.778217, accuracy 0.298000\n",
      "iteration 769 / 1500: loss 13.716350, accuracy 0.295000\n",
      "iteration 770 / 1500: loss 13.648435, accuracy 0.292000\n",
      "iteration 771 / 1500: loss 13.579256, accuracy 0.324000\n",
      "iteration 772 / 1500: loss 13.577091, accuracy 0.285000\n",
      "iteration 773 / 1500: loss 13.508568, accuracy 0.288000\n",
      "iteration 774 / 1500: loss 13.482418, accuracy 0.278000\n",
      "iteration 775 / 1500: loss 13.363108, accuracy 0.324000\n",
      "iteration 776 / 1500: loss 13.317445, accuracy 0.316000\n",
      "iteration 777 / 1500: loss 13.267415, accuracy 0.303000\n",
      "iteration 778 / 1500: loss 13.227728, accuracy 0.316000\n",
      "iteration 779 / 1500: loss 13.131498, accuracy 0.322000\n",
      "iteration 780 / 1500: loss 13.127984, accuracy 0.290000\n",
      "iteration 781 / 1500: loss 13.035825, accuracy 0.304000\n",
      "iteration 782 / 1500: loss 12.995763, accuracy 0.304000\n",
      "iteration 783 / 1500: loss 12.988220, accuracy 0.293000\n",
      "iteration 784 / 1500: loss 12.875153, accuracy 0.308000\n",
      "iteration 785 / 1500: loss 12.811996, accuracy 0.331000\n",
      "iteration 786 / 1500: loss 12.793128, accuracy 0.283000\n",
      "iteration 787 / 1500: loss 12.708615, accuracy 0.326000\n",
      "iteration 788 / 1500: loss 12.689381, accuracy 0.291000\n",
      "iteration 789 / 1500: loss 12.631084, accuracy 0.307000\n",
      "iteration 790 / 1500: loss 12.558138, accuracy 0.298000\n",
      "iteration 791 / 1500: loss 12.565022, accuracy 0.291000\n",
      "iteration 792 / 1500: loss 12.467848, accuracy 0.295000\n",
      "iteration 793 / 1500: loss 12.446659, accuracy 0.320000\n",
      "iteration 794 / 1500: loss 12.338300, accuracy 0.319000\n",
      "iteration 795 / 1500: loss 12.325493, accuracy 0.312000\n",
      "iteration 796 / 1500: loss 12.284566, accuracy 0.295000\n",
      "iteration 797 / 1500: loss 12.219282, accuracy 0.333000\n",
      "iteration 798 / 1500: loss 12.184059, accuracy 0.311000\n",
      "iteration 799 / 1500: loss 12.105279, accuracy 0.305000\n",
      "iteration 800 / 1500: loss 12.063801, accuracy 0.324000\n",
      "iteration 801 / 1500: loss 12.022636, accuracy 0.305000\n",
      "iteration 802 / 1500: loss 11.988765, accuracy 0.312000\n",
      "iteration 803 / 1500: loss 11.944813, accuracy 0.302000\n",
      "iteration 804 / 1500: loss 11.894737, accuracy 0.288000\n",
      "iteration 805 / 1500: loss 11.829373, accuracy 0.297000\n",
      "iteration 806 / 1500: loss 11.809189, accuracy 0.304000\n",
      "iteration 807 / 1500: loss 11.756801, accuracy 0.305000\n",
      "iteration 808 / 1500: loss 11.703315, accuracy 0.277000\n",
      "iteration 809 / 1500: loss 11.635120, accuracy 0.313000\n",
      "iteration 810 / 1500: loss 11.602272, accuracy 0.317000\n",
      "iteration 811 / 1500: loss 11.546947, accuracy 0.323000\n",
      "iteration 812 / 1500: loss 11.520488, accuracy 0.281000\n",
      "iteration 813 / 1500: loss 11.451322, accuracy 0.302000\n",
      "iteration 814 / 1500: loss 11.403597, accuracy 0.302000\n",
      "iteration 815 / 1500: loss 11.363540, accuracy 0.310000\n",
      "iteration 816 / 1500: loss 11.357455, accuracy 0.289000\n",
      "iteration 817 / 1500: loss 11.302518, accuracy 0.297000\n",
      "iteration 818 / 1500: loss 11.246298, accuracy 0.298000\n",
      "iteration 819 / 1500: loss 11.187713, accuracy 0.307000\n",
      "iteration 820 / 1500: loss 11.136143, accuracy 0.307000\n",
      "iteration 821 / 1500: loss 11.105821, accuracy 0.329000\n",
      "iteration 822 / 1500: loss 11.030163, accuracy 0.308000\n",
      "iteration 823 / 1500: loss 11.000554, accuracy 0.313000\n",
      "iteration 824 / 1500: loss 10.949788, accuracy 0.309000\n",
      "iteration 825 / 1500: loss 10.933364, accuracy 0.284000\n",
      "iteration 826 / 1500: loss 10.871657, accuracy 0.323000\n",
      "iteration 827 / 1500: loss 10.850590, accuracy 0.308000\n",
      "iteration 828 / 1500: loss 10.765790, accuracy 0.315000\n",
      "iteration 829 / 1500: loss 10.757123, accuracy 0.311000\n",
      "iteration 830 / 1500: loss 10.695690, accuracy 0.308000\n",
      "iteration 831 / 1500: loss 10.662541, accuracy 0.307000\n",
      "iteration 832 / 1500: loss 10.631878, accuracy 0.315000\n",
      "iteration 833 / 1500: loss 10.596244, accuracy 0.313000\n",
      "iteration 834 / 1500: loss 10.521466, accuracy 0.329000\n",
      "iteration 835 / 1500: loss 10.477659, accuracy 0.328000\n",
      "iteration 836 / 1500: loss 10.438999, accuracy 0.326000\n",
      "iteration 837 / 1500: loss 10.427331, accuracy 0.316000\n",
      "iteration 838 / 1500: loss 10.379486, accuracy 0.304000\n",
      "iteration 839 / 1500: loss 10.326560, accuracy 0.302000\n",
      "iteration 840 / 1500: loss 10.263788, accuracy 0.341000\n",
      "iteration 841 / 1500: loss 10.257702, accuracy 0.297000\n",
      "iteration 842 / 1500: loss 10.232247, accuracy 0.295000\n",
      "iteration 843 / 1500: loss 10.146822, accuracy 0.300000\n",
      "iteration 844 / 1500: loss 10.123439, accuracy 0.328000\n",
      "iteration 845 / 1500: loss 10.041396, accuracy 0.343000\n",
      "iteration 846 / 1500: loss 10.066457, accuracy 0.306000\n",
      "iteration 847 / 1500: loss 10.005839, accuracy 0.328000\n",
      "iteration 848 / 1500: loss 9.945893, accuracy 0.343000\n",
      "iteration 849 / 1500: loss 9.951710, accuracy 0.312000\n",
      "iteration 850 / 1500: loss 9.890384, accuracy 0.304000\n",
      "iteration 851 / 1500: loss 9.854630, accuracy 0.308000\n",
      "iteration 852 / 1500: loss 9.823856, accuracy 0.309000\n",
      "iteration 853 / 1500: loss 9.858080, accuracy 0.278000\n",
      "iteration 854 / 1500: loss 9.748337, accuracy 0.326000\n",
      "iteration 855 / 1500: loss 9.725597, accuracy 0.291000\n",
      "iteration 856 / 1500: loss 9.670233, accuracy 0.324000\n",
      "iteration 857 / 1500: loss 9.598831, accuracy 0.336000\n",
      "iteration 858 / 1500: loss 9.581711, accuracy 0.325000\n",
      "iteration 859 / 1500: loss 9.570176, accuracy 0.314000\n",
      "iteration 860 / 1500: loss 9.537372, accuracy 0.311000\n",
      "iteration 861 / 1500: loss 9.476089, accuracy 0.319000\n",
      "iteration 862 / 1500: loss 9.443819, accuracy 0.319000\n",
      "iteration 863 / 1500: loss 9.415814, accuracy 0.302000\n",
      "iteration 864 / 1500: loss 9.341072, accuracy 0.325000\n",
      "iteration 865 / 1500: loss 9.345566, accuracy 0.310000\n",
      "iteration 866 / 1500: loss 9.312317, accuracy 0.316000\n",
      "iteration 867 / 1500: loss 9.288305, accuracy 0.303000\n",
      "iteration 868 / 1500: loss 9.257345, accuracy 0.303000\n",
      "iteration 869 / 1500: loss 9.186559, accuracy 0.316000\n",
      "iteration 870 / 1500: loss 9.165072, accuracy 0.315000\n",
      "iteration 871 / 1500: loss 9.177268, accuracy 0.304000\n",
      "iteration 872 / 1500: loss 9.084361, accuracy 0.332000\n",
      "iteration 873 / 1500: loss 9.064280, accuracy 0.329000\n",
      "iteration 874 / 1500: loss 9.031006, accuracy 0.326000\n",
      "iteration 875 / 1500: loss 8.995197, accuracy 0.336000\n",
      "iteration 876 / 1500: loss 8.952385, accuracy 0.325000\n",
      "iteration 877 / 1500: loss 8.917552, accuracy 0.307000\n",
      "iteration 878 / 1500: loss 8.890770, accuracy 0.333000\n",
      "iteration 879 / 1500: loss 8.851962, accuracy 0.313000\n",
      "iteration 880 / 1500: loss 8.858287, accuracy 0.312000\n",
      "iteration 881 / 1500: loss 8.787097, accuracy 0.332000\n",
      "iteration 882 / 1500: loss 8.748381, accuracy 0.330000\n",
      "iteration 883 / 1500: loss 8.704195, accuracy 0.326000\n",
      "iteration 884 / 1500: loss 8.703613, accuracy 0.324000\n",
      "iteration 885 / 1500: loss 8.664861, accuracy 0.316000\n",
      "iteration 886 / 1500: loss 8.622706, accuracy 0.328000\n",
      "iteration 887 / 1500: loss 8.627721, accuracy 0.305000\n",
      "iteration 888 / 1500: loss 8.562155, accuracy 0.307000\n",
      "iteration 889 / 1500: loss 8.548042, accuracy 0.324000\n",
      "iteration 890 / 1500: loss 8.488427, accuracy 0.330000\n",
      "iteration 891 / 1500: loss 8.485446, accuracy 0.325000\n",
      "iteration 892 / 1500: loss 8.425065, accuracy 0.311000\n",
      "iteration 893 / 1500: loss 8.425643, accuracy 0.310000\n",
      "iteration 894 / 1500: loss 8.376549, accuracy 0.316000\n",
      "iteration 895 / 1500: loss 8.365357, accuracy 0.300000\n",
      "iteration 896 / 1500: loss 8.269487, accuracy 0.349000\n",
      "iteration 897 / 1500: loss 8.251124, accuracy 0.346000\n",
      "iteration 898 / 1500: loss 8.253255, accuracy 0.321000\n",
      "iteration 899 / 1500: loss 8.194248, accuracy 0.316000\n",
      "iteration 900 / 1500: loss 8.200025, accuracy 0.306000\n",
      "iteration 901 / 1500: loss 8.185490, accuracy 0.299000\n",
      "iteration 902 / 1500: loss 8.126316, accuracy 0.322000\n",
      "iteration 903 / 1500: loss 8.107470, accuracy 0.300000\n",
      "iteration 904 / 1500: loss 8.089986, accuracy 0.325000\n",
      "iteration 905 / 1500: loss 7.997030, accuracy 0.342000\n",
      "iteration 906 / 1500: loss 8.024038, accuracy 0.301000\n",
      "iteration 907 / 1500: loss 7.969391, accuracy 0.330000\n",
      "iteration 908 / 1500: loss 7.974534, accuracy 0.296000\n",
      "iteration 909 / 1500: loss 7.939509, accuracy 0.301000\n",
      "iteration 910 / 1500: loss 7.901174, accuracy 0.309000\n",
      "iteration 911 / 1500: loss 7.852029, accuracy 0.326000\n",
      "iteration 912 / 1500: loss 7.830324, accuracy 0.316000\n",
      "iteration 913 / 1500: loss 7.803499, accuracy 0.321000\n",
      "iteration 914 / 1500: loss 7.805766, accuracy 0.323000\n",
      "iteration 915 / 1500: loss 7.774306, accuracy 0.288000\n",
      "iteration 916 / 1500: loss 7.754739, accuracy 0.311000\n",
      "iteration 917 / 1500: loss 7.727616, accuracy 0.316000\n",
      "iteration 918 / 1500: loss 7.659752, accuracy 0.343000\n",
      "iteration 919 / 1500: loss 7.636239, accuracy 0.334000\n",
      "iteration 920 / 1500: loss 7.646393, accuracy 0.308000\n",
      "iteration 921 / 1500: loss 7.601957, accuracy 0.300000\n",
      "iteration 922 / 1500: loss 7.527067, accuracy 0.342000\n",
      "iteration 923 / 1500: loss 7.557075, accuracy 0.318000\n",
      "iteration 924 / 1500: loss 7.488876, accuracy 0.327000\n",
      "iteration 925 / 1500: loss 7.526894, accuracy 0.312000\n",
      "iteration 926 / 1500: loss 7.449932, accuracy 0.314000\n",
      "iteration 927 / 1500: loss 7.433237, accuracy 0.309000\n",
      "iteration 928 / 1500: loss 7.406254, accuracy 0.336000\n",
      "iteration 929 / 1500: loss 7.346568, accuracy 0.328000\n",
      "iteration 930 / 1500: loss 7.355046, accuracy 0.319000\n",
      "iteration 931 / 1500: loss 7.321038, accuracy 0.320000\n",
      "iteration 932 / 1500: loss 7.303057, accuracy 0.337000\n",
      "iteration 933 / 1500: loss 7.282779, accuracy 0.304000\n",
      "iteration 934 / 1500: loss 7.262439, accuracy 0.325000\n",
      "iteration 935 / 1500: loss 7.224436, accuracy 0.310000\n",
      "iteration 936 / 1500: loss 7.183127, accuracy 0.330000\n",
      "iteration 937 / 1500: loss 7.174924, accuracy 0.325000\n",
      "iteration 938 / 1500: loss 7.132400, accuracy 0.327000\n",
      "iteration 939 / 1500: loss 7.094310, accuracy 0.349000\n",
      "iteration 940 / 1500: loss 7.103081, accuracy 0.320000\n",
      "iteration 941 / 1500: loss 7.050464, accuracy 0.333000\n",
      "iteration 942 / 1500: loss 7.054397, accuracy 0.321000\n",
      "iteration 943 / 1500: loss 7.045423, accuracy 0.313000\n",
      "iteration 944 / 1500: loss 7.011651, accuracy 0.310000\n",
      "iteration 945 / 1500: loss 6.990899, accuracy 0.316000\n",
      "iteration 946 / 1500: loss 6.962228, accuracy 0.308000\n",
      "iteration 947 / 1500: loss 6.958010, accuracy 0.323000\n",
      "iteration 948 / 1500: loss 6.894339, accuracy 0.340000\n",
      "iteration 949 / 1500: loss 6.879752, accuracy 0.338000\n",
      "iteration 950 / 1500: loss 6.855512, accuracy 0.323000\n",
      "iteration 951 / 1500: loss 6.836515, accuracy 0.309000\n",
      "iteration 952 / 1500: loss 6.818953, accuracy 0.314000\n",
      "iteration 953 / 1500: loss 6.755526, accuracy 0.324000\n",
      "iteration 954 / 1500: loss 6.722421, accuracy 0.358000\n",
      "iteration 955 / 1500: loss 6.699622, accuracy 0.361000\n",
      "iteration 956 / 1500: loss 6.719180, accuracy 0.326000\n",
      "iteration 957 / 1500: loss 6.700009, accuracy 0.321000\n",
      "iteration 958 / 1500: loss 6.689516, accuracy 0.303000\n",
      "iteration 959 / 1500: loss 6.657195, accuracy 0.319000\n",
      "iteration 960 / 1500: loss 6.620119, accuracy 0.337000\n",
      "iteration 961 / 1500: loss 6.598522, accuracy 0.330000\n",
      "iteration 962 / 1500: loss 6.579978, accuracy 0.328000\n",
      "iteration 963 / 1500: loss 6.568354, accuracy 0.327000\n",
      "iteration 964 / 1500: loss 6.541138, accuracy 0.330000\n",
      "iteration 965 / 1500: loss 6.522895, accuracy 0.313000\n",
      "iteration 966 / 1500: loss 6.486120, accuracy 0.335000\n",
      "iteration 967 / 1500: loss 6.463819, accuracy 0.344000\n",
      "iteration 968 / 1500: loss 6.480116, accuracy 0.312000\n",
      "iteration 969 / 1500: loss 6.430767, accuracy 0.333000\n",
      "iteration 970 / 1500: loss 6.394692, accuracy 0.353000\n",
      "iteration 971 / 1500: loss 6.372156, accuracy 0.333000\n",
      "iteration 972 / 1500: loss 6.371105, accuracy 0.327000\n",
      "iteration 973 / 1500: loss 6.364223, accuracy 0.311000\n",
      "iteration 974 / 1500: loss 6.317040, accuracy 0.346000\n",
      "iteration 975 / 1500: loss 6.316739, accuracy 0.327000\n",
      "iteration 976 / 1500: loss 6.294106, accuracy 0.317000\n",
      "iteration 977 / 1500: loss 6.274616, accuracy 0.333000\n",
      "iteration 978 / 1500: loss 6.253389, accuracy 0.316000\n",
      "iteration 979 / 1500: loss 6.209428, accuracy 0.354000\n",
      "iteration 980 / 1500: loss 6.196905, accuracy 0.323000\n",
      "iteration 981 / 1500: loss 6.173863, accuracy 0.332000\n",
      "iteration 982 / 1500: loss 6.157659, accuracy 0.335000\n",
      "iteration 983 / 1500: loss 6.161968, accuracy 0.332000\n",
      "iteration 984 / 1500: loss 6.120967, accuracy 0.329000\n",
      "iteration 985 / 1500: loss 6.082512, accuracy 0.334000\n",
      "iteration 986 / 1500: loss 6.092785, accuracy 0.328000\n",
      "iteration 987 / 1500: loss 6.051399, accuracy 0.342000\n",
      "iteration 988 / 1500: loss 6.057801, accuracy 0.327000\n",
      "iteration 989 / 1500: loss 6.023312, accuracy 0.345000\n",
      "iteration 990 / 1500: loss 6.004607, accuracy 0.325000\n",
      "iteration 991 / 1500: loss 5.980948, accuracy 0.333000\n",
      "iteration 992 / 1500: loss 5.951642, accuracy 0.349000\n",
      "iteration 993 / 1500: loss 5.935142, accuracy 0.346000\n",
      "iteration 994 / 1500: loss 5.917814, accuracy 0.341000\n",
      "iteration 995 / 1500: loss 5.910462, accuracy 0.323000\n",
      "iteration 996 / 1500: loss 5.872225, accuracy 0.319000\n",
      "iteration 997 / 1500: loss 5.869929, accuracy 0.350000\n",
      "iteration 998 / 1500: loss 5.873524, accuracy 0.311000\n",
      "iteration 999 / 1500: loss 5.836026, accuracy 0.318000\n",
      "iteration 1000 / 1500: loss 5.808539, accuracy 0.329000\n",
      "iteration 1001 / 1500: loss 5.793443, accuracy 0.331000\n",
      "iteration 1002 / 1500: loss 5.760898, accuracy 0.356000\n",
      "iteration 1003 / 1500: loss 5.762202, accuracy 0.323000\n",
      "iteration 1004 / 1500: loss 5.748831, accuracy 0.319000\n",
      "iteration 1005 / 1500: loss 5.741048, accuracy 0.315000\n",
      "iteration 1006 / 1500: loss 5.724392, accuracy 0.332000\n",
      "iteration 1007 / 1500: loss 5.675481, accuracy 0.343000\n",
      "iteration 1008 / 1500: loss 5.651814, accuracy 0.337000\n",
      "iteration 1009 / 1500: loss 5.652052, accuracy 0.337000\n",
      "iteration 1010 / 1500: loss 5.628591, accuracy 0.335000\n",
      "iteration 1011 / 1500: loss 5.589067, accuracy 0.356000\n",
      "iteration 1012 / 1500: loss 5.613361, accuracy 0.321000\n",
      "iteration 1013 / 1500: loss 5.596627, accuracy 0.330000\n",
      "iteration 1014 / 1500: loss 5.540963, accuracy 0.326000\n",
      "iteration 1015 / 1500: loss 5.542800, accuracy 0.347000\n",
      "iteration 1016 / 1500: loss 5.557803, accuracy 0.309000\n",
      "iteration 1017 / 1500: loss 5.495555, accuracy 0.330000\n",
      "iteration 1018 / 1500: loss 5.495661, accuracy 0.332000\n",
      "iteration 1019 / 1500: loss 5.496318, accuracy 0.313000\n",
      "iteration 1020 / 1500: loss 5.463856, accuracy 0.334000\n",
      "iteration 1021 / 1500: loss 5.444055, accuracy 0.346000\n",
      "iteration 1022 / 1500: loss 5.448300, accuracy 0.307000\n",
      "iteration 1023 / 1500: loss 5.401605, accuracy 0.332000\n",
      "iteration 1024 / 1500: loss 5.414893, accuracy 0.332000\n",
      "iteration 1025 / 1500: loss 5.403536, accuracy 0.330000\n",
      "iteration 1026 / 1500: loss 5.370220, accuracy 0.338000\n",
      "iteration 1027 / 1500: loss 5.338187, accuracy 0.329000\n",
      "iteration 1028 / 1500: loss 5.335068, accuracy 0.334000\n",
      "iteration 1029 / 1500: loss 5.327153, accuracy 0.328000\n",
      "iteration 1030 / 1500: loss 5.301438, accuracy 0.318000\n",
      "iteration 1031 / 1500: loss 5.302890, accuracy 0.323000\n",
      "iteration 1032 / 1500: loss 5.258574, accuracy 0.315000\n",
      "iteration 1033 / 1500: loss 5.259998, accuracy 0.335000\n",
      "iteration 1034 / 1500: loss 5.235519, accuracy 0.343000\n",
      "iteration 1035 / 1500: loss 5.228895, accuracy 0.333000\n",
      "iteration 1036 / 1500: loss 5.218360, accuracy 0.315000\n",
      "iteration 1037 / 1500: loss 5.170090, accuracy 0.331000\n",
      "iteration 1038 / 1500: loss 5.166555, accuracy 0.355000\n",
      "iteration 1039 / 1500: loss 5.171457, accuracy 0.309000\n",
      "iteration 1040 / 1500: loss 5.130997, accuracy 0.341000\n",
      "iteration 1041 / 1500: loss 5.112698, accuracy 0.338000\n",
      "iteration 1042 / 1500: loss 5.108880, accuracy 0.344000\n",
      "iteration 1043 / 1500: loss 5.097477, accuracy 0.320000\n",
      "iteration 1044 / 1500: loss 5.072830, accuracy 0.340000\n",
      "iteration 1045 / 1500: loss 5.073582, accuracy 0.339000\n",
      "iteration 1046 / 1500: loss 5.024451, accuracy 0.338000\n",
      "iteration 1047 / 1500: loss 5.033315, accuracy 0.336000\n",
      "iteration 1048 / 1500: loss 5.028673, accuracy 0.322000\n",
      "iteration 1049 / 1500: loss 5.017806, accuracy 0.329000\n",
      "iteration 1050 / 1500: loss 5.011042, accuracy 0.312000\n",
      "iteration 1051 / 1500: loss 4.978021, accuracy 0.322000\n",
      "iteration 1052 / 1500: loss 4.983973, accuracy 0.333000\n",
      "iteration 1053 / 1500: loss 4.940101, accuracy 0.331000\n",
      "iteration 1054 / 1500: loss 4.939754, accuracy 0.330000\n",
      "iteration 1055 / 1500: loss 4.925316, accuracy 0.316000\n",
      "iteration 1056 / 1500: loss 4.909073, accuracy 0.337000\n",
      "iteration 1057 / 1500: loss 4.895048, accuracy 0.336000\n",
      "iteration 1058 / 1500: loss 4.880561, accuracy 0.342000\n",
      "iteration 1059 / 1500: loss 4.848503, accuracy 0.359000\n",
      "iteration 1060 / 1500: loss 4.843235, accuracy 0.343000\n",
      "iteration 1061 / 1500: loss 4.856303, accuracy 0.326000\n",
      "iteration 1062 / 1500: loss 4.818983, accuracy 0.334000\n",
      "iteration 1063 / 1500: loss 4.804874, accuracy 0.330000\n",
      "iteration 1064 / 1500: loss 4.801158, accuracy 0.316000\n",
      "iteration 1065 / 1500: loss 4.785008, accuracy 0.334000\n",
      "iteration 1066 / 1500: loss 4.777963, accuracy 0.342000\n",
      "iteration 1067 / 1500: loss 4.755157, accuracy 0.325000\n",
      "iteration 1068 / 1500: loss 4.768632, accuracy 0.331000\n",
      "iteration 1069 / 1500: loss 4.731202, accuracy 0.321000\n",
      "iteration 1070 / 1500: loss 4.720506, accuracy 0.343000\n",
      "iteration 1071 / 1500: loss 4.719472, accuracy 0.329000\n",
      "iteration 1072 / 1500: loss 4.672804, accuracy 0.322000\n",
      "iteration 1073 / 1500: loss 4.670838, accuracy 0.345000\n",
      "iteration 1074 / 1500: loss 4.649142, accuracy 0.329000\n",
      "iteration 1075 / 1500: loss 4.638400, accuracy 0.345000\n",
      "iteration 1076 / 1500: loss 4.650951, accuracy 0.320000\n",
      "iteration 1077 / 1500: loss 4.624414, accuracy 0.358000\n",
      "iteration 1078 / 1500: loss 4.623502, accuracy 0.340000\n",
      "iteration 1079 / 1500: loss 4.622328, accuracy 0.339000\n",
      "iteration 1080 / 1500: loss 4.579236, accuracy 0.340000\n",
      "iteration 1081 / 1500: loss 4.584819, accuracy 0.340000\n",
      "iteration 1082 / 1500: loss 4.540725, accuracy 0.359000\n",
      "iteration 1083 / 1500: loss 4.533392, accuracy 0.334000\n",
      "iteration 1084 / 1500: loss 4.519000, accuracy 0.338000\n",
      "iteration 1085 / 1500: loss 4.523943, accuracy 0.346000\n",
      "iteration 1086 / 1500: loss 4.508156, accuracy 0.363000\n",
      "iteration 1087 / 1500: loss 4.477002, accuracy 0.354000\n",
      "iteration 1088 / 1500: loss 4.471564, accuracy 0.349000\n",
      "iteration 1089 / 1500: loss 4.480099, accuracy 0.335000\n",
      "iteration 1090 / 1500: loss 4.455122, accuracy 0.346000\n",
      "iteration 1091 / 1500: loss 4.472765, accuracy 0.321000\n",
      "iteration 1092 / 1500: loss 4.434299, accuracy 0.314000\n",
      "iteration 1093 / 1500: loss 4.430095, accuracy 0.342000\n",
      "iteration 1094 / 1500: loss 4.437873, accuracy 0.326000\n",
      "iteration 1095 / 1500: loss 4.410996, accuracy 0.333000\n",
      "iteration 1096 / 1500: loss 4.401351, accuracy 0.329000\n",
      "iteration 1097 / 1500: loss 4.368119, accuracy 0.363000\n",
      "iteration 1098 / 1500: loss 4.370372, accuracy 0.318000\n",
      "iteration 1099 / 1500: loss 4.365758, accuracy 0.348000\n",
      "iteration 1100 / 1500: loss 4.351069, accuracy 0.343000\n",
      "iteration 1101 / 1500: loss 4.364891, accuracy 0.336000\n",
      "iteration 1102 / 1500: loss 4.339390, accuracy 0.318000\n",
      "iteration 1103 / 1500: loss 4.322730, accuracy 0.319000\n",
      "iteration 1104 / 1500: loss 4.293705, accuracy 0.354000\n",
      "iteration 1105 / 1500: loss 4.323415, accuracy 0.303000\n",
      "iteration 1106 / 1500: loss 4.292665, accuracy 0.345000\n",
      "iteration 1107 / 1500: loss 4.287031, accuracy 0.336000\n",
      "iteration 1108 / 1500: loss 4.277466, accuracy 0.305000\n",
      "iteration 1109 / 1500: loss 4.259006, accuracy 0.336000\n",
      "iteration 1110 / 1500: loss 4.265628, accuracy 0.318000\n",
      "iteration 1111 / 1500: loss 4.220822, accuracy 0.321000\n",
      "iteration 1112 / 1500: loss 4.224431, accuracy 0.322000\n",
      "iteration 1113 / 1500: loss 4.238039, accuracy 0.315000\n",
      "iteration 1114 / 1500: loss 4.190260, accuracy 0.327000\n",
      "iteration 1115 / 1500: loss 4.192449, accuracy 0.340000\n",
      "iteration 1116 / 1500: loss 4.182913, accuracy 0.338000\n",
      "iteration 1117 / 1500: loss 4.148159, accuracy 0.345000\n",
      "iteration 1118 / 1500: loss 4.172653, accuracy 0.330000\n",
      "iteration 1119 / 1500: loss 4.158704, accuracy 0.347000\n",
      "iteration 1120 / 1500: loss 4.164251, accuracy 0.305000\n",
      "iteration 1121 / 1500: loss 4.147137, accuracy 0.332000\n",
      "iteration 1122 / 1500: loss 4.134877, accuracy 0.327000\n",
      "iteration 1123 / 1500: loss 4.128774, accuracy 0.305000\n",
      "iteration 1124 / 1500: loss 4.136001, accuracy 0.310000\n",
      "iteration 1125 / 1500: loss 4.091670, accuracy 0.345000\n",
      "iteration 1126 / 1500: loss 4.081083, accuracy 0.315000\n",
      "iteration 1127 / 1500: loss 4.058472, accuracy 0.335000\n",
      "iteration 1128 / 1500: loss 4.071324, accuracy 0.328000\n",
      "iteration 1129 / 1500: loss 4.085178, accuracy 0.324000\n",
      "iteration 1130 / 1500: loss 4.050111, accuracy 0.334000\n",
      "iteration 1131 / 1500: loss 4.020438, accuracy 0.339000\n",
      "iteration 1132 / 1500: loss 4.031920, accuracy 0.330000\n",
      "iteration 1133 / 1500: loss 4.024446, accuracy 0.321000\n",
      "iteration 1134 / 1500: loss 4.016237, accuracy 0.319000\n",
      "iteration 1135 / 1500: loss 3.973835, accuracy 0.356000\n",
      "iteration 1136 / 1500: loss 3.988458, accuracy 0.339000\n",
      "iteration 1137 / 1500: loss 4.000057, accuracy 0.319000\n",
      "iteration 1138 / 1500: loss 3.963841, accuracy 0.343000\n",
      "iteration 1139 / 1500: loss 3.948993, accuracy 0.333000\n",
      "iteration 1140 / 1500: loss 3.973324, accuracy 0.343000\n",
      "iteration 1141 / 1500: loss 3.933594, accuracy 0.333000\n",
      "iteration 1142 / 1500: loss 3.931912, accuracy 0.343000\n",
      "iteration 1143 / 1500: loss 3.905820, accuracy 0.333000\n",
      "iteration 1144 / 1500: loss 3.900954, accuracy 0.343000\n",
      "iteration 1145 / 1500: loss 3.880511, accuracy 0.360000\n",
      "iteration 1146 / 1500: loss 3.886051, accuracy 0.339000\n",
      "iteration 1147 / 1500: loss 3.879906, accuracy 0.319000\n",
      "iteration 1148 / 1500: loss 3.875138, accuracy 0.350000\n",
      "iteration 1149 / 1500: loss 3.880478, accuracy 0.333000\n",
      "iteration 1150 / 1500: loss 3.837055, accuracy 0.342000\n",
      "iteration 1151 / 1500: loss 3.839650, accuracy 0.326000\n",
      "iteration 1152 / 1500: loss 3.835275, accuracy 0.347000\n",
      "iteration 1153 / 1500: loss 3.865263, accuracy 0.303000\n",
      "iteration 1154 / 1500: loss 3.778954, accuracy 0.340000\n",
      "iteration 1155 / 1500: loss 3.811752, accuracy 0.332000\n",
      "iteration 1156 / 1500: loss 3.797941, accuracy 0.338000\n",
      "iteration 1157 / 1500: loss 3.805219, accuracy 0.332000\n",
      "iteration 1158 / 1500: loss 3.787212, accuracy 0.328000\n",
      "iteration 1159 / 1500: loss 3.781693, accuracy 0.330000\n",
      "iteration 1160 / 1500: loss 3.758863, accuracy 0.346000\n",
      "iteration 1161 / 1500: loss 3.753954, accuracy 0.356000\n",
      "iteration 1162 / 1500: loss 3.743755, accuracy 0.338000\n",
      "iteration 1163 / 1500: loss 3.753238, accuracy 0.323000\n",
      "iteration 1164 / 1500: loss 3.745197, accuracy 0.332000\n",
      "iteration 1165 / 1500: loss 3.756744, accuracy 0.317000\n",
      "iteration 1166 / 1500: loss 3.706242, accuracy 0.354000\n",
      "iteration 1167 / 1500: loss 3.700224, accuracy 0.340000\n",
      "iteration 1168 / 1500: loss 3.703665, accuracy 0.330000\n",
      "iteration 1169 / 1500: loss 3.716486, accuracy 0.313000\n",
      "iteration 1170 / 1500: loss 3.678234, accuracy 0.340000\n",
      "iteration 1171 / 1500: loss 3.676637, accuracy 0.333000\n",
      "iteration 1172 / 1500: loss 3.680213, accuracy 0.325000\n",
      "iteration 1173 / 1500: loss 3.651277, accuracy 0.358000\n",
      "iteration 1174 / 1500: loss 3.675138, accuracy 0.308000\n",
      "iteration 1175 / 1500: loss 3.646281, accuracy 0.359000\n",
      "iteration 1176 / 1500: loss 3.646005, accuracy 0.342000\n",
      "iteration 1177 / 1500: loss 3.643995, accuracy 0.333000\n",
      "iteration 1178 / 1500: loss 3.632098, accuracy 0.319000\n",
      "iteration 1179 / 1500: loss 3.616112, accuracy 0.347000\n",
      "iteration 1180 / 1500: loss 3.639344, accuracy 0.317000\n",
      "iteration 1181 / 1500: loss 3.582574, accuracy 0.355000\n",
      "iteration 1182 / 1500: loss 3.588994, accuracy 0.334000\n",
      "iteration 1183 / 1500: loss 3.588541, accuracy 0.338000\n",
      "iteration 1184 / 1500: loss 3.584052, accuracy 0.344000\n",
      "iteration 1185 / 1500: loss 3.569901, accuracy 0.342000\n",
      "iteration 1186 / 1500: loss 3.538107, accuracy 0.350000\n",
      "iteration 1187 / 1500: loss 3.532749, accuracy 0.377000\n",
      "iteration 1188 / 1500: loss 3.540308, accuracy 0.351000\n",
      "iteration 1189 / 1500: loss 3.499998, accuracy 0.358000\n",
      "iteration 1190 / 1500: loss 3.550401, accuracy 0.324000\n",
      "iteration 1191 / 1500: loss 3.519410, accuracy 0.354000\n",
      "iteration 1192 / 1500: loss 3.492140, accuracy 0.360000\n",
      "iteration 1193 / 1500: loss 3.524316, accuracy 0.334000\n",
      "iteration 1194 / 1500: loss 3.520465, accuracy 0.331000\n",
      "iteration 1195 / 1500: loss 3.498214, accuracy 0.337000\n",
      "iteration 1196 / 1500: loss 3.465550, accuracy 0.338000\n",
      "iteration 1197 / 1500: loss 3.476949, accuracy 0.345000\n",
      "iteration 1198 / 1500: loss 3.491991, accuracy 0.316000\n",
      "iteration 1199 / 1500: loss 3.463169, accuracy 0.341000\n",
      "iteration 1200 / 1500: loss 3.443341, accuracy 0.343000\n",
      "iteration 1201 / 1500: loss 3.438764, accuracy 0.335000\n",
      "iteration 1202 / 1500: loss 3.435406, accuracy 0.361000\n",
      "iteration 1203 / 1500: loss 3.445155, accuracy 0.330000\n",
      "iteration 1204 / 1500: loss 3.419740, accuracy 0.346000\n",
      "iteration 1205 / 1500: loss 3.440152, accuracy 0.337000\n",
      "iteration 1206 / 1500: loss 3.416674, accuracy 0.336000\n",
      "iteration 1207 / 1500: loss 3.430378, accuracy 0.333000\n",
      "iteration 1208 / 1500: loss 3.411466, accuracy 0.312000\n",
      "iteration 1209 / 1500: loss 3.431427, accuracy 0.328000\n",
      "iteration 1210 / 1500: loss 3.388273, accuracy 0.338000\n",
      "iteration 1211 / 1500: loss 3.353235, accuracy 0.355000\n",
      "iteration 1212 / 1500: loss 3.390425, accuracy 0.332000\n",
      "iteration 1213 / 1500: loss 3.380986, accuracy 0.333000\n",
      "iteration 1214 / 1500: loss 3.395705, accuracy 0.323000\n",
      "iteration 1215 / 1500: loss 3.352471, accuracy 0.346000\n",
      "iteration 1216 / 1500: loss 3.328245, accuracy 0.345000\n",
      "iteration 1217 / 1500: loss 3.330106, accuracy 0.354000\n",
      "iteration 1218 / 1500: loss 3.380732, accuracy 0.310000\n",
      "iteration 1219 / 1500: loss 3.319269, accuracy 0.370000\n",
      "iteration 1220 / 1500: loss 3.329488, accuracy 0.352000\n",
      "iteration 1221 / 1500: loss 3.315741, accuracy 0.337000\n",
      "iteration 1222 / 1500: loss 3.346433, accuracy 0.328000\n",
      "iteration 1223 / 1500: loss 3.315141, accuracy 0.340000\n",
      "iteration 1224 / 1500: loss 3.307837, accuracy 0.335000\n",
      "iteration 1225 / 1500: loss 3.314744, accuracy 0.330000\n",
      "iteration 1226 / 1500: loss 3.272709, accuracy 0.344000\n",
      "iteration 1227 / 1500: loss 3.286773, accuracy 0.355000\n",
      "iteration 1228 / 1500: loss 3.289945, accuracy 0.316000\n",
      "iteration 1229 / 1500: loss 3.249869, accuracy 0.331000\n",
      "iteration 1230 / 1500: loss 3.249529, accuracy 0.364000\n",
      "iteration 1231 / 1500: loss 3.243317, accuracy 0.368000\n",
      "iteration 1232 / 1500: loss 3.245865, accuracy 0.357000\n",
      "iteration 1233 / 1500: loss 3.245182, accuracy 0.352000\n",
      "iteration 1234 / 1500: loss 3.256849, accuracy 0.316000\n",
      "iteration 1235 / 1500: loss 3.227877, accuracy 0.349000\n",
      "iteration 1236 / 1500: loss 3.222943, accuracy 0.348000\n",
      "iteration 1237 / 1500: loss 3.225546, accuracy 0.353000\n",
      "iteration 1238 / 1500: loss 3.234210, accuracy 0.337000\n",
      "iteration 1239 / 1500: loss 3.232909, accuracy 0.323000\n",
      "iteration 1240 / 1500: loss 3.230694, accuracy 0.352000\n",
      "iteration 1241 / 1500: loss 3.233217, accuracy 0.313000\n",
      "iteration 1242 / 1500: loss 3.194862, accuracy 0.357000\n",
      "iteration 1243 / 1500: loss 3.171967, accuracy 0.365000\n",
      "iteration 1244 / 1500: loss 3.173172, accuracy 0.353000\n",
      "iteration 1245 / 1500: loss 3.199117, accuracy 0.326000\n",
      "iteration 1246 / 1500: loss 3.158746, accuracy 0.336000\n",
      "iteration 1247 / 1500: loss 3.160516, accuracy 0.325000\n",
      "iteration 1248 / 1500: loss 3.165543, accuracy 0.336000\n",
      "iteration 1249 / 1500: loss 3.150684, accuracy 0.331000\n",
      "iteration 1250 / 1500: loss 3.128057, accuracy 0.345000\n",
      "iteration 1251 / 1500: loss 3.134032, accuracy 0.367000\n",
      "iteration 1252 / 1500: loss 3.145322, accuracy 0.346000\n",
      "iteration 1253 / 1500: loss 3.132469, accuracy 0.334000\n",
      "iteration 1254 / 1500: loss 3.134290, accuracy 0.340000\n",
      "iteration 1255 / 1500: loss 3.117621, accuracy 0.340000\n",
      "iteration 1256 / 1500: loss 3.107818, accuracy 0.346000\n",
      "iteration 1257 / 1500: loss 3.121740, accuracy 0.339000\n",
      "iteration 1258 / 1500: loss 3.124499, accuracy 0.324000\n",
      "iteration 1259 / 1500: loss 3.122268, accuracy 0.337000\n",
      "iteration 1260 / 1500: loss 3.106804, accuracy 0.333000\n",
      "iteration 1261 / 1500: loss 3.089488, accuracy 0.339000\n",
      "iteration 1262 / 1500: loss 3.086792, accuracy 0.357000\n",
      "iteration 1263 / 1500: loss 3.072755, accuracy 0.348000\n",
      "iteration 1264 / 1500: loss 3.104200, accuracy 0.320000\n",
      "iteration 1265 / 1500: loss 3.109668, accuracy 0.313000\n",
      "iteration 1266 / 1500: loss 3.041600, accuracy 0.355000\n",
      "iteration 1267 / 1500: loss 3.059599, accuracy 0.334000\n",
      "iteration 1268 / 1500: loss 3.052063, accuracy 0.357000\n",
      "iteration 1269 / 1500: loss 3.071216, accuracy 0.324000\n",
      "iteration 1270 / 1500: loss 3.055683, accuracy 0.319000\n",
      "iteration 1271 / 1500: loss 3.052997, accuracy 0.331000\n",
      "iteration 1272 / 1500: loss 3.043627, accuracy 0.338000\n",
      "iteration 1273 / 1500: loss 3.043834, accuracy 0.347000\n",
      "iteration 1274 / 1500: loss 3.003536, accuracy 0.347000\n",
      "iteration 1275 / 1500: loss 3.004451, accuracy 0.361000\n",
      "iteration 1276 / 1500: loss 3.020700, accuracy 0.359000\n",
      "iteration 1277 / 1500: loss 3.034348, accuracy 0.330000\n",
      "iteration 1278 / 1500: loss 2.994865, accuracy 0.358000\n",
      "iteration 1279 / 1500: loss 3.017981, accuracy 0.336000\n",
      "iteration 1280 / 1500: loss 3.009993, accuracy 0.352000\n",
      "iteration 1281 / 1500: loss 2.996931, accuracy 0.341000\n",
      "iteration 1282 / 1500: loss 2.963428, accuracy 0.352000\n",
      "iteration 1283 / 1500: loss 2.990990, accuracy 0.340000\n",
      "iteration 1284 / 1500: loss 3.006676, accuracy 0.330000\n",
      "iteration 1285 / 1500: loss 2.962379, accuracy 0.334000\n",
      "iteration 1286 / 1500: loss 2.953462, accuracy 0.365000\n",
      "iteration 1287 / 1500: loss 2.974250, accuracy 0.325000\n",
      "iteration 1288 / 1500: loss 2.922279, accuracy 0.369000\n",
      "iteration 1289 / 1500: loss 2.985790, accuracy 0.342000\n",
      "iteration 1290 / 1500: loss 2.981803, accuracy 0.319000\n",
      "iteration 1291 / 1500: loss 2.935777, accuracy 0.356000\n",
      "iteration 1292 / 1500: loss 2.946733, accuracy 0.360000\n",
      "iteration 1293 / 1500: loss 2.960136, accuracy 0.332000\n",
      "iteration 1294 / 1500: loss 2.914042, accuracy 0.356000\n",
      "iteration 1295 / 1500: loss 2.917534, accuracy 0.382000\n",
      "iteration 1296 / 1500: loss 2.935420, accuracy 0.335000\n",
      "iteration 1297 / 1500: loss 2.918976, accuracy 0.349000\n",
      "iteration 1298 / 1500: loss 2.923654, accuracy 0.317000\n",
      "iteration 1299 / 1500: loss 2.931068, accuracy 0.316000\n",
      "iteration 1300 / 1500: loss 2.894363, accuracy 0.344000\n",
      "iteration 1301 / 1500: loss 2.895948, accuracy 0.341000\n",
      "iteration 1302 / 1500: loss 2.915107, accuracy 0.345000\n",
      "iteration 1303 / 1500: loss 2.909406, accuracy 0.348000\n",
      "iteration 1304 / 1500: loss 2.890330, accuracy 0.354000\n",
      "iteration 1305 / 1500: loss 2.858347, accuracy 0.350000\n",
      "iteration 1306 / 1500: loss 2.879525, accuracy 0.354000\n",
      "iteration 1307 / 1500: loss 2.912820, accuracy 0.316000\n",
      "iteration 1308 / 1500: loss 2.906315, accuracy 0.312000\n",
      "iteration 1309 / 1500: loss 2.893693, accuracy 0.330000\n",
      "iteration 1310 / 1500: loss 2.877872, accuracy 0.358000\n",
      "iteration 1311 / 1500: loss 2.880793, accuracy 0.326000\n",
      "iteration 1312 / 1500: loss 2.855289, accuracy 0.340000\n",
      "iteration 1313 / 1500: loss 2.849179, accuracy 0.340000\n",
      "iteration 1314 / 1500: loss 2.878258, accuracy 0.341000\n",
      "iteration 1315 / 1500: loss 2.864196, accuracy 0.332000\n",
      "iteration 1316 / 1500: loss 2.884610, accuracy 0.327000\n",
      "iteration 1317 / 1500: loss 2.862139, accuracy 0.338000\n",
      "iteration 1318 / 1500: loss 2.839536, accuracy 0.321000\n",
      "iteration 1319 / 1500: loss 2.817172, accuracy 0.326000\n",
      "iteration 1320 / 1500: loss 2.823183, accuracy 0.340000\n",
      "iteration 1321 / 1500: loss 2.807629, accuracy 0.364000\n",
      "iteration 1322 / 1500: loss 2.812145, accuracy 0.349000\n",
      "iteration 1323 / 1500: loss 2.800821, accuracy 0.349000\n",
      "iteration 1324 / 1500: loss 2.822002, accuracy 0.347000\n",
      "iteration 1325 / 1500: loss 2.767934, accuracy 0.348000\n",
      "iteration 1326 / 1500: loss 2.819440, accuracy 0.335000\n",
      "iteration 1327 / 1500: loss 2.807428, accuracy 0.357000\n",
      "iteration 1328 / 1500: loss 2.809467, accuracy 0.320000\n",
      "iteration 1329 / 1500: loss 2.795873, accuracy 0.345000\n",
      "iteration 1330 / 1500: loss 2.826136, accuracy 0.334000\n",
      "iteration 1331 / 1500: loss 2.797307, accuracy 0.347000\n",
      "iteration 1332 / 1500: loss 2.822731, accuracy 0.303000\n",
      "iteration 1333 / 1500: loss 2.800328, accuracy 0.323000\n",
      "iteration 1334 / 1500: loss 2.801905, accuracy 0.344000\n",
      "iteration 1335 / 1500: loss 2.791603, accuracy 0.324000\n",
      "iteration 1336 / 1500: loss 2.783844, accuracy 0.322000\n",
      "iteration 1337 / 1500: loss 2.731205, accuracy 0.368000\n",
      "iteration 1338 / 1500: loss 2.747148, accuracy 0.354000\n",
      "iteration 1339 / 1500: loss 2.763366, accuracy 0.334000\n",
      "iteration 1340 / 1500: loss 2.771097, accuracy 0.314000\n",
      "iteration 1341 / 1500: loss 2.767826, accuracy 0.323000\n",
      "iteration 1342 / 1500: loss 2.755847, accuracy 0.326000\n",
      "iteration 1343 / 1500: loss 2.756674, accuracy 0.343000\n",
      "iteration 1344 / 1500: loss 2.756576, accuracy 0.326000\n",
      "iteration 1345 / 1500: loss 2.737626, accuracy 0.354000\n",
      "iteration 1346 / 1500: loss 2.719792, accuracy 0.347000\n",
      "iteration 1347 / 1500: loss 2.745767, accuracy 0.340000\n",
      "iteration 1348 / 1500: loss 2.755059, accuracy 0.336000\n",
      "iteration 1349 / 1500: loss 2.681222, accuracy 0.363000\n",
      "iteration 1350 / 1500: loss 2.720086, accuracy 0.344000\n",
      "iteration 1351 / 1500: loss 2.727145, accuracy 0.340000\n",
      "iteration 1352 / 1500: loss 2.720924, accuracy 0.345000\n",
      "iteration 1353 / 1500: loss 2.738049, accuracy 0.328000\n",
      "iteration 1354 / 1500: loss 2.726368, accuracy 0.351000\n",
      "iteration 1355 / 1500: loss 2.699420, accuracy 0.335000\n",
      "iteration 1356 / 1500: loss 2.692699, accuracy 0.349000\n",
      "iteration 1357 / 1500: loss 2.711627, accuracy 0.358000\n",
      "iteration 1358 / 1500: loss 2.721523, accuracy 0.330000\n",
      "iteration 1359 / 1500: loss 2.700628, accuracy 0.331000\n",
      "iteration 1360 / 1500: loss 2.704488, accuracy 0.344000\n",
      "iteration 1361 / 1500: loss 2.670442, accuracy 0.364000\n",
      "iteration 1362 / 1500: loss 2.688499, accuracy 0.349000\n",
      "iteration 1363 / 1500: loss 2.701863, accuracy 0.324000\n",
      "iteration 1364 / 1500: loss 2.674383, accuracy 0.350000\n",
      "iteration 1365 / 1500: loss 2.688300, accuracy 0.322000\n",
      "iteration 1366 / 1500: loss 2.670141, accuracy 0.364000\n",
      "iteration 1367 / 1500: loss 2.658707, accuracy 0.381000\n",
      "iteration 1368 / 1500: loss 2.671085, accuracy 0.342000\n",
      "iteration 1369 / 1500: loss 2.670315, accuracy 0.314000\n",
      "iteration 1370 / 1500: loss 2.677668, accuracy 0.326000\n",
      "iteration 1371 / 1500: loss 2.655726, accuracy 0.333000\n",
      "iteration 1372 / 1500: loss 2.678857, accuracy 0.345000\n",
      "iteration 1373 / 1500: loss 2.667083, accuracy 0.304000\n",
      "iteration 1374 / 1500: loss 2.660583, accuracy 0.343000\n",
      "iteration 1375 / 1500: loss 2.647242, accuracy 0.343000\n",
      "iteration 1376 / 1500: loss 2.648483, accuracy 0.326000\n",
      "iteration 1377 / 1500: loss 2.605272, accuracy 0.352000\n",
      "iteration 1378 / 1500: loss 2.698063, accuracy 0.295000\n",
      "iteration 1379 / 1500: loss 2.613671, accuracy 0.354000\n",
      "iteration 1380 / 1500: loss 2.634946, accuracy 0.345000\n",
      "iteration 1381 / 1500: loss 2.635722, accuracy 0.332000\n",
      "iteration 1382 / 1500: loss 2.618534, accuracy 0.334000\n",
      "iteration 1383 / 1500: loss 2.629455, accuracy 0.342000\n",
      "iteration 1384 / 1500: loss 2.622140, accuracy 0.340000\n",
      "iteration 1385 / 1500: loss 2.627132, accuracy 0.341000\n",
      "iteration 1386 / 1500: loss 2.626537, accuracy 0.314000\n",
      "iteration 1387 / 1500: loss 2.611685, accuracy 0.340000\n",
      "iteration 1388 / 1500: loss 2.604159, accuracy 0.331000\n",
      "iteration 1389 / 1500: loss 2.621305, accuracy 0.333000\n",
      "iteration 1390 / 1500: loss 2.619074, accuracy 0.347000\n",
      "iteration 1391 / 1500: loss 2.611690, accuracy 0.336000\n",
      "iteration 1392 / 1500: loss 2.616767, accuracy 0.325000\n",
      "iteration 1393 / 1500: loss 2.591853, accuracy 0.356000\n",
      "iteration 1394 / 1500: loss 2.629871, accuracy 0.324000\n",
      "iteration 1395 / 1500: loss 2.618579, accuracy 0.332000\n",
      "iteration 1396 / 1500: loss 2.618997, accuracy 0.328000\n",
      "iteration 1397 / 1500: loss 2.625086, accuracy 0.322000\n",
      "iteration 1398 / 1500: loss 2.558254, accuracy 0.347000\n",
      "iteration 1399 / 1500: loss 2.564436, accuracy 0.358000\n",
      "iteration 1400 / 1500: loss 2.556449, accuracy 0.376000\n",
      "iteration 1401 / 1500: loss 2.584808, accuracy 0.343000\n",
      "iteration 1402 / 1500: loss 2.565840, accuracy 0.365000\n",
      "iteration 1403 / 1500: loss 2.608016, accuracy 0.307000\n",
      "iteration 1404 / 1500: loss 2.575548, accuracy 0.356000\n",
      "iteration 1405 / 1500: loss 2.561666, accuracy 0.324000\n",
      "iteration 1406 / 1500: loss 2.603379, accuracy 0.325000\n",
      "iteration 1407 / 1500: loss 2.584082, accuracy 0.336000\n",
      "iteration 1408 / 1500: loss 2.551364, accuracy 0.348000\n",
      "iteration 1409 / 1500: loss 2.549102, accuracy 0.365000\n",
      "iteration 1410 / 1500: loss 2.564750, accuracy 0.326000\n",
      "iteration 1411 / 1500: loss 2.548094, accuracy 0.363000\n",
      "iteration 1412 / 1500: loss 2.524333, accuracy 0.357000\n",
      "iteration 1413 / 1500: loss 2.518002, accuracy 0.367000\n",
      "iteration 1414 / 1500: loss 2.547840, accuracy 0.344000\n",
      "iteration 1415 / 1500: loss 2.534780, accuracy 0.340000\n",
      "iteration 1416 / 1500: loss 2.532880, accuracy 0.350000\n",
      "iteration 1417 / 1500: loss 2.526564, accuracy 0.334000\n",
      "iteration 1418 / 1500: loss 2.532202, accuracy 0.354000\n",
      "iteration 1419 / 1500: loss 2.570016, accuracy 0.335000\n",
      "iteration 1420 / 1500: loss 2.538150, accuracy 0.327000\n",
      "iteration 1421 / 1500: loss 2.523502, accuracy 0.336000\n",
      "iteration 1422 / 1500: loss 2.521232, accuracy 0.342000\n",
      "iteration 1423 / 1500: loss 2.498285, accuracy 0.351000\n",
      "iteration 1424 / 1500: loss 2.536598, accuracy 0.332000\n",
      "iteration 1425 / 1500: loss 2.517174, accuracy 0.359000\n",
      "iteration 1426 / 1500: loss 2.514042, accuracy 0.321000\n",
      "iteration 1427 / 1500: loss 2.531137, accuracy 0.310000\n",
      "iteration 1428 / 1500: loss 2.520344, accuracy 0.322000\n",
      "iteration 1429 / 1500: loss 2.504818, accuracy 0.360000\n",
      "iteration 1430 / 1500: loss 2.513918, accuracy 0.320000\n",
      "iteration 1431 / 1500: loss 2.488142, accuracy 0.341000\n",
      "iteration 1432 / 1500: loss 2.492740, accuracy 0.346000\n",
      "iteration 1433 / 1500: loss 2.482384, accuracy 0.364000\n",
      "iteration 1434 / 1500: loss 2.464184, accuracy 0.359000\n",
      "iteration 1435 / 1500: loss 2.496243, accuracy 0.340000\n",
      "iteration 1436 / 1500: loss 2.477959, accuracy 0.324000\n",
      "iteration 1437 / 1500: loss 2.488323, accuracy 0.334000\n",
      "iteration 1438 / 1500: loss 2.463202, accuracy 0.350000\n",
      "iteration 1439 / 1500: loss 2.490188, accuracy 0.344000\n",
      "iteration 1440 / 1500: loss 2.495397, accuracy 0.345000\n",
      "iteration 1441 / 1500: loss 2.480305, accuracy 0.336000\n",
      "iteration 1442 / 1500: loss 2.485624, accuracy 0.335000\n",
      "iteration 1443 / 1500: loss 2.471432, accuracy 0.357000\n",
      "iteration 1444 / 1500: loss 2.491130, accuracy 0.328000\n",
      "iteration 1445 / 1500: loss 2.472270, accuracy 0.343000\n",
      "iteration 1446 / 1500: loss 2.470424, accuracy 0.334000\n",
      "iteration 1447 / 1500: loss 2.477628, accuracy 0.338000\n",
      "iteration 1448 / 1500: loss 2.486477, accuracy 0.322000\n",
      "iteration 1449 / 1500: loss 2.461804, accuracy 0.346000\n",
      "iteration 1450 / 1500: loss 2.463328, accuracy 0.330000\n",
      "iteration 1451 / 1500: loss 2.453488, accuracy 0.344000\n",
      "iteration 1452 / 1500: loss 2.447361, accuracy 0.337000\n",
      "iteration 1453 / 1500: loss 2.453894, accuracy 0.346000\n",
      "iteration 1454 / 1500: loss 2.448269, accuracy 0.314000\n",
      "iteration 1455 / 1500: loss 2.427338, accuracy 0.354000\n",
      "iteration 1456 / 1500: loss 2.455317, accuracy 0.325000\n",
      "iteration 1457 / 1500: loss 2.456378, accuracy 0.332000\n",
      "iteration 1458 / 1500: loss 2.445167, accuracy 0.342000\n",
      "iteration 1459 / 1500: loss 2.457085, accuracy 0.335000\n",
      "iteration 1460 / 1500: loss 2.442295, accuracy 0.336000\n",
      "iteration 1461 / 1500: loss 2.451469, accuracy 0.323000\n",
      "iteration 1462 / 1500: loss 2.424147, accuracy 0.354000\n",
      "iteration 1463 / 1500: loss 2.425569, accuracy 0.361000\n",
      "iteration 1464 / 1500: loss 2.421431, accuracy 0.343000\n",
      "iteration 1465 / 1500: loss 2.423893, accuracy 0.353000\n",
      "iteration 1466 / 1500: loss 2.445733, accuracy 0.300000\n",
      "iteration 1467 / 1500: loss 2.402527, accuracy 0.385000\n",
      "iteration 1468 / 1500: loss 2.431215, accuracy 0.334000\n",
      "iteration 1469 / 1500: loss 2.428443, accuracy 0.346000\n",
      "iteration 1470 / 1500: loss 2.434775, accuracy 0.331000\n",
      "iteration 1471 / 1500: loss 2.416903, accuracy 0.339000\n",
      "iteration 1472 / 1500: loss 2.433288, accuracy 0.357000\n",
      "iteration 1473 / 1500: loss 2.405072, accuracy 0.363000\n",
      "iteration 1474 / 1500: loss 2.415439, accuracy 0.343000\n",
      "iteration 1475 / 1500: loss 2.397283, accuracy 0.365000\n",
      "iteration 1476 / 1500: loss 2.428357, accuracy 0.331000\n",
      "iteration 1477 / 1500: loss 2.430477, accuracy 0.341000\n",
      "iteration 1478 / 1500: loss 2.409669, accuracy 0.348000\n",
      "iteration 1479 / 1500: loss 2.413034, accuracy 0.341000\n",
      "iteration 1480 / 1500: loss 2.426851, accuracy 0.331000\n",
      "iteration 1481 / 1500: loss 2.414119, accuracy 0.319000\n",
      "iteration 1482 / 1500: loss 2.405198, accuracy 0.360000\n",
      "iteration 1483 / 1500: loss 2.377186, accuracy 0.355000\n",
      "iteration 1484 / 1500: loss 2.412029, accuracy 0.314000\n",
      "iteration 1485 / 1500: loss 2.384389, accuracy 0.345000\n",
      "iteration 1486 / 1500: loss 2.367910, accuracy 0.348000\n",
      "iteration 1487 / 1500: loss 2.400995, accuracy 0.338000\n",
      "iteration 1488 / 1500: loss 2.383945, accuracy 0.355000\n",
      "iteration 1489 / 1500: loss 2.361726, accuracy 0.369000\n",
      "iteration 1490 / 1500: loss 2.421091, accuracy 0.324000\n",
      "iteration 1491 / 1500: loss 2.412155, accuracy 0.329000\n",
      "iteration 1492 / 1500: loss 2.382779, accuracy 0.348000\n",
      "iteration 1493 / 1500: loss 2.401090, accuracy 0.347000\n",
      "iteration 1494 / 1500: loss 2.378798, accuracy 0.341000\n",
      "iteration 1495 / 1500: loss 2.372115, accuracy 0.368000\n",
      "iteration 1496 / 1500: loss 2.376887, accuracy 0.353000\n",
      "iteration 1497 / 1500: loss 2.349631, accuracy 0.361000\n",
      "iteration 1498 / 1500: loss 2.353687, accuracy 0.357000\n",
      "iteration 1499 / 1500: loss 2.359034, accuracy 0.356000\n",
      "iteration 0 / 1500: loss 540.579789, accuracy 0.074000\n",
      "iteration 1 / 1500: loss 534.689453, accuracy 0.081000\n",
      "iteration 2 / 1500: loss 529.159770, accuracy 0.083000\n",
      "iteration 3 / 1500: loss 523.583724, accuracy 0.089000\n",
      "iteration 4 / 1500: loss 517.992455, accuracy 0.090000\n",
      "iteration 5 / 1500: loss 512.461100, accuracy 0.116000\n",
      "iteration 6 / 1500: loss 507.177539, accuracy 0.081000\n",
      "iteration 7 / 1500: loss 501.725508, accuracy 0.097000\n",
      "iteration 8 / 1500: loss 496.452730, accuracy 0.100000\n",
      "iteration 9 / 1500: loss 491.348178, accuracy 0.093000\n",
      "iteration 10 / 1500: loss 486.057377, accuracy 0.094000\n",
      "iteration 11 / 1500: loss 480.864976, accuracy 0.106000\n",
      "iteration 12 / 1500: loss 475.934362, accuracy 0.101000\n",
      "iteration 13 / 1500: loss 470.751366, accuracy 0.101000\n",
      "iteration 14 / 1500: loss 465.963353, accuracy 0.106000\n",
      "iteration 15 / 1500: loss 461.022644, accuracy 0.106000\n",
      "iteration 16 / 1500: loss 456.356295, accuracy 0.087000\n",
      "iteration 17 / 1500: loss 451.244763, accuracy 0.104000\n",
      "iteration 18 / 1500: loss 446.516593, accuracy 0.119000\n",
      "iteration 19 / 1500: loss 442.031753, accuracy 0.095000\n",
      "iteration 20 / 1500: loss 437.166587, accuracy 0.116000\n",
      "iteration 21 / 1500: loss 432.592711, accuracy 0.112000\n",
      "iteration 22 / 1500: loss 428.205273, accuracy 0.099000\n",
      "iteration 23 / 1500: loss 423.805748, accuracy 0.090000\n",
      "iteration 24 / 1500: loss 419.275395, accuracy 0.104000\n",
      "iteration 25 / 1500: loss 414.816974, accuracy 0.119000\n",
      "iteration 26 / 1500: loss 410.517358, accuracy 0.108000\n",
      "iteration 27 / 1500: loss 406.180878, accuracy 0.129000\n",
      "iteration 28 / 1500: loss 401.972181, accuracy 0.092000\n",
      "iteration 29 / 1500: loss 397.550635, accuracy 0.121000\n",
      "iteration 30 / 1500: loss 393.456042, accuracy 0.099000\n",
      "iteration 31 / 1500: loss 389.516118, accuracy 0.090000\n",
      "iteration 32 / 1500: loss 385.427343, accuracy 0.105000\n",
      "iteration 33 / 1500: loss 381.361226, accuracy 0.098000\n",
      "iteration 34 / 1500: loss 377.193546, accuracy 0.104000\n",
      "iteration 35 / 1500: loss 373.336203, accuracy 0.109000\n",
      "iteration 36 / 1500: loss 369.348663, accuracy 0.122000\n",
      "iteration 37 / 1500: loss 365.574059, accuracy 0.101000\n",
      "iteration 38 / 1500: loss 361.711138, accuracy 0.109000\n",
      "iteration 39 / 1500: loss 357.846186, accuracy 0.120000\n",
      "iteration 40 / 1500: loss 354.251525, accuracy 0.101000\n",
      "iteration 41 / 1500: loss 350.549034, accuracy 0.124000\n",
      "iteration 42 / 1500: loss 346.787659, accuracy 0.120000\n",
      "iteration 43 / 1500: loss 343.257125, accuracy 0.113000\n",
      "iteration 44 / 1500: loss 339.484282, accuracy 0.126000\n",
      "iteration 45 / 1500: loss 336.128454, accuracy 0.110000\n",
      "iteration 46 / 1500: loss 332.359386, accuracy 0.133000\n",
      "iteration 47 / 1500: loss 328.944252, accuracy 0.125000\n",
      "iteration 48 / 1500: loss 325.592461, accuracy 0.137000\n",
      "iteration 49 / 1500: loss 322.168532, accuracy 0.119000\n",
      "iteration 50 / 1500: loss 318.801058, accuracy 0.127000\n",
      "iteration 51 / 1500: loss 315.370344, accuracy 0.147000\n",
      "iteration 52 / 1500: loss 312.067306, accuracy 0.142000\n",
      "iteration 53 / 1500: loss 308.920051, accuracy 0.117000\n",
      "iteration 54 / 1500: loss 305.645666, accuracy 0.125000\n",
      "iteration 55 / 1500: loss 302.421369, accuracy 0.123000\n",
      "iteration 56 / 1500: loss 299.236786, accuracy 0.149000\n",
      "iteration 57 / 1500: loss 296.203795, accuracy 0.138000\n",
      "iteration 58 / 1500: loss 292.975542, accuracy 0.138000\n",
      "iteration 59 / 1500: loss 289.872310, accuracy 0.128000\n",
      "iteration 60 / 1500: loss 286.994171, accuracy 0.137000\n",
      "iteration 61 / 1500: loss 283.926823, accuracy 0.131000\n",
      "iteration 62 / 1500: loss 280.911438, accuracy 0.152000\n",
      "iteration 63 / 1500: loss 278.012564, accuracy 0.129000\n",
      "iteration 64 / 1500: loss 274.982420, accuracy 0.157000\n",
      "iteration 65 / 1500: loss 272.286781, accuracy 0.129000\n",
      "iteration 66 / 1500: loss 269.388931, accuracy 0.144000\n",
      "iteration 67 / 1500: loss 266.649040, accuracy 0.128000\n",
      "iteration 68 / 1500: loss 263.758866, accuracy 0.141000\n",
      "iteration 69 / 1500: loss 261.091717, accuracy 0.131000\n",
      "iteration 70 / 1500: loss 258.420126, accuracy 0.117000\n",
      "iteration 71 / 1500: loss 255.632876, accuracy 0.125000\n",
      "iteration 72 / 1500: loss 252.939143, accuracy 0.151000\n",
      "iteration 73 / 1500: loss 250.289728, accuracy 0.125000\n",
      "iteration 74 / 1500: loss 247.698794, accuracy 0.139000\n",
      "iteration 75 / 1500: loss 245.196138, accuracy 0.142000\n",
      "iteration 76 / 1500: loss 242.691258, accuracy 0.122000\n",
      "iteration 77 / 1500: loss 239.956837, accuracy 0.155000\n",
      "iteration 78 / 1500: loss 237.475663, accuracy 0.153000\n",
      "iteration 79 / 1500: loss 235.083363, accuracy 0.140000\n",
      "iteration 80 / 1500: loss 232.617040, accuracy 0.138000\n",
      "iteration 81 / 1500: loss 230.113977, accuracy 0.143000\n",
      "iteration 82 / 1500: loss 227.774237, accuracy 0.148000\n",
      "iteration 83 / 1500: loss 225.403820, accuracy 0.129000\n",
      "iteration 84 / 1500: loss 223.020106, accuracy 0.145000\n",
      "iteration 85 / 1500: loss 220.689662, accuracy 0.134000\n",
      "iteration 86 / 1500: loss 218.402052, accuracy 0.158000\n",
      "iteration 87 / 1500: loss 216.165717, accuracy 0.141000\n",
      "iteration 88 / 1500: loss 213.901861, accuracy 0.142000\n",
      "iteration 89 / 1500: loss 211.720488, accuracy 0.146000\n",
      "iteration 90 / 1500: loss 209.421631, accuracy 0.146000\n",
      "iteration 91 / 1500: loss 207.279774, accuracy 0.164000\n",
      "iteration 92 / 1500: loss 205.115030, accuracy 0.164000\n",
      "iteration 93 / 1500: loss 203.007218, accuracy 0.154000\n",
      "iteration 94 / 1500: loss 200.920116, accuracy 0.145000\n",
      "iteration 95 / 1500: loss 198.753775, accuracy 0.169000\n",
      "iteration 96 / 1500: loss 196.700868, accuracy 0.153000\n",
      "iteration 97 / 1500: loss 194.716899, accuracy 0.159000\n",
      "iteration 98 / 1500: loss 192.608162, accuracy 0.163000\n",
      "iteration 99 / 1500: loss 190.671778, accuracy 0.143000\n",
      "iteration 100 / 1500: loss 188.646281, accuracy 0.146000\n",
      "iteration 101 / 1500: loss 186.659055, accuracy 0.164000\n",
      "iteration 102 / 1500: loss 184.697133, accuracy 0.161000\n",
      "iteration 103 / 1500: loss 182.757306, accuracy 0.162000\n",
      "iteration 104 / 1500: loss 180.972091, accuracy 0.146000\n",
      "iteration 105 / 1500: loss 179.070633, accuracy 0.161000\n",
      "iteration 106 / 1500: loss 177.205643, accuracy 0.165000\n",
      "iteration 107 / 1500: loss 175.281469, accuracy 0.177000\n",
      "iteration 108 / 1500: loss 173.649206, accuracy 0.132000\n",
      "iteration 109 / 1500: loss 171.646355, accuracy 0.184000\n",
      "iteration 110 / 1500: loss 169.877484, accuracy 0.180000\n",
      "iteration 111 / 1500: loss 168.171146, accuracy 0.158000\n",
      "iteration 112 / 1500: loss 166.398718, accuracy 0.168000\n",
      "iteration 113 / 1500: loss 164.717981, accuracy 0.174000\n",
      "iteration 114 / 1500: loss 163.032591, accuracy 0.145000\n",
      "iteration 115 / 1500: loss 161.290122, accuracy 0.183000\n",
      "iteration 116 / 1500: loss 159.587011, accuracy 0.188000\n",
      "iteration 117 / 1500: loss 157.955751, accuracy 0.172000\n",
      "iteration 118 / 1500: loss 156.356932, accuracy 0.151000\n",
      "iteration 119 / 1500: loss 154.678249, accuracy 0.184000\n",
      "iteration 120 / 1500: loss 153.098345, accuracy 0.154000\n",
      "iteration 121 / 1500: loss 151.537000, accuracy 0.158000\n",
      "iteration 122 / 1500: loss 149.885413, accuracy 0.187000\n",
      "iteration 123 / 1500: loss 148.363036, accuracy 0.173000\n",
      "iteration 124 / 1500: loss 146.777110, accuracy 0.218000\n",
      "iteration 125 / 1500: loss 145.368094, accuracy 0.152000\n",
      "iteration 126 / 1500: loss 143.828206, accuracy 0.162000\n",
      "iteration 127 / 1500: loss 142.298691, accuracy 0.193000\n",
      "iteration 128 / 1500: loss 140.800095, accuracy 0.189000\n",
      "iteration 129 / 1500: loss 139.325007, accuracy 0.184000\n",
      "iteration 130 / 1500: loss 137.857250, accuracy 0.192000\n",
      "iteration 131 / 1500: loss 136.477435, accuracy 0.179000\n",
      "iteration 132 / 1500: loss 135.106928, accuracy 0.160000\n",
      "iteration 133 / 1500: loss 133.713837, accuracy 0.165000\n",
      "iteration 134 / 1500: loss 132.338754, accuracy 0.183000\n",
      "iteration 135 / 1500: loss 130.938250, accuracy 0.163000\n",
      "iteration 136 / 1500: loss 129.570672, accuracy 0.193000\n",
      "iteration 137 / 1500: loss 128.241223, accuracy 0.182000\n",
      "iteration 138 / 1500: loss 126.962317, accuracy 0.181000\n",
      "iteration 139 / 1500: loss 125.598051, accuracy 0.188000\n",
      "iteration 140 / 1500: loss 124.318417, accuracy 0.176000\n",
      "iteration 141 / 1500: loss 123.026639, accuracy 0.168000\n",
      "iteration 142 / 1500: loss 121.778740, accuracy 0.174000\n",
      "iteration 143 / 1500: loss 120.498398, accuracy 0.188000\n",
      "iteration 144 / 1500: loss 119.278671, accuracy 0.187000\n",
      "iteration 145 / 1500: loss 118.016055, accuracy 0.209000\n",
      "iteration 146 / 1500: loss 116.796031, accuracy 0.182000\n",
      "iteration 147 / 1500: loss 115.556054, accuracy 0.183000\n",
      "iteration 148 / 1500: loss 114.444689, accuracy 0.171000\n",
      "iteration 149 / 1500: loss 113.223376, accuracy 0.164000\n",
      "iteration 150 / 1500: loss 112.084684, accuracy 0.182000\n",
      "iteration 151 / 1500: loss 110.955867, accuracy 0.197000\n",
      "iteration 152 / 1500: loss 109.718970, accuracy 0.199000\n",
      "iteration 153 / 1500: loss 108.599346, accuracy 0.185000\n",
      "iteration 154 / 1500: loss 107.479744, accuracy 0.206000\n",
      "iteration 155 / 1500: loss 106.389926, accuracy 0.192000\n",
      "iteration 156 / 1500: loss 105.306073, accuracy 0.186000\n",
      "iteration 157 / 1500: loss 104.214391, accuracy 0.191000\n",
      "iteration 158 / 1500: loss 103.127069, accuracy 0.188000\n",
      "iteration 159 / 1500: loss 102.043876, accuracy 0.190000\n",
      "iteration 160 / 1500: loss 101.032584, accuracy 0.196000\n",
      "iteration 161 / 1500: loss 99.970186, accuracy 0.208000\n",
      "iteration 162 / 1500: loss 98.962219, accuracy 0.188000\n",
      "iteration 163 / 1500: loss 97.940158, accuracy 0.207000\n",
      "iteration 164 / 1500: loss 96.918466, accuracy 0.193000\n",
      "iteration 165 / 1500: loss 95.959110, accuracy 0.196000\n",
      "iteration 166 / 1500: loss 94.929989, accuracy 0.204000\n",
      "iteration 167 / 1500: loss 93.923481, accuracy 0.235000\n",
      "iteration 168 / 1500: loss 92.981726, accuracy 0.215000\n",
      "iteration 169 / 1500: loss 92.098395, accuracy 0.179000\n",
      "iteration 170 / 1500: loss 91.077838, accuracy 0.217000\n",
      "iteration 171 / 1500: loss 90.221254, accuracy 0.194000\n",
      "iteration 172 / 1500: loss 89.245879, accuracy 0.193000\n",
      "iteration 173 / 1500: loss 88.315650, accuracy 0.198000\n",
      "iteration 174 / 1500: loss 87.402916, accuracy 0.222000\n",
      "iteration 175 / 1500: loss 86.449600, accuracy 0.248000\n",
      "iteration 176 / 1500: loss 85.650677, accuracy 0.208000\n",
      "iteration 177 / 1500: loss 84.730601, accuracy 0.204000\n",
      "iteration 178 / 1500: loss 83.865204, accuracy 0.210000\n",
      "iteration 179 / 1500: loss 82.984182, accuracy 0.233000\n",
      "iteration 180 / 1500: loss 82.173914, accuracy 0.208000\n",
      "iteration 181 / 1500: loss 81.373532, accuracy 0.211000\n",
      "iteration 182 / 1500: loss 80.536310, accuracy 0.192000\n",
      "iteration 183 / 1500: loss 79.713686, accuracy 0.197000\n",
      "iteration 184 / 1500: loss 78.853389, accuracy 0.217000\n",
      "iteration 185 / 1500: loss 78.049472, accuracy 0.210000\n",
      "iteration 186 / 1500: loss 77.261065, accuracy 0.204000\n",
      "iteration 187 / 1500: loss 76.432677, accuracy 0.218000\n",
      "iteration 188 / 1500: loss 75.688084, accuracy 0.206000\n",
      "iteration 189 / 1500: loss 74.922689, accuracy 0.196000\n",
      "iteration 190 / 1500: loss 74.144371, accuracy 0.233000\n",
      "iteration 191 / 1500: loss 73.409974, accuracy 0.214000\n",
      "iteration 192 / 1500: loss 72.589184, accuracy 0.243000\n",
      "iteration 193 / 1500: loss 71.898442, accuracy 0.211000\n",
      "iteration 194 / 1500: loss 71.222770, accuracy 0.191000\n",
      "iteration 195 / 1500: loss 70.441956, accuracy 0.206000\n",
      "iteration 196 / 1500: loss 69.743419, accuracy 0.191000\n",
      "iteration 197 / 1500: loss 68.989433, accuracy 0.217000\n",
      "iteration 198 / 1500: loss 68.305377, accuracy 0.213000\n",
      "iteration 199 / 1500: loss 67.615129, accuracy 0.223000\n",
      "iteration 200 / 1500: loss 66.886894, accuracy 0.223000\n",
      "iteration 201 / 1500: loss 66.266149, accuracy 0.198000\n",
      "iteration 202 / 1500: loss 65.527875, accuracy 0.227000\n",
      "iteration 203 / 1500: loss 64.926351, accuracy 0.205000\n",
      "iteration 204 / 1500: loss 64.243747, accuracy 0.218000\n",
      "iteration 205 / 1500: loss 63.610691, accuracy 0.206000\n",
      "iteration 206 / 1500: loss 62.934638, accuracy 0.230000\n",
      "iteration 207 / 1500: loss 62.293433, accuracy 0.220000\n",
      "iteration 208 / 1500: loss 61.660936, accuracy 0.230000\n",
      "iteration 209 / 1500: loss 61.054473, accuracy 0.232000\n",
      "iteration 210 / 1500: loss 60.446093, accuracy 0.210000\n",
      "iteration 211 / 1500: loss 59.766179, accuracy 0.231000\n",
      "iteration 212 / 1500: loss 59.186569, accuracy 0.232000\n",
      "iteration 213 / 1500: loss 58.579599, accuracy 0.226000\n",
      "iteration 214 / 1500: loss 57.991537, accuracy 0.209000\n",
      "iteration 215 / 1500: loss 57.405637, accuracy 0.231000\n",
      "iteration 216 / 1500: loss 56.812721, accuracy 0.234000\n",
      "iteration 217 / 1500: loss 56.239863, accuracy 0.218000\n",
      "iteration 218 / 1500: loss 55.683038, accuracy 0.231000\n",
      "iteration 219 / 1500: loss 55.124735, accuracy 0.238000\n",
      "iteration 220 / 1500: loss 54.616302, accuracy 0.199000\n",
      "iteration 221 / 1500: loss 53.963002, accuracy 0.254000\n",
      "iteration 222 / 1500: loss 53.455792, accuracy 0.240000\n",
      "iteration 223 / 1500: loss 52.941550, accuracy 0.207000\n",
      "iteration 224 / 1500: loss 52.387038, accuracy 0.243000\n",
      "iteration 225 / 1500: loss 51.858449, accuracy 0.229000\n",
      "iteration 226 / 1500: loss 51.336818, accuracy 0.223000\n",
      "iteration 227 / 1500: loss 50.841177, accuracy 0.218000\n",
      "iteration 228 / 1500: loss 50.325092, accuracy 0.233000\n",
      "iteration 229 / 1500: loss 49.791962, accuracy 0.230000\n",
      "iteration 230 / 1500: loss 49.317765, accuracy 0.212000\n",
      "iteration 231 / 1500: loss 48.783284, accuracy 0.242000\n",
      "iteration 232 / 1500: loss 48.337399, accuracy 0.209000\n",
      "iteration 233 / 1500: loss 47.837013, accuracy 0.222000\n",
      "iteration 234 / 1500: loss 47.361059, accuracy 0.222000\n",
      "iteration 235 / 1500: loss 46.888813, accuracy 0.245000\n",
      "iteration 236 / 1500: loss 46.404690, accuracy 0.237000\n",
      "iteration 237 / 1500: loss 45.889153, accuracy 0.260000\n",
      "iteration 238 / 1500: loss 45.456534, accuracy 0.230000\n",
      "iteration 239 / 1500: loss 45.025642, accuracy 0.232000\n",
      "iteration 240 / 1500: loss 44.515290, accuracy 0.267000\n",
      "iteration 241 / 1500: loss 44.100032, accuracy 0.225000\n",
      "iteration 242 / 1500: loss 43.666498, accuracy 0.238000\n",
      "iteration 243 / 1500: loss 43.237064, accuracy 0.231000\n",
      "iteration 244 / 1500: loss 42.815623, accuracy 0.250000\n",
      "iteration 245 / 1500: loss 42.386429, accuracy 0.229000\n",
      "iteration 246 / 1500: loss 41.951530, accuracy 0.242000\n",
      "iteration 247 / 1500: loss 41.529716, accuracy 0.265000\n",
      "iteration 248 / 1500: loss 41.144922, accuracy 0.229000\n",
      "iteration 249 / 1500: loss 40.691558, accuracy 0.238000\n",
      "iteration 250 / 1500: loss 40.280950, accuracy 0.245000\n",
      "iteration 251 / 1500: loss 39.873377, accuracy 0.255000\n",
      "iteration 252 / 1500: loss 39.507161, accuracy 0.250000\n",
      "iteration 253 / 1500: loss 39.114240, accuracy 0.240000\n",
      "iteration 254 / 1500: loss 38.756220, accuracy 0.244000\n",
      "iteration 255 / 1500: loss 38.336478, accuracy 0.241000\n",
      "iteration 256 / 1500: loss 37.970721, accuracy 0.240000\n",
      "iteration 257 / 1500: loss 37.563063, accuracy 0.257000\n",
      "iteration 258 / 1500: loss 37.213477, accuracy 0.255000\n",
      "iteration 259 / 1500: loss 36.798254, accuracy 0.277000\n",
      "iteration 260 / 1500: loss 36.456699, accuracy 0.266000\n",
      "iteration 261 / 1500: loss 36.102842, accuracy 0.265000\n",
      "iteration 262 / 1500: loss 35.790605, accuracy 0.246000\n",
      "iteration 263 / 1500: loss 35.403998, accuracy 0.263000\n",
      "iteration 264 / 1500: loss 34.994217, accuracy 0.294000\n",
      "iteration 265 / 1500: loss 34.700545, accuracy 0.267000\n",
      "iteration 266 / 1500: loss 34.357241, accuracy 0.247000\n",
      "iteration 267 / 1500: loss 33.972493, accuracy 0.263000\n",
      "iteration 268 / 1500: loss 33.661155, accuracy 0.255000\n",
      "iteration 269 / 1500: loss 33.361987, accuracy 0.231000\n",
      "iteration 270 / 1500: loss 33.009363, accuracy 0.261000\n",
      "iteration 271 / 1500: loss 32.687825, accuracy 0.256000\n",
      "iteration 272 / 1500: loss 32.353775, accuracy 0.266000\n",
      "iteration 273 / 1500: loss 32.052117, accuracy 0.257000\n",
      "iteration 274 / 1500: loss 31.711868, accuracy 0.278000\n",
      "iteration 275 / 1500: loss 31.428483, accuracy 0.269000\n",
      "iteration 276 / 1500: loss 31.138170, accuracy 0.239000\n",
      "iteration 277 / 1500: loss 30.800481, accuracy 0.270000\n",
      "iteration 278 / 1500: loss 30.517792, accuracy 0.259000\n",
      "iteration 279 / 1500: loss 30.227646, accuracy 0.273000\n",
      "iteration 280 / 1500: loss 29.911331, accuracy 0.266000\n",
      "iteration 281 / 1500: loss 29.632570, accuracy 0.263000\n",
      "iteration 282 / 1500: loss 29.303718, accuracy 0.281000\n",
      "iteration 283 / 1500: loss 29.089515, accuracy 0.235000\n",
      "iteration 284 / 1500: loss 28.769384, accuracy 0.268000\n",
      "iteration 285 / 1500: loss 28.490205, accuracy 0.263000\n",
      "iteration 286 / 1500: loss 28.203135, accuracy 0.265000\n",
      "iteration 287 / 1500: loss 27.926273, accuracy 0.292000\n",
      "iteration 288 / 1500: loss 27.662969, accuracy 0.246000\n",
      "iteration 289 / 1500: loss 27.372107, accuracy 0.279000\n",
      "iteration 290 / 1500: loss 27.127748, accuracy 0.283000\n",
      "iteration 291 / 1500: loss 26.836980, accuracy 0.276000\n",
      "iteration 292 / 1500: loss 26.611976, accuracy 0.266000\n",
      "iteration 293 / 1500: loss 26.338157, accuracy 0.259000\n",
      "iteration 294 / 1500: loss 26.075298, accuracy 0.278000\n",
      "iteration 295 / 1500: loss 25.822688, accuracy 0.288000\n",
      "iteration 296 / 1500: loss 25.561243, accuracy 0.279000\n",
      "iteration 297 / 1500: loss 25.373146, accuracy 0.255000\n",
      "iteration 298 / 1500: loss 25.090928, accuracy 0.277000\n",
      "iteration 299 / 1500: loss 24.858561, accuracy 0.271000\n",
      "iteration 300 / 1500: loss 24.637848, accuracy 0.268000\n",
      "iteration 301 / 1500: loss 24.366878, accuracy 0.266000\n",
      "iteration 302 / 1500: loss 24.081928, accuracy 0.319000\n",
      "iteration 303 / 1500: loss 23.889033, accuracy 0.277000\n",
      "iteration 304 / 1500: loss 23.633569, accuracy 0.301000\n",
      "iteration 305 / 1500: loss 23.445082, accuracy 0.263000\n",
      "iteration 306 / 1500: loss 23.243941, accuracy 0.275000\n",
      "iteration 307 / 1500: loss 23.020707, accuracy 0.275000\n",
      "iteration 308 / 1500: loss 22.783432, accuracy 0.289000\n",
      "iteration 309 / 1500: loss 22.559961, accuracy 0.298000\n",
      "iteration 310 / 1500: loss 22.354798, accuracy 0.277000\n",
      "iteration 311 / 1500: loss 22.115279, accuracy 0.288000\n",
      "iteration 312 / 1500: loss 21.948316, accuracy 0.278000\n",
      "iteration 313 / 1500: loss 21.691142, accuracy 0.292000\n",
      "iteration 314 / 1500: loss 21.497418, accuracy 0.287000\n",
      "iteration 315 / 1500: loss 21.336770, accuracy 0.257000\n",
      "iteration 316 / 1500: loss 21.093370, accuracy 0.267000\n",
      "iteration 317 / 1500: loss 20.885831, accuracy 0.283000\n",
      "iteration 318 / 1500: loss 20.707197, accuracy 0.283000\n",
      "iteration 319 / 1500: loss 20.518607, accuracy 0.285000\n",
      "iteration 320 / 1500: loss 20.303396, accuracy 0.297000\n",
      "iteration 321 / 1500: loss 20.128532, accuracy 0.270000\n",
      "iteration 322 / 1500: loss 19.935665, accuracy 0.278000\n",
      "iteration 323 / 1500: loss 19.751783, accuracy 0.285000\n",
      "iteration 324 / 1500: loss 19.581037, accuracy 0.279000\n",
      "iteration 325 / 1500: loss 19.340170, accuracy 0.311000\n",
      "iteration 326 / 1500: loss 19.174295, accuracy 0.292000\n",
      "iteration 327 / 1500: loss 18.980771, accuracy 0.334000\n",
      "iteration 328 / 1500: loss 18.825220, accuracy 0.305000\n",
      "iteration 329 / 1500: loss 18.669129, accuracy 0.281000\n",
      "iteration 330 / 1500: loss 18.485092, accuracy 0.278000\n",
      "iteration 331 / 1500: loss 18.300397, accuracy 0.310000\n",
      "iteration 332 / 1500: loss 18.128419, accuracy 0.288000\n",
      "iteration 333 / 1500: loss 17.963062, accuracy 0.305000\n",
      "iteration 334 / 1500: loss 17.835341, accuracy 0.275000\n",
      "iteration 335 / 1500: loss 17.599810, accuracy 0.313000\n",
      "iteration 336 / 1500: loss 17.464184, accuracy 0.283000\n",
      "iteration 337 / 1500: loss 17.345653, accuracy 0.286000\n",
      "iteration 338 / 1500: loss 17.173880, accuracy 0.280000\n",
      "iteration 339 / 1500: loss 16.981574, accuracy 0.342000\n",
      "iteration 340 / 1500: loss 16.848628, accuracy 0.294000\n",
      "iteration 341 / 1500: loss 16.679803, accuracy 0.285000\n",
      "iteration 342 / 1500: loss 16.538249, accuracy 0.302000\n",
      "iteration 343 / 1500: loss 16.338453, accuracy 0.326000\n",
      "iteration 344 / 1500: loss 16.237573, accuracy 0.297000\n",
      "iteration 345 / 1500: loss 16.106445, accuracy 0.270000\n",
      "iteration 346 / 1500: loss 15.923167, accuracy 0.306000\n",
      "iteration 347 / 1500: loss 15.757890, accuracy 0.325000\n",
      "iteration 348 / 1500: loss 15.655401, accuracy 0.287000\n",
      "iteration 349 / 1500: loss 15.520386, accuracy 0.294000\n",
      "iteration 350 / 1500: loss 15.347046, accuracy 0.291000\n",
      "iteration 351 / 1500: loss 15.204100, accuracy 0.318000\n",
      "iteration 352 / 1500: loss 15.129760, accuracy 0.280000\n",
      "iteration 353 / 1500: loss 14.940237, accuracy 0.311000\n",
      "iteration 354 / 1500: loss 14.792791, accuracy 0.304000\n",
      "iteration 355 / 1500: loss 14.672920, accuracy 0.298000\n",
      "iteration 356 / 1500: loss 14.522639, accuracy 0.298000\n",
      "iteration 357 / 1500: loss 14.387061, accuracy 0.320000\n",
      "iteration 358 / 1500: loss 14.278201, accuracy 0.305000\n",
      "iteration 359 / 1500: loss 14.119089, accuracy 0.318000\n",
      "iteration 360 / 1500: loss 14.029708, accuracy 0.296000\n",
      "iteration 361 / 1500: loss 13.910218, accuracy 0.293000\n",
      "iteration 362 / 1500: loss 13.796993, accuracy 0.288000\n",
      "iteration 363 / 1500: loss 13.622559, accuracy 0.306000\n",
      "iteration 364 / 1500: loss 13.557925, accuracy 0.285000\n",
      "iteration 365 / 1500: loss 13.399427, accuracy 0.315000\n",
      "iteration 366 / 1500: loss 13.273604, accuracy 0.299000\n",
      "iteration 367 / 1500: loss 13.166477, accuracy 0.302000\n",
      "iteration 368 / 1500: loss 13.100095, accuracy 0.253000\n",
      "iteration 369 / 1500: loss 12.975776, accuracy 0.289000\n",
      "iteration 370 / 1500: loss 12.835675, accuracy 0.305000\n",
      "iteration 371 / 1500: loss 12.702531, accuracy 0.301000\n",
      "iteration 372 / 1500: loss 12.590422, accuracy 0.313000\n",
      "iteration 373 / 1500: loss 12.503658, accuracy 0.294000\n",
      "iteration 374 / 1500: loss 12.403814, accuracy 0.289000\n",
      "iteration 375 / 1500: loss 12.282417, accuracy 0.288000\n",
      "iteration 376 / 1500: loss 12.191022, accuracy 0.289000\n",
      "iteration 377 / 1500: loss 12.029024, accuracy 0.333000\n",
      "iteration 378 / 1500: loss 11.957574, accuracy 0.283000\n",
      "iteration 379 / 1500: loss 11.840152, accuracy 0.315000\n",
      "iteration 380 / 1500: loss 11.760881, accuracy 0.283000\n",
      "iteration 381 / 1500: loss 11.647766, accuracy 0.304000\n",
      "iteration 382 / 1500: loss 11.539496, accuracy 0.312000\n",
      "iteration 383 / 1500: loss 11.411168, accuracy 0.321000\n",
      "iteration 384 / 1500: loss 11.354799, accuracy 0.310000\n",
      "iteration 385 / 1500: loss 11.236957, accuracy 0.308000\n",
      "iteration 386 / 1500: loss 11.162811, accuracy 0.304000\n",
      "iteration 387 / 1500: loss 11.076958, accuracy 0.308000\n",
      "iteration 388 / 1500: loss 10.958139, accuracy 0.322000\n",
      "iteration 389 / 1500: loss 10.873397, accuracy 0.319000\n",
      "iteration 390 / 1500: loss 10.758443, accuracy 0.316000\n",
      "iteration 391 / 1500: loss 10.708195, accuracy 0.298000\n",
      "iteration 392 / 1500: loss 10.609250, accuracy 0.302000\n",
      "iteration 393 / 1500: loss 10.499663, accuracy 0.324000\n",
      "iteration 394 / 1500: loss 10.411493, accuracy 0.330000\n",
      "iteration 395 / 1500: loss 10.323563, accuracy 0.332000\n",
      "iteration 396 / 1500: loss 10.253126, accuracy 0.317000\n",
      "iteration 397 / 1500: loss 10.158529, accuracy 0.305000\n",
      "iteration 398 / 1500: loss 10.047865, accuracy 0.324000\n",
      "iteration 399 / 1500: loss 9.965760, accuracy 0.311000\n",
      "iteration 400 / 1500: loss 9.901882, accuracy 0.311000\n",
      "iteration 401 / 1500: loss 9.826338, accuracy 0.308000\n",
      "iteration 402 / 1500: loss 9.740698, accuracy 0.324000\n",
      "iteration 403 / 1500: loss 9.659009, accuracy 0.315000\n",
      "iteration 404 / 1500: loss 9.573217, accuracy 0.319000\n",
      "iteration 405 / 1500: loss 9.472354, accuracy 0.350000\n",
      "iteration 406 / 1500: loss 9.415824, accuracy 0.306000\n",
      "iteration 407 / 1500: loss 9.359593, accuracy 0.321000\n",
      "iteration 408 / 1500: loss 9.271883, accuracy 0.329000\n",
      "iteration 409 / 1500: loss 9.179744, accuracy 0.313000\n",
      "iteration 410 / 1500: loss 9.128400, accuracy 0.308000\n",
      "iteration 411 / 1500: loss 9.036599, accuracy 0.329000\n",
      "iteration 412 / 1500: loss 8.956261, accuracy 0.328000\n",
      "iteration 413 / 1500: loss 8.889457, accuracy 0.328000\n",
      "iteration 414 / 1500: loss 8.817025, accuracy 0.322000\n",
      "iteration 415 / 1500: loss 8.748880, accuracy 0.322000\n",
      "iteration 416 / 1500: loss 8.686954, accuracy 0.319000\n",
      "iteration 417 / 1500: loss 8.571045, accuracy 0.328000\n",
      "iteration 418 / 1500: loss 8.545557, accuracy 0.316000\n",
      "iteration 419 / 1500: loss 8.477961, accuracy 0.333000\n",
      "iteration 420 / 1500: loss 8.392341, accuracy 0.344000\n",
      "iteration 421 / 1500: loss 8.327850, accuracy 0.345000\n",
      "iteration 422 / 1500: loss 8.268421, accuracy 0.334000\n",
      "iteration 423 / 1500: loss 8.201582, accuracy 0.340000\n",
      "iteration 424 / 1500: loss 8.147306, accuracy 0.321000\n",
      "iteration 425 / 1500: loss 8.089243, accuracy 0.313000\n",
      "iteration 426 / 1500: loss 8.048764, accuracy 0.320000\n",
      "iteration 427 / 1500: loss 7.947110, accuracy 0.312000\n",
      "iteration 428 / 1500: loss 7.891475, accuracy 0.323000\n",
      "iteration 429 / 1500: loss 7.813859, accuracy 0.347000\n",
      "iteration 430 / 1500: loss 7.800889, accuracy 0.317000\n",
      "iteration 431 / 1500: loss 7.675957, accuracy 0.330000\n",
      "iteration 432 / 1500: loss 7.665729, accuracy 0.315000\n",
      "iteration 433 / 1500: loss 7.621699, accuracy 0.305000\n",
      "iteration 434 / 1500: loss 7.528637, accuracy 0.316000\n",
      "iteration 435 / 1500: loss 7.495126, accuracy 0.316000\n",
      "iteration 436 / 1500: loss 7.412059, accuracy 0.329000\n",
      "iteration 437 / 1500: loss 7.356128, accuracy 0.314000\n",
      "iteration 438 / 1500: loss 7.303127, accuracy 0.302000\n",
      "iteration 439 / 1500: loss 7.233457, accuracy 0.341000\n",
      "iteration 440 / 1500: loss 7.215924, accuracy 0.309000\n",
      "iteration 441 / 1500: loss 7.133306, accuracy 0.338000\n",
      "iteration 442 / 1500: loss 7.087382, accuracy 0.321000\n",
      "iteration 443 / 1500: loss 7.076272, accuracy 0.304000\n",
      "iteration 444 / 1500: loss 6.963315, accuracy 0.339000\n",
      "iteration 445 / 1500: loss 6.905046, accuracy 0.332000\n",
      "iteration 446 / 1500: loss 6.912801, accuracy 0.305000\n",
      "iteration 447 / 1500: loss 6.856816, accuracy 0.309000\n",
      "iteration 448 / 1500: loss 6.769809, accuracy 0.333000\n",
      "iteration 449 / 1500: loss 6.723122, accuracy 0.323000\n",
      "iteration 450 / 1500: loss 6.705299, accuracy 0.314000\n",
      "iteration 451 / 1500: loss 6.597263, accuracy 0.343000\n",
      "iteration 452 / 1500: loss 6.587730, accuracy 0.330000\n",
      "iteration 453 / 1500: loss 6.525248, accuracy 0.325000\n",
      "iteration 454 / 1500: loss 6.480190, accuracy 0.329000\n",
      "iteration 455 / 1500: loss 6.439574, accuracy 0.339000\n",
      "iteration 456 / 1500: loss 6.356630, accuracy 0.336000\n",
      "iteration 457 / 1500: loss 6.365724, accuracy 0.295000\n",
      "iteration 458 / 1500: loss 6.313069, accuracy 0.322000\n",
      "iteration 459 / 1500: loss 6.276251, accuracy 0.309000\n",
      "iteration 460 / 1500: loss 6.234030, accuracy 0.308000\n",
      "iteration 461 / 1500: loss 6.154425, accuracy 0.343000\n",
      "iteration 462 / 1500: loss 6.168436, accuracy 0.295000\n",
      "iteration 463 / 1500: loss 6.101992, accuracy 0.333000\n",
      "iteration 464 / 1500: loss 6.043243, accuracy 0.334000\n",
      "iteration 465 / 1500: loss 5.989717, accuracy 0.327000\n",
      "iteration 466 / 1500: loss 5.973170, accuracy 0.314000\n",
      "iteration 467 / 1500: loss 5.889956, accuracy 0.344000\n",
      "iteration 468 / 1500: loss 5.883458, accuracy 0.333000\n",
      "iteration 469 / 1500: loss 5.798599, accuracy 0.345000\n",
      "iteration 470 / 1500: loss 5.810001, accuracy 0.316000\n",
      "iteration 471 / 1500: loss 5.761091, accuracy 0.336000\n",
      "iteration 472 / 1500: loss 5.727995, accuracy 0.312000\n",
      "iteration 473 / 1500: loss 5.680469, accuracy 0.331000\n",
      "iteration 474 / 1500: loss 5.657455, accuracy 0.309000\n",
      "iteration 475 / 1500: loss 5.610981, accuracy 0.322000\n",
      "iteration 476 / 1500: loss 5.582428, accuracy 0.321000\n",
      "iteration 477 / 1500: loss 5.542702, accuracy 0.303000\n",
      "iteration 478 / 1500: loss 5.476813, accuracy 0.347000\n",
      "iteration 479 / 1500: loss 5.446465, accuracy 0.329000\n",
      "iteration 480 / 1500: loss 5.460697, accuracy 0.317000\n",
      "iteration 481 / 1500: loss 5.365941, accuracy 0.346000\n",
      "iteration 482 / 1500: loss 5.345874, accuracy 0.330000\n",
      "iteration 483 / 1500: loss 5.321531, accuracy 0.317000\n",
      "iteration 484 / 1500: loss 5.243995, accuracy 0.355000\n",
      "iteration 485 / 1500: loss 5.278487, accuracy 0.310000\n",
      "iteration 486 / 1500: loss 5.244156, accuracy 0.312000\n",
      "iteration 487 / 1500: loss 5.204026, accuracy 0.323000\n",
      "iteration 488 / 1500: loss 5.122207, accuracy 0.356000\n",
      "iteration 489 / 1500: loss 5.127195, accuracy 0.317000\n",
      "iteration 490 / 1500: loss 5.108894, accuracy 0.298000\n",
      "iteration 491 / 1500: loss 5.068575, accuracy 0.330000\n",
      "iteration 492 / 1500: loss 5.028767, accuracy 0.312000\n",
      "iteration 493 / 1500: loss 4.972616, accuracy 0.339000\n",
      "iteration 494 / 1500: loss 4.952421, accuracy 0.339000\n",
      "iteration 495 / 1500: loss 4.946894, accuracy 0.329000\n",
      "iteration 496 / 1500: loss 4.900687, accuracy 0.310000\n",
      "iteration 497 / 1500: loss 4.872719, accuracy 0.330000\n",
      "iteration 498 / 1500: loss 4.831245, accuracy 0.324000\n",
      "iteration 499 / 1500: loss 4.834447, accuracy 0.308000\n",
      "iteration 500 / 1500: loss 4.794695, accuracy 0.322000\n",
      "iteration 501 / 1500: loss 4.745880, accuracy 0.334000\n",
      "iteration 502 / 1500: loss 4.723875, accuracy 0.335000\n",
      "iteration 503 / 1500: loss 4.696902, accuracy 0.323000\n",
      "iteration 504 / 1500: loss 4.683191, accuracy 0.330000\n",
      "iteration 505 / 1500: loss 4.629381, accuracy 0.342000\n",
      "iteration 506 / 1500: loss 4.635737, accuracy 0.322000\n",
      "iteration 507 / 1500: loss 4.605148, accuracy 0.321000\n",
      "iteration 508 / 1500: loss 4.566877, accuracy 0.339000\n",
      "iteration 509 / 1500: loss 4.547358, accuracy 0.323000\n",
      "iteration 510 / 1500: loss 4.511872, accuracy 0.327000\n",
      "iteration 511 / 1500: loss 4.518680, accuracy 0.325000\n",
      "iteration 512 / 1500: loss 4.459079, accuracy 0.324000\n",
      "iteration 513 / 1500: loss 4.422283, accuracy 0.324000\n",
      "iteration 514 / 1500: loss 4.384353, accuracy 0.357000\n",
      "iteration 515 / 1500: loss 4.395760, accuracy 0.325000\n",
      "iteration 516 / 1500: loss 4.366813, accuracy 0.318000\n",
      "iteration 517 / 1500: loss 4.317147, accuracy 0.338000\n",
      "iteration 518 / 1500: loss 4.308965, accuracy 0.335000\n",
      "iteration 519 / 1500: loss 4.268400, accuracy 0.362000\n",
      "iteration 520 / 1500: loss 4.281737, accuracy 0.315000\n",
      "iteration 521 / 1500: loss 4.193805, accuracy 0.379000\n",
      "iteration 522 / 1500: loss 4.218185, accuracy 0.337000\n",
      "iteration 523 / 1500: loss 4.170668, accuracy 0.349000\n",
      "iteration 524 / 1500: loss 4.172344, accuracy 0.335000\n",
      "iteration 525 / 1500: loss 4.148736, accuracy 0.356000\n",
      "iteration 526 / 1500: loss 4.153440, accuracy 0.320000\n",
      "iteration 527 / 1500: loss 4.137376, accuracy 0.321000\n",
      "iteration 528 / 1500: loss 4.118100, accuracy 0.313000\n",
      "iteration 529 / 1500: loss 4.058772, accuracy 0.335000\n",
      "iteration 530 / 1500: loss 4.064602, accuracy 0.333000\n",
      "iteration 531 / 1500: loss 4.014756, accuracy 0.315000\n",
      "iteration 532 / 1500: loss 4.033655, accuracy 0.317000\n",
      "iteration 533 / 1500: loss 3.982066, accuracy 0.335000\n",
      "iteration 534 / 1500: loss 3.971116, accuracy 0.326000\n",
      "iteration 535 / 1500: loss 3.955324, accuracy 0.329000\n",
      "iteration 536 / 1500: loss 3.931516, accuracy 0.325000\n",
      "iteration 537 / 1500: loss 3.878924, accuracy 0.356000\n",
      "iteration 538 / 1500: loss 3.838109, accuracy 0.374000\n",
      "iteration 539 / 1500: loss 3.844823, accuracy 0.354000\n",
      "iteration 540 / 1500: loss 3.854166, accuracy 0.334000\n",
      "iteration 541 / 1500: loss 3.814388, accuracy 0.331000\n",
      "iteration 542 / 1500: loss 3.798208, accuracy 0.333000\n",
      "iteration 543 / 1500: loss 3.768075, accuracy 0.345000\n",
      "iteration 544 / 1500: loss 3.789143, accuracy 0.315000\n",
      "iteration 545 / 1500: loss 3.719643, accuracy 0.368000\n",
      "iteration 546 / 1500: loss 3.724581, accuracy 0.326000\n",
      "iteration 547 / 1500: loss 3.699858, accuracy 0.366000\n",
      "iteration 548 / 1500: loss 3.683969, accuracy 0.373000\n",
      "iteration 549 / 1500: loss 3.684841, accuracy 0.339000\n",
      "iteration 550 / 1500: loss 3.654986, accuracy 0.364000\n",
      "iteration 551 / 1500: loss 3.647785, accuracy 0.325000\n",
      "iteration 552 / 1500: loss 3.627993, accuracy 0.361000\n",
      "iteration 553 / 1500: loss 3.610540, accuracy 0.326000\n",
      "iteration 554 / 1500: loss 3.612972, accuracy 0.327000\n",
      "iteration 555 / 1500: loss 3.610428, accuracy 0.320000\n",
      "iteration 556 / 1500: loss 3.566348, accuracy 0.346000\n",
      "iteration 557 / 1500: loss 3.573441, accuracy 0.309000\n",
      "iteration 558 / 1500: loss 3.521423, accuracy 0.354000\n",
      "iteration 559 / 1500: loss 3.522761, accuracy 0.345000\n",
      "iteration 560 / 1500: loss 3.515334, accuracy 0.307000\n",
      "iteration 561 / 1500: loss 3.486428, accuracy 0.331000\n",
      "iteration 562 / 1500: loss 3.480410, accuracy 0.325000\n",
      "iteration 563 / 1500: loss 3.463527, accuracy 0.336000\n",
      "iteration 564 / 1500: loss 3.428104, accuracy 0.350000\n",
      "iteration 565 / 1500: loss 3.444158, accuracy 0.322000\n",
      "iteration 566 / 1500: loss 3.423045, accuracy 0.326000\n",
      "iteration 567 / 1500: loss 3.380888, accuracy 0.337000\n",
      "iteration 568 / 1500: loss 3.378536, accuracy 0.334000\n",
      "iteration 569 / 1500: loss 3.363247, accuracy 0.348000\n",
      "iteration 570 / 1500: loss 3.335187, accuracy 0.365000\n",
      "iteration 571 / 1500: loss 3.376349, accuracy 0.303000\n",
      "iteration 572 / 1500: loss 3.335407, accuracy 0.342000\n",
      "iteration 573 / 1500: loss 3.316904, accuracy 0.337000\n",
      "iteration 574 / 1500: loss 3.289408, accuracy 0.342000\n",
      "iteration 575 / 1500: loss 3.271905, accuracy 0.345000\n",
      "iteration 576 / 1500: loss 3.287941, accuracy 0.321000\n",
      "iteration 577 / 1500: loss 3.253756, accuracy 0.331000\n",
      "iteration 578 / 1500: loss 3.238154, accuracy 0.358000\n",
      "iteration 579 / 1500: loss 3.207641, accuracy 0.341000\n",
      "iteration 580 / 1500: loss 3.247407, accuracy 0.313000\n",
      "iteration 581 / 1500: loss 3.221230, accuracy 0.338000\n",
      "iteration 582 / 1500: loss 3.202093, accuracy 0.353000\n",
      "iteration 583 / 1500: loss 3.190507, accuracy 0.344000\n",
      "iteration 584 / 1500: loss 3.192869, accuracy 0.352000\n",
      "iteration 585 / 1500: loss 3.181817, accuracy 0.322000\n",
      "iteration 586 / 1500: loss 3.154900, accuracy 0.336000\n",
      "iteration 587 / 1500: loss 3.162945, accuracy 0.332000\n",
      "iteration 588 / 1500: loss 3.103115, accuracy 0.361000\n",
      "iteration 589 / 1500: loss 3.120695, accuracy 0.344000\n",
      "iteration 590 / 1500: loss 3.115984, accuracy 0.323000\n",
      "iteration 591 / 1500: loss 3.093758, accuracy 0.338000\n",
      "iteration 592 / 1500: loss 3.070724, accuracy 0.355000\n",
      "iteration 593 / 1500: loss 3.080137, accuracy 0.339000\n",
      "iteration 594 / 1500: loss 3.062875, accuracy 0.325000\n",
      "iteration 595 / 1500: loss 3.018606, accuracy 0.365000\n",
      "iteration 596 / 1500: loss 3.056410, accuracy 0.335000\n",
      "iteration 597 / 1500: loss 3.021916, accuracy 0.364000\n",
      "iteration 598 / 1500: loss 3.036742, accuracy 0.319000\n",
      "iteration 599 / 1500: loss 3.015234, accuracy 0.323000\n",
      "iteration 600 / 1500: loss 2.970342, accuracy 0.334000\n",
      "iteration 601 / 1500: loss 2.997031, accuracy 0.316000\n",
      "iteration 602 / 1500: loss 2.971961, accuracy 0.360000\n",
      "iteration 603 / 1500: loss 2.950866, accuracy 0.351000\n",
      "iteration 604 / 1500: loss 2.979423, accuracy 0.349000\n",
      "iteration 605 / 1500: loss 2.958380, accuracy 0.336000\n",
      "iteration 606 / 1500: loss 2.972613, accuracy 0.341000\n",
      "iteration 607 / 1500: loss 2.929552, accuracy 0.333000\n",
      "iteration 608 / 1500: loss 2.915775, accuracy 0.337000\n",
      "iteration 609 / 1500: loss 2.910092, accuracy 0.355000\n",
      "iteration 610 / 1500: loss 2.902197, accuracy 0.346000\n",
      "iteration 611 / 1500: loss 2.931462, accuracy 0.321000\n",
      "iteration 612 / 1500: loss 2.888593, accuracy 0.342000\n",
      "iteration 613 / 1500: loss 2.889018, accuracy 0.310000\n",
      "iteration 614 / 1500: loss 2.858459, accuracy 0.352000\n",
      "iteration 615 / 1500: loss 2.841854, accuracy 0.353000\n",
      "iteration 616 / 1500: loss 2.846569, accuracy 0.324000\n",
      "iteration 617 / 1500: loss 2.840599, accuracy 0.351000\n",
      "iteration 618 / 1500: loss 2.852271, accuracy 0.319000\n",
      "iteration 619 / 1500: loss 2.840979, accuracy 0.318000\n",
      "iteration 620 / 1500: loss 2.835166, accuracy 0.333000\n",
      "iteration 621 / 1500: loss 2.826236, accuracy 0.321000\n",
      "iteration 622 / 1500: loss 2.800103, accuracy 0.353000\n",
      "iteration 623 / 1500: loss 2.803175, accuracy 0.336000\n",
      "iteration 624 / 1500: loss 2.792589, accuracy 0.357000\n",
      "iteration 625 / 1500: loss 2.786071, accuracy 0.335000\n",
      "iteration 626 / 1500: loss 2.771039, accuracy 0.335000\n",
      "iteration 627 / 1500: loss 2.769188, accuracy 0.334000\n",
      "iteration 628 / 1500: loss 2.746999, accuracy 0.352000\n",
      "iteration 629 / 1500: loss 2.740505, accuracy 0.360000\n",
      "iteration 630 / 1500: loss 2.742440, accuracy 0.317000\n",
      "iteration 631 / 1500: loss 2.739622, accuracy 0.329000\n",
      "iteration 632 / 1500: loss 2.731615, accuracy 0.336000\n",
      "iteration 633 / 1500: loss 2.696162, accuracy 0.345000\n",
      "iteration 634 / 1500: loss 2.749729, accuracy 0.325000\n",
      "iteration 635 / 1500: loss 2.707459, accuracy 0.335000\n",
      "iteration 636 / 1500: loss 2.725656, accuracy 0.341000\n",
      "iteration 637 / 1500: loss 2.693384, accuracy 0.347000\n",
      "iteration 638 / 1500: loss 2.672412, accuracy 0.365000\n",
      "iteration 639 / 1500: loss 2.688318, accuracy 0.328000\n",
      "iteration 640 / 1500: loss 2.690302, accuracy 0.307000\n",
      "iteration 641 / 1500: loss 2.656956, accuracy 0.370000\n",
      "iteration 642 / 1500: loss 2.623375, accuracy 0.368000\n",
      "iteration 643 / 1500: loss 2.647702, accuracy 0.323000\n",
      "iteration 644 / 1500: loss 2.671272, accuracy 0.349000\n",
      "iteration 645 / 1500: loss 2.625825, accuracy 0.328000\n",
      "iteration 646 / 1500: loss 2.636370, accuracy 0.350000\n",
      "iteration 647 / 1500: loss 2.645968, accuracy 0.325000\n",
      "iteration 648 / 1500: loss 2.604235, accuracy 0.356000\n",
      "iteration 649 / 1500: loss 2.625037, accuracy 0.340000\n",
      "iteration 650 / 1500: loss 2.616245, accuracy 0.336000\n",
      "iteration 651 / 1500: loss 2.606152, accuracy 0.317000\n",
      "iteration 652 / 1500: loss 2.586918, accuracy 0.363000\n",
      "iteration 653 / 1500: loss 2.594009, accuracy 0.342000\n",
      "iteration 654 / 1500: loss 2.598553, accuracy 0.332000\n",
      "iteration 655 / 1500: loss 2.578439, accuracy 0.339000\n",
      "iteration 656 / 1500: loss 2.589879, accuracy 0.325000\n",
      "iteration 657 / 1500: loss 2.556544, accuracy 0.335000\n",
      "iteration 658 / 1500: loss 2.570222, accuracy 0.337000\n",
      "iteration 659 / 1500: loss 2.573657, accuracy 0.344000\n",
      "iteration 660 / 1500: loss 2.575932, accuracy 0.330000\n",
      "iteration 661 / 1500: loss 2.522572, accuracy 0.349000\n",
      "iteration 662 / 1500: loss 2.531756, accuracy 0.358000\n",
      "iteration 663 / 1500: loss 2.565591, accuracy 0.322000\n",
      "iteration 664 / 1500: loss 2.517552, accuracy 0.328000\n",
      "iteration 665 / 1500: loss 2.519121, accuracy 0.342000\n",
      "iteration 666 / 1500: loss 2.514657, accuracy 0.322000\n",
      "iteration 667 / 1500: loss 2.558532, accuracy 0.318000\n",
      "iteration 668 / 1500: loss 2.529274, accuracy 0.341000\n",
      "iteration 669 / 1500: loss 2.505053, accuracy 0.347000\n",
      "iteration 670 / 1500: loss 2.489959, accuracy 0.343000\n",
      "iteration 671 / 1500: loss 2.494960, accuracy 0.341000\n",
      "iteration 672 / 1500: loss 2.528501, accuracy 0.317000\n",
      "iteration 673 / 1500: loss 2.493239, accuracy 0.350000\n",
      "iteration 674 / 1500: loss 2.482808, accuracy 0.325000\n",
      "iteration 675 / 1500: loss 2.496256, accuracy 0.339000\n",
      "iteration 676 / 1500: loss 2.488273, accuracy 0.346000\n",
      "iteration 677 / 1500: loss 2.480052, accuracy 0.368000\n",
      "iteration 678 / 1500: loss 2.479806, accuracy 0.321000\n",
      "iteration 679 / 1500: loss 2.452153, accuracy 0.357000\n",
      "iteration 680 / 1500: loss 2.453194, accuracy 0.344000\n",
      "iteration 681 / 1500: loss 2.431721, accuracy 0.350000\n",
      "iteration 682 / 1500: loss 2.453684, accuracy 0.325000\n",
      "iteration 683 / 1500: loss 2.437961, accuracy 0.349000\n",
      "iteration 684 / 1500: loss 2.442223, accuracy 0.347000\n",
      "iteration 685 / 1500: loss 2.425283, accuracy 0.351000\n",
      "iteration 686 / 1500: loss 2.427274, accuracy 0.367000\n",
      "iteration 687 / 1500: loss 2.426233, accuracy 0.328000\n",
      "iteration 688 / 1500: loss 2.436201, accuracy 0.337000\n",
      "iteration 689 / 1500: loss 2.406260, accuracy 0.358000\n",
      "iteration 690 / 1500: loss 2.400433, accuracy 0.335000\n",
      "iteration 691 / 1500: loss 2.405683, accuracy 0.345000\n",
      "iteration 692 / 1500: loss 2.405037, accuracy 0.342000\n",
      "iteration 693 / 1500: loss 2.407990, accuracy 0.351000\n",
      "iteration 694 / 1500: loss 2.394134, accuracy 0.341000\n",
      "iteration 695 / 1500: loss 2.402334, accuracy 0.345000\n",
      "iteration 696 / 1500: loss 2.387730, accuracy 0.336000\n",
      "iteration 697 / 1500: loss 2.407185, accuracy 0.305000\n",
      "iteration 698 / 1500: loss 2.429030, accuracy 0.311000\n",
      "iteration 699 / 1500: loss 2.397316, accuracy 0.315000\n",
      "iteration 700 / 1500: loss 2.380779, accuracy 0.333000\n",
      "iteration 701 / 1500: loss 2.370570, accuracy 0.329000\n",
      "iteration 702 / 1500: loss 2.389316, accuracy 0.332000\n",
      "iteration 703 / 1500: loss 2.361324, accuracy 0.350000\n",
      "iteration 704 / 1500: loss 2.369238, accuracy 0.339000\n",
      "iteration 705 / 1500: loss 2.354878, accuracy 0.339000\n",
      "iteration 706 / 1500: loss 2.367153, accuracy 0.339000\n",
      "iteration 707 / 1500: loss 2.397760, accuracy 0.315000\n",
      "iteration 708 / 1500: loss 2.348530, accuracy 0.356000\n",
      "iteration 709 / 1500: loss 2.355957, accuracy 0.332000\n",
      "iteration 710 / 1500: loss 2.357715, accuracy 0.338000\n",
      "iteration 711 / 1500: loss 2.336543, accuracy 0.352000\n",
      "iteration 712 / 1500: loss 2.339936, accuracy 0.346000\n",
      "iteration 713 / 1500: loss 2.352055, accuracy 0.337000\n",
      "iteration 714 / 1500: loss 2.352838, accuracy 0.329000\n",
      "iteration 715 / 1500: loss 2.313339, accuracy 0.354000\n",
      "iteration 716 / 1500: loss 2.320947, accuracy 0.338000\n",
      "iteration 717 / 1500: loss 2.323261, accuracy 0.331000\n",
      "iteration 718 / 1500: loss 2.339013, accuracy 0.335000\n",
      "iteration 719 / 1500: loss 2.309801, accuracy 0.338000\n",
      "iteration 720 / 1500: loss 2.335801, accuracy 0.324000\n",
      "iteration 721 / 1500: loss 2.314793, accuracy 0.329000\n",
      "iteration 722 / 1500: loss 2.289609, accuracy 0.359000\n",
      "iteration 723 / 1500: loss 2.292739, accuracy 0.346000\n",
      "iteration 724 / 1500: loss 2.329405, accuracy 0.336000\n",
      "iteration 725 / 1500: loss 2.315536, accuracy 0.352000\n",
      "iteration 726 / 1500: loss 2.318255, accuracy 0.340000\n",
      "iteration 727 / 1500: loss 2.287983, accuracy 0.363000\n",
      "iteration 728 / 1500: loss 2.280746, accuracy 0.361000\n",
      "iteration 729 / 1500: loss 2.329287, accuracy 0.324000\n",
      "iteration 730 / 1500: loss 2.287740, accuracy 0.338000\n",
      "iteration 731 / 1500: loss 2.261614, accuracy 0.349000\n",
      "iteration 732 / 1500: loss 2.279498, accuracy 0.336000\n",
      "iteration 733 / 1500: loss 2.262293, accuracy 0.341000\n",
      "iteration 734 / 1500: loss 2.258756, accuracy 0.359000\n",
      "iteration 735 / 1500: loss 2.286304, accuracy 0.336000\n",
      "iteration 736 / 1500: loss 2.252743, accuracy 0.365000\n",
      "iteration 737 / 1500: loss 2.244737, accuracy 0.347000\n",
      "iteration 738 / 1500: loss 2.257449, accuracy 0.343000\n",
      "iteration 739 / 1500: loss 2.233240, accuracy 0.367000\n",
      "iteration 740 / 1500: loss 2.262881, accuracy 0.363000\n",
      "iteration 741 / 1500: loss 2.270753, accuracy 0.335000\n",
      "iteration 742 / 1500: loss 2.241803, accuracy 0.344000\n",
      "iteration 743 / 1500: loss 2.241484, accuracy 0.335000\n",
      "iteration 744 / 1500: loss 2.256899, accuracy 0.342000\n",
      "iteration 745 / 1500: loss 2.270334, accuracy 0.334000\n",
      "iteration 746 / 1500: loss 2.240830, accuracy 0.346000\n",
      "iteration 747 / 1500: loss 2.234260, accuracy 0.350000\n",
      "iteration 748 / 1500: loss 2.258939, accuracy 0.350000\n",
      "iteration 749 / 1500: loss 2.272670, accuracy 0.336000\n",
      "iteration 750 / 1500: loss 2.272754, accuracy 0.304000\n",
      "iteration 751 / 1500: loss 2.268139, accuracy 0.319000\n",
      "iteration 752 / 1500: loss 2.237852, accuracy 0.335000\n",
      "iteration 753 / 1500: loss 2.281554, accuracy 0.329000\n",
      "iteration 754 / 1500: loss 2.219812, accuracy 0.370000\n",
      "iteration 755 / 1500: loss 2.254508, accuracy 0.338000\n",
      "iteration 756 / 1500: loss 2.243559, accuracy 0.323000\n",
      "iteration 757 / 1500: loss 2.255852, accuracy 0.326000\n",
      "iteration 758 / 1500: loss 2.212665, accuracy 0.336000\n",
      "iteration 759 / 1500: loss 2.229840, accuracy 0.349000\n",
      "iteration 760 / 1500: loss 2.225893, accuracy 0.358000\n",
      "iteration 761 / 1500: loss 2.217107, accuracy 0.363000\n",
      "iteration 762 / 1500: loss 2.250473, accuracy 0.316000\n",
      "iteration 763 / 1500: loss 2.232514, accuracy 0.332000\n",
      "iteration 764 / 1500: loss 2.224379, accuracy 0.327000\n",
      "iteration 765 / 1500: loss 2.203300, accuracy 0.371000\n",
      "iteration 766 / 1500: loss 2.190037, accuracy 0.358000\n",
      "iteration 767 / 1500: loss 2.213405, accuracy 0.350000\n",
      "iteration 768 / 1500: loss 2.204629, accuracy 0.369000\n",
      "iteration 769 / 1500: loss 2.203700, accuracy 0.321000\n",
      "iteration 770 / 1500: loss 2.215165, accuracy 0.356000\n",
      "iteration 771 / 1500: loss 2.209888, accuracy 0.341000\n",
      "iteration 772 / 1500: loss 2.191184, accuracy 0.349000\n",
      "iteration 773 / 1500: loss 2.222712, accuracy 0.311000\n",
      "iteration 774 / 1500: loss 2.201130, accuracy 0.337000\n",
      "iteration 775 / 1500: loss 2.218077, accuracy 0.329000\n",
      "iteration 776 / 1500: loss 2.212276, accuracy 0.335000\n",
      "iteration 777 / 1500: loss 2.233978, accuracy 0.336000\n",
      "iteration 778 / 1500: loss 2.196551, accuracy 0.341000\n",
      "iteration 779 / 1500: loss 2.214296, accuracy 0.344000\n",
      "iteration 780 / 1500: loss 2.187601, accuracy 0.351000\n",
      "iteration 781 / 1500: loss 2.165127, accuracy 0.348000\n",
      "iteration 782 / 1500: loss 2.218885, accuracy 0.318000\n",
      "iteration 783 / 1500: loss 2.182485, accuracy 0.321000\n",
      "iteration 784 / 1500: loss 2.188080, accuracy 0.341000\n",
      "iteration 785 / 1500: loss 2.181847, accuracy 0.347000\n",
      "iteration 786 / 1500: loss 2.196487, accuracy 0.335000\n",
      "iteration 787 / 1500: loss 2.184344, accuracy 0.339000\n",
      "iteration 788 / 1500: loss 2.193606, accuracy 0.328000\n",
      "iteration 789 / 1500: loss 2.200662, accuracy 0.327000\n",
      "iteration 790 / 1500: loss 2.173538, accuracy 0.328000\n",
      "iteration 791 / 1500: loss 2.184461, accuracy 0.339000\n",
      "iteration 792 / 1500: loss 2.164153, accuracy 0.345000\n",
      "iteration 793 / 1500: loss 2.164548, accuracy 0.342000\n",
      "iteration 794 / 1500: loss 2.202242, accuracy 0.334000\n",
      "iteration 795 / 1500: loss 2.203028, accuracy 0.329000\n",
      "iteration 796 / 1500: loss 2.177902, accuracy 0.344000\n",
      "iteration 797 / 1500: loss 2.205835, accuracy 0.335000\n",
      "iteration 798 / 1500: loss 2.165798, accuracy 0.355000\n",
      "iteration 799 / 1500: loss 2.136499, accuracy 0.340000\n",
      "iteration 800 / 1500: loss 2.152764, accuracy 0.353000\n",
      "iteration 801 / 1500: loss 2.168754, accuracy 0.335000\n",
      "iteration 802 / 1500: loss 2.189111, accuracy 0.328000\n",
      "iteration 803 / 1500: loss 2.159239, accuracy 0.329000\n",
      "iteration 804 / 1500: loss 2.166248, accuracy 0.347000\n",
      "iteration 805 / 1500: loss 2.163189, accuracy 0.330000\n",
      "iteration 806 / 1500: loss 2.171846, accuracy 0.324000\n",
      "iteration 807 / 1500: loss 2.187794, accuracy 0.345000\n",
      "iteration 808 / 1500: loss 2.158298, accuracy 0.340000\n",
      "iteration 809 / 1500: loss 2.157429, accuracy 0.340000\n",
      "iteration 810 / 1500: loss 2.147432, accuracy 0.353000\n",
      "iteration 811 / 1500: loss 2.155899, accuracy 0.338000\n",
      "iteration 812 / 1500: loss 2.129408, accuracy 0.341000\n",
      "iteration 813 / 1500: loss 2.110552, accuracy 0.362000\n",
      "iteration 814 / 1500: loss 2.143693, accuracy 0.348000\n",
      "iteration 815 / 1500: loss 2.146116, accuracy 0.341000\n",
      "iteration 816 / 1500: loss 2.179214, accuracy 0.320000\n",
      "iteration 817 / 1500: loss 2.142903, accuracy 0.338000\n",
      "iteration 818 / 1500: loss 2.152987, accuracy 0.327000\n",
      "iteration 819 / 1500: loss 2.139185, accuracy 0.353000\n",
      "iteration 820 / 1500: loss 2.155407, accuracy 0.362000\n",
      "iteration 821 / 1500: loss 2.135546, accuracy 0.354000\n",
      "iteration 822 / 1500: loss 2.147545, accuracy 0.321000\n",
      "iteration 823 / 1500: loss 2.132671, accuracy 0.351000\n",
      "iteration 824 / 1500: loss 2.167948, accuracy 0.312000\n",
      "iteration 825 / 1500: loss 2.169326, accuracy 0.329000\n",
      "iteration 826 / 1500: loss 2.123123, accuracy 0.360000\n",
      "iteration 827 / 1500: loss 2.142007, accuracy 0.318000\n",
      "iteration 828 / 1500: loss 2.119143, accuracy 0.374000\n",
      "iteration 829 / 1500: loss 2.129709, accuracy 0.334000\n",
      "iteration 830 / 1500: loss 2.121772, accuracy 0.380000\n",
      "iteration 831 / 1500: loss 2.140273, accuracy 0.340000\n",
      "iteration 832 / 1500: loss 2.139426, accuracy 0.316000\n",
      "iteration 833 / 1500: loss 2.131760, accuracy 0.337000\n",
      "iteration 834 / 1500: loss 2.121961, accuracy 0.341000\n",
      "iteration 835 / 1500: loss 2.142959, accuracy 0.350000\n",
      "iteration 836 / 1500: loss 2.138417, accuracy 0.353000\n",
      "iteration 837 / 1500: loss 2.142786, accuracy 0.317000\n",
      "iteration 838 / 1500: loss 2.105060, accuracy 0.355000\n",
      "iteration 839 / 1500: loss 2.142520, accuracy 0.343000\n",
      "iteration 840 / 1500: loss 2.114375, accuracy 0.342000\n",
      "iteration 841 / 1500: loss 2.122193, accuracy 0.338000\n",
      "iteration 842 / 1500: loss 2.109072, accuracy 0.349000\n",
      "iteration 843 / 1500: loss 2.127633, accuracy 0.343000\n",
      "iteration 844 / 1500: loss 2.122610, accuracy 0.344000\n",
      "iteration 845 / 1500: loss 2.128031, accuracy 0.338000\n",
      "iteration 846 / 1500: loss 2.122085, accuracy 0.340000\n",
      "iteration 847 / 1500: loss 2.127975, accuracy 0.347000\n",
      "iteration 848 / 1500: loss 2.123256, accuracy 0.341000\n",
      "iteration 849 / 1500: loss 2.118896, accuracy 0.339000\n",
      "iteration 850 / 1500: loss 2.138751, accuracy 0.349000\n",
      "iteration 851 / 1500: loss 2.116448, accuracy 0.327000\n",
      "iteration 852 / 1500: loss 2.125608, accuracy 0.331000\n",
      "iteration 853 / 1500: loss 2.093900, accuracy 0.344000\n",
      "iteration 854 / 1500: loss 2.143565, accuracy 0.331000\n",
      "iteration 855 / 1500: loss 2.146672, accuracy 0.331000\n",
      "iteration 856 / 1500: loss 2.114746, accuracy 0.328000\n",
      "iteration 857 / 1500: loss 2.157184, accuracy 0.314000\n",
      "iteration 858 / 1500: loss 2.102771, accuracy 0.358000\n",
      "iteration 859 / 1500: loss 2.123276, accuracy 0.337000\n",
      "iteration 860 / 1500: loss 2.089785, accuracy 0.360000\n",
      "iteration 861 / 1500: loss 2.114203, accuracy 0.337000\n",
      "iteration 862 / 1500: loss 2.136001, accuracy 0.319000\n",
      "iteration 863 / 1500: loss 2.104336, accuracy 0.345000\n",
      "iteration 864 / 1500: loss 2.086261, accuracy 0.358000\n",
      "iteration 865 / 1500: loss 2.105211, accuracy 0.346000\n",
      "iteration 866 / 1500: loss 2.141170, accuracy 0.338000\n",
      "iteration 867 / 1500: loss 2.104475, accuracy 0.342000\n",
      "iteration 868 / 1500: loss 2.104222, accuracy 0.352000\n",
      "iteration 869 / 1500: loss 2.096248, accuracy 0.352000\n",
      "iteration 870 / 1500: loss 2.118742, accuracy 0.334000\n",
      "iteration 871 / 1500: loss 2.098921, accuracy 0.359000\n",
      "iteration 872 / 1500: loss 2.119641, accuracy 0.346000\n",
      "iteration 873 / 1500: loss 2.087104, accuracy 0.363000\n",
      "iteration 874 / 1500: loss 2.090047, accuracy 0.360000\n",
      "iteration 875 / 1500: loss 2.091969, accuracy 0.341000\n",
      "iteration 876 / 1500: loss 2.110075, accuracy 0.341000\n",
      "iteration 877 / 1500: loss 2.108612, accuracy 0.344000\n",
      "iteration 878 / 1500: loss 2.071739, accuracy 0.338000\n",
      "iteration 879 / 1500: loss 2.112043, accuracy 0.340000\n",
      "iteration 880 / 1500: loss 2.105592, accuracy 0.324000\n",
      "iteration 881 / 1500: loss 2.104941, accuracy 0.333000\n",
      "iteration 882 / 1500: loss 2.066427, accuracy 0.361000\n",
      "iteration 883 / 1500: loss 2.124460, accuracy 0.347000\n",
      "iteration 884 / 1500: loss 2.079001, accuracy 0.357000\n",
      "iteration 885 / 1500: loss 2.131105, accuracy 0.342000\n",
      "iteration 886 / 1500: loss 2.098361, accuracy 0.350000\n",
      "iteration 887 / 1500: loss 2.110662, accuracy 0.313000\n",
      "iteration 888 / 1500: loss 2.083817, accuracy 0.348000\n",
      "iteration 889 / 1500: loss 2.092099, accuracy 0.324000\n",
      "iteration 890 / 1500: loss 2.063785, accuracy 0.349000\n",
      "iteration 891 / 1500: loss 2.101245, accuracy 0.338000\n",
      "iteration 892 / 1500: loss 2.091736, accuracy 0.347000\n",
      "iteration 893 / 1500: loss 2.068945, accuracy 0.339000\n",
      "iteration 894 / 1500: loss 2.074850, accuracy 0.351000\n",
      "iteration 895 / 1500: loss 2.105976, accuracy 0.337000\n",
      "iteration 896 / 1500: loss 2.107493, accuracy 0.326000\n",
      "iteration 897 / 1500: loss 2.094808, accuracy 0.337000\n",
      "iteration 898 / 1500: loss 2.099138, accuracy 0.308000\n",
      "iteration 899 / 1500: loss 2.107503, accuracy 0.329000\n",
      "iteration 900 / 1500: loss 2.114168, accuracy 0.339000\n",
      "iteration 901 / 1500: loss 2.085167, accuracy 0.332000\n",
      "iteration 902 / 1500: loss 2.111694, accuracy 0.290000\n",
      "iteration 903 / 1500: loss 2.085589, accuracy 0.355000\n",
      "iteration 904 / 1500: loss 2.097390, accuracy 0.347000\n",
      "iteration 905 / 1500: loss 2.095291, accuracy 0.334000\n",
      "iteration 906 / 1500: loss 2.073091, accuracy 0.356000\n",
      "iteration 907 / 1500: loss 2.099683, accuracy 0.340000\n",
      "iteration 908 / 1500: loss 2.088193, accuracy 0.345000\n",
      "iteration 909 / 1500: loss 2.067721, accuracy 0.352000\n",
      "iteration 910 / 1500: loss 2.062677, accuracy 0.354000\n",
      "iteration 911 / 1500: loss 2.090021, accuracy 0.339000\n",
      "iteration 912 / 1500: loss 2.085694, accuracy 0.356000\n",
      "iteration 913 / 1500: loss 2.081465, accuracy 0.365000\n",
      "iteration 914 / 1500: loss 2.104213, accuracy 0.347000\n",
      "iteration 915 / 1500: loss 2.092440, accuracy 0.321000\n",
      "iteration 916 / 1500: loss 2.119513, accuracy 0.314000\n",
      "iteration 917 / 1500: loss 2.085606, accuracy 0.344000\n",
      "iteration 918 / 1500: loss 2.060154, accuracy 0.361000\n",
      "iteration 919 / 1500: loss 2.082026, accuracy 0.343000\n",
      "iteration 920 / 1500: loss 2.103496, accuracy 0.324000\n",
      "iteration 921 / 1500: loss 2.076145, accuracy 0.361000\n",
      "iteration 922 / 1500: loss 2.083879, accuracy 0.321000\n",
      "iteration 923 / 1500: loss 2.098479, accuracy 0.323000\n",
      "iteration 924 / 1500: loss 2.103072, accuracy 0.318000\n",
      "iteration 925 / 1500: loss 2.080071, accuracy 0.330000\n",
      "iteration 926 / 1500: loss 2.086482, accuracy 0.332000\n",
      "iteration 927 / 1500: loss 2.075642, accuracy 0.337000\n",
      "iteration 928 / 1500: loss 2.073935, accuracy 0.343000\n",
      "iteration 929 / 1500: loss 2.100754, accuracy 0.324000\n",
      "iteration 930 / 1500: loss 2.101288, accuracy 0.321000\n",
      "iteration 931 / 1500: loss 2.062332, accuracy 0.356000\n",
      "iteration 932 / 1500: loss 2.068728, accuracy 0.350000\n",
      "iteration 933 / 1500: loss 2.079675, accuracy 0.323000\n",
      "iteration 934 / 1500: loss 2.121297, accuracy 0.318000\n",
      "iteration 935 / 1500: loss 2.090706, accuracy 0.325000\n",
      "iteration 936 / 1500: loss 2.072584, accuracy 0.339000\n",
      "iteration 937 / 1500: loss 2.077239, accuracy 0.359000\n",
      "iteration 938 / 1500: loss 2.080182, accuracy 0.325000\n",
      "iteration 939 / 1500: loss 2.091913, accuracy 0.337000\n",
      "iteration 940 / 1500: loss 2.088531, accuracy 0.347000\n",
      "iteration 941 / 1500: loss 2.053628, accuracy 0.344000\n",
      "iteration 942 / 1500: loss 2.084793, accuracy 0.327000\n",
      "iteration 943 / 1500: loss 2.075058, accuracy 0.333000\n",
      "iteration 944 / 1500: loss 2.068779, accuracy 0.340000\n",
      "iteration 945 / 1500: loss 2.075694, accuracy 0.327000\n",
      "iteration 946 / 1500: loss 2.096537, accuracy 0.335000\n",
      "iteration 947 / 1500: loss 2.068768, accuracy 0.342000\n",
      "iteration 948 / 1500: loss 2.091986, accuracy 0.323000\n",
      "iteration 949 / 1500: loss 2.090838, accuracy 0.333000\n",
      "iteration 950 / 1500: loss 2.094506, accuracy 0.326000\n",
      "iteration 951 / 1500: loss 2.090124, accuracy 0.327000\n",
      "iteration 952 / 1500: loss 2.074955, accuracy 0.331000\n",
      "iteration 953 / 1500: loss 2.084894, accuracy 0.329000\n",
      "iteration 954 / 1500: loss 2.070484, accuracy 0.329000\n",
      "iteration 955 / 1500: loss 2.048800, accuracy 0.363000\n",
      "iteration 956 / 1500: loss 2.052217, accuracy 0.353000\n",
      "iteration 957 / 1500: loss 2.083903, accuracy 0.336000\n",
      "iteration 958 / 1500: loss 2.101240, accuracy 0.325000\n",
      "iteration 959 / 1500: loss 2.082193, accuracy 0.334000\n",
      "iteration 960 / 1500: loss 2.082048, accuracy 0.323000\n",
      "iteration 961 / 1500: loss 2.052708, accuracy 0.366000\n",
      "iteration 962 / 1500: loss 2.059225, accuracy 0.335000\n",
      "iteration 963 / 1500: loss 2.086537, accuracy 0.323000\n",
      "iteration 964 / 1500: loss 2.061000, accuracy 0.354000\n",
      "iteration 965 / 1500: loss 2.062579, accuracy 0.341000\n",
      "iteration 966 / 1500: loss 2.097146, accuracy 0.323000\n",
      "iteration 967 / 1500: loss 2.078002, accuracy 0.338000\n",
      "iteration 968 / 1500: loss 2.067473, accuracy 0.331000\n",
      "iteration 969 / 1500: loss 2.077519, accuracy 0.333000\n",
      "iteration 970 / 1500: loss 2.075226, accuracy 0.334000\n",
      "iteration 971 / 1500: loss 2.079669, accuracy 0.332000\n",
      "iteration 972 / 1500: loss 2.088149, accuracy 0.338000\n",
      "iteration 973 / 1500: loss 2.083360, accuracy 0.332000\n",
      "iteration 974 / 1500: loss 2.082239, accuracy 0.318000\n",
      "iteration 975 / 1500: loss 2.065066, accuracy 0.349000\n",
      "iteration 976 / 1500: loss 2.042440, accuracy 0.375000\n",
      "iteration 977 / 1500: loss 2.049554, accuracy 0.361000\n",
      "iteration 978 / 1500: loss 2.044102, accuracy 0.367000\n",
      "iteration 979 / 1500: loss 2.071448, accuracy 0.334000\n",
      "iteration 980 / 1500: loss 2.076958, accuracy 0.346000\n",
      "iteration 981 / 1500: loss 2.068735, accuracy 0.363000\n",
      "iteration 982 / 1500: loss 2.039334, accuracy 0.336000\n",
      "iteration 983 / 1500: loss 2.082098, accuracy 0.352000\n",
      "iteration 984 / 1500: loss 2.079083, accuracy 0.337000\n",
      "iteration 985 / 1500: loss 2.039339, accuracy 0.374000\n",
      "iteration 986 / 1500: loss 2.071774, accuracy 0.332000\n",
      "iteration 987 / 1500: loss 2.069431, accuracy 0.331000\n",
      "iteration 988 / 1500: loss 2.094555, accuracy 0.325000\n",
      "iteration 989 / 1500: loss 2.067945, accuracy 0.352000\n",
      "iteration 990 / 1500: loss 2.066996, accuracy 0.325000\n",
      "iteration 991 / 1500: loss 2.056856, accuracy 0.355000\n",
      "iteration 992 / 1500: loss 2.066252, accuracy 0.357000\n",
      "iteration 993 / 1500: loss 2.080549, accuracy 0.328000\n",
      "iteration 994 / 1500: loss 2.083653, accuracy 0.322000\n",
      "iteration 995 / 1500: loss 2.068966, accuracy 0.326000\n",
      "iteration 996 / 1500: loss 2.063999, accuracy 0.343000\n",
      "iteration 997 / 1500: loss 2.094637, accuracy 0.331000\n",
      "iteration 998 / 1500: loss 2.079966, accuracy 0.328000\n",
      "iteration 999 / 1500: loss 2.052918, accuracy 0.346000\n",
      "iteration 1000 / 1500: loss 2.056209, accuracy 0.344000\n",
      "iteration 1001 / 1500: loss 2.069080, accuracy 0.356000\n",
      "iteration 1002 / 1500: loss 2.059147, accuracy 0.339000\n",
      "iteration 1003 / 1500: loss 2.089027, accuracy 0.313000\n",
      "iteration 1004 / 1500: loss 2.051324, accuracy 0.346000\n",
      "iteration 1005 / 1500: loss 2.066113, accuracy 0.331000\n",
      "iteration 1006 / 1500: loss 2.096735, accuracy 0.319000\n",
      "iteration 1007 / 1500: loss 2.068017, accuracy 0.325000\n",
      "iteration 1008 / 1500: loss 2.100051, accuracy 0.326000\n",
      "iteration 1009 / 1500: loss 2.071684, accuracy 0.329000\n",
      "iteration 1010 / 1500: loss 2.037355, accuracy 0.364000\n",
      "iteration 1011 / 1500: loss 2.064218, accuracy 0.350000\n",
      "iteration 1012 / 1500: loss 2.079958, accuracy 0.327000\n",
      "iteration 1013 / 1500: loss 2.054945, accuracy 0.351000\n",
      "iteration 1014 / 1500: loss 2.072059, accuracy 0.334000\n",
      "iteration 1015 / 1500: loss 2.066611, accuracy 0.315000\n",
      "iteration 1016 / 1500: loss 2.055223, accuracy 0.356000\n",
      "iteration 1017 / 1500: loss 2.058737, accuracy 0.341000\n",
      "iteration 1018 / 1500: loss 2.065028, accuracy 0.334000\n",
      "iteration 1019 / 1500: loss 2.062657, accuracy 0.328000\n",
      "iteration 1020 / 1500: loss 2.068291, accuracy 0.358000\n",
      "iteration 1021 / 1500: loss 2.016625, accuracy 0.368000\n",
      "iteration 1022 / 1500: loss 2.052109, accuracy 0.336000\n",
      "iteration 1023 / 1500: loss 2.066901, accuracy 0.346000\n",
      "iteration 1024 / 1500: loss 2.061013, accuracy 0.341000\n",
      "iteration 1025 / 1500: loss 2.051622, accuracy 0.356000\n",
      "iteration 1026 / 1500: loss 2.068485, accuracy 0.338000\n",
      "iteration 1027 / 1500: loss 2.056929, accuracy 0.345000\n",
      "iteration 1028 / 1500: loss 2.062821, accuracy 0.340000\n",
      "iteration 1029 / 1500: loss 2.064732, accuracy 0.339000\n",
      "iteration 1030 / 1500: loss 2.078606, accuracy 0.335000\n",
      "iteration 1031 / 1500: loss 2.055034, accuracy 0.337000\n",
      "iteration 1032 / 1500: loss 2.061521, accuracy 0.355000\n",
      "iteration 1033 / 1500: loss 2.034372, accuracy 0.335000\n",
      "iteration 1034 / 1500: loss 2.045873, accuracy 0.348000\n",
      "iteration 1035 / 1500: loss 2.054866, accuracy 0.348000\n",
      "iteration 1036 / 1500: loss 2.080760, accuracy 0.339000\n",
      "iteration 1037 / 1500: loss 2.077952, accuracy 0.320000\n",
      "iteration 1038 / 1500: loss 2.035200, accuracy 0.332000\n",
      "iteration 1039 / 1500: loss 2.054027, accuracy 0.346000\n",
      "iteration 1040 / 1500: loss 2.045790, accuracy 0.355000\n",
      "iteration 1041 / 1500: loss 2.057973, accuracy 0.351000\n",
      "iteration 1042 / 1500: loss 2.047900, accuracy 0.357000\n",
      "iteration 1043 / 1500: loss 2.083372, accuracy 0.323000\n",
      "iteration 1044 / 1500: loss 2.057751, accuracy 0.347000\n",
      "iteration 1045 / 1500: loss 2.050383, accuracy 0.341000\n",
      "iteration 1046 / 1500: loss 2.040965, accuracy 0.360000\n",
      "iteration 1047 / 1500: loss 2.053951, accuracy 0.348000\n",
      "iteration 1048 / 1500: loss 2.038991, accuracy 0.348000\n",
      "iteration 1049 / 1500: loss 2.056349, accuracy 0.333000\n",
      "iteration 1050 / 1500: loss 2.073013, accuracy 0.363000\n",
      "iteration 1051 / 1500: loss 2.071825, accuracy 0.339000\n",
      "iteration 1052 / 1500: loss 2.050840, accuracy 0.347000\n",
      "iteration 1053 / 1500: loss 2.064786, accuracy 0.342000\n",
      "iteration 1054 / 1500: loss 2.037774, accuracy 0.373000\n",
      "iteration 1055 / 1500: loss 2.035643, accuracy 0.375000\n",
      "iteration 1056 / 1500: loss 2.074438, accuracy 0.339000\n",
      "iteration 1057 / 1500: loss 2.036863, accuracy 0.365000\n",
      "iteration 1058 / 1500: loss 2.049391, accuracy 0.351000\n",
      "iteration 1059 / 1500: loss 2.056356, accuracy 0.324000\n",
      "iteration 1060 / 1500: loss 2.044676, accuracy 0.362000\n",
      "iteration 1061 / 1500: loss 2.082579, accuracy 0.323000\n",
      "iteration 1062 / 1500: loss 2.041034, accuracy 0.321000\n",
      "iteration 1063 / 1500: loss 2.058732, accuracy 0.338000\n",
      "iteration 1064 / 1500: loss 2.039236, accuracy 0.359000\n",
      "iteration 1065 / 1500: loss 2.065241, accuracy 0.326000\n",
      "iteration 1066 / 1500: loss 2.077512, accuracy 0.321000\n",
      "iteration 1067 / 1500: loss 2.071838, accuracy 0.343000\n",
      "iteration 1068 / 1500: loss 2.090263, accuracy 0.327000\n",
      "iteration 1069 / 1500: loss 2.033573, accuracy 0.333000\n",
      "iteration 1070 / 1500: loss 2.055306, accuracy 0.348000\n",
      "iteration 1071 / 1500: loss 2.067031, accuracy 0.346000\n",
      "iteration 1072 / 1500: loss 2.024080, accuracy 0.352000\n",
      "iteration 1073 / 1500: loss 2.070313, accuracy 0.332000\n",
      "iteration 1074 / 1500: loss 2.051380, accuracy 0.337000\n",
      "iteration 1075 / 1500: loss 2.056509, accuracy 0.353000\n",
      "iteration 1076 / 1500: loss 2.073536, accuracy 0.319000\n",
      "iteration 1077 / 1500: loss 2.064248, accuracy 0.324000\n",
      "iteration 1078 / 1500: loss 2.053456, accuracy 0.337000\n",
      "iteration 1079 / 1500: loss 2.072055, accuracy 0.329000\n",
      "iteration 1080 / 1500: loss 2.093203, accuracy 0.327000\n",
      "iteration 1081 / 1500: loss 2.026364, accuracy 0.348000\n",
      "iteration 1082 / 1500: loss 2.049851, accuracy 0.351000\n",
      "iteration 1083 / 1500: loss 2.054285, accuracy 0.321000\n",
      "iteration 1084 / 1500: loss 2.068016, accuracy 0.334000\n",
      "iteration 1085 / 1500: loss 2.054533, accuracy 0.327000\n",
      "iteration 1086 / 1500: loss 2.057898, accuracy 0.345000\n",
      "iteration 1087 / 1500: loss 2.078502, accuracy 0.320000\n",
      "iteration 1088 / 1500: loss 2.082224, accuracy 0.330000\n",
      "iteration 1089 / 1500: loss 2.054796, accuracy 0.336000\n",
      "iteration 1090 / 1500: loss 2.045222, accuracy 0.362000\n",
      "iteration 1091 / 1500: loss 2.061853, accuracy 0.350000\n",
      "iteration 1092 / 1500: loss 2.043744, accuracy 0.370000\n",
      "iteration 1093 / 1500: loss 2.044614, accuracy 0.351000\n",
      "iteration 1094 / 1500: loss 2.058578, accuracy 0.337000\n",
      "iteration 1095 / 1500: loss 2.050280, accuracy 0.365000\n",
      "iteration 1096 / 1500: loss 2.071497, accuracy 0.340000\n",
      "iteration 1097 / 1500: loss 2.056222, accuracy 0.326000\n",
      "iteration 1098 / 1500: loss 2.015983, accuracy 0.363000\n",
      "iteration 1099 / 1500: loss 2.076883, accuracy 0.349000\n",
      "iteration 1100 / 1500: loss 2.062422, accuracy 0.311000\n",
      "iteration 1101 / 1500: loss 2.060121, accuracy 0.332000\n",
      "iteration 1102 / 1500: loss 2.028388, accuracy 0.371000\n",
      "iteration 1103 / 1500: loss 2.059969, accuracy 0.349000\n",
      "iteration 1104 / 1500: loss 2.041506, accuracy 0.347000\n",
      "iteration 1105 / 1500: loss 2.059224, accuracy 0.356000\n",
      "iteration 1106 / 1500: loss 2.068260, accuracy 0.345000\n",
      "iteration 1107 / 1500: loss 2.039336, accuracy 0.356000\n",
      "iteration 1108 / 1500: loss 2.069098, accuracy 0.330000\n",
      "iteration 1109 / 1500: loss 2.060373, accuracy 0.355000\n",
      "iteration 1110 / 1500: loss 2.042450, accuracy 0.350000\n",
      "iteration 1111 / 1500: loss 2.065653, accuracy 0.346000\n",
      "iteration 1112 / 1500: loss 2.032503, accuracy 0.356000\n",
      "iteration 1113 / 1500: loss 2.038638, accuracy 0.373000\n",
      "iteration 1114 / 1500: loss 2.084518, accuracy 0.323000\n",
      "iteration 1115 / 1500: loss 2.053996, accuracy 0.342000\n",
      "iteration 1116 / 1500: loss 2.059028, accuracy 0.334000\n",
      "iteration 1117 / 1500: loss 2.049613, accuracy 0.337000\n",
      "iteration 1118 / 1500: loss 2.053817, accuracy 0.349000\n",
      "iteration 1119 / 1500: loss 2.046394, accuracy 0.343000\n",
      "iteration 1120 / 1500: loss 2.056169, accuracy 0.358000\n",
      "iteration 1121 / 1500: loss 2.051917, accuracy 0.376000\n",
      "iteration 1122 / 1500: loss 2.046750, accuracy 0.344000\n",
      "iteration 1123 / 1500: loss 2.088827, accuracy 0.324000\n",
      "iteration 1124 / 1500: loss 2.046488, accuracy 0.337000\n",
      "iteration 1125 / 1500: loss 2.080498, accuracy 0.319000\n",
      "iteration 1126 / 1500: loss 2.065384, accuracy 0.341000\n",
      "iteration 1127 / 1500: loss 2.081922, accuracy 0.346000\n",
      "iteration 1128 / 1500: loss 2.068643, accuracy 0.336000\n",
      "iteration 1129 / 1500: loss 2.058375, accuracy 0.328000\n",
      "iteration 1130 / 1500: loss 2.058954, accuracy 0.338000\n",
      "iteration 1131 / 1500: loss 2.045736, accuracy 0.342000\n",
      "iteration 1132 / 1500: loss 2.042315, accuracy 0.364000\n",
      "iteration 1133 / 1500: loss 2.047329, accuracy 0.329000\n",
      "iteration 1134 / 1500: loss 2.084586, accuracy 0.331000\n",
      "iteration 1135 / 1500: loss 2.063849, accuracy 0.345000\n",
      "iteration 1136 / 1500: loss 2.073132, accuracy 0.327000\n",
      "iteration 1137 / 1500: loss 2.031264, accuracy 0.329000\n",
      "iteration 1138 / 1500: loss 2.038305, accuracy 0.363000\n",
      "iteration 1139 / 1500: loss 2.043047, accuracy 0.339000\n",
      "iteration 1140 / 1500: loss 2.047871, accuracy 0.338000\n",
      "iteration 1141 / 1500: loss 2.048347, accuracy 0.345000\n",
      "iteration 1142 / 1500: loss 2.015693, accuracy 0.362000\n",
      "iteration 1143 / 1500: loss 2.015204, accuracy 0.364000\n",
      "iteration 1144 / 1500: loss 2.039885, accuracy 0.352000\n",
      "iteration 1145 / 1500: loss 2.087068, accuracy 0.340000\n",
      "iteration 1146 / 1500: loss 2.034643, accuracy 0.347000\n",
      "iteration 1147 / 1500: loss 2.066690, accuracy 0.344000\n",
      "iteration 1148 / 1500: loss 2.077442, accuracy 0.324000\n",
      "iteration 1149 / 1500: loss 2.044896, accuracy 0.357000\n",
      "iteration 1150 / 1500: loss 2.068196, accuracy 0.319000\n",
      "iteration 1151 / 1500: loss 2.040901, accuracy 0.351000\n",
      "iteration 1152 / 1500: loss 2.086907, accuracy 0.317000\n",
      "iteration 1153 / 1500: loss 2.072852, accuracy 0.327000\n",
      "iteration 1154 / 1500: loss 2.043249, accuracy 0.353000\n",
      "iteration 1155 / 1500: loss 2.045466, accuracy 0.349000\n",
      "iteration 1156 / 1500: loss 2.086311, accuracy 0.334000\n",
      "iteration 1157 / 1500: loss 2.032037, accuracy 0.362000\n",
      "iteration 1158 / 1500: loss 2.046360, accuracy 0.340000\n",
      "iteration 1159 / 1500: loss 2.091846, accuracy 0.324000\n",
      "iteration 1160 / 1500: loss 2.062236, accuracy 0.337000\n",
      "iteration 1161 / 1500: loss 2.087771, accuracy 0.326000\n",
      "iteration 1162 / 1500: loss 2.024637, accuracy 0.370000\n",
      "iteration 1163 / 1500: loss 2.073285, accuracy 0.341000\n",
      "iteration 1164 / 1500: loss 2.044580, accuracy 0.355000\n",
      "iteration 1165 / 1500: loss 2.059115, accuracy 0.329000\n",
      "iteration 1166 / 1500: loss 2.049792, accuracy 0.347000\n",
      "iteration 1167 / 1500: loss 2.064311, accuracy 0.328000\n",
      "iteration 1168 / 1500: loss 2.063043, accuracy 0.318000\n",
      "iteration 1169 / 1500: loss 2.065409, accuracy 0.330000\n",
      "iteration 1170 / 1500: loss 2.080718, accuracy 0.328000\n",
      "iteration 1171 / 1500: loss 2.033041, accuracy 0.354000\n",
      "iteration 1172 / 1500: loss 2.062148, accuracy 0.336000\n",
      "iteration 1173 / 1500: loss 2.069900, accuracy 0.325000\n",
      "iteration 1174 / 1500: loss 2.070944, accuracy 0.334000\n",
      "iteration 1175 / 1500: loss 2.044375, accuracy 0.353000\n",
      "iteration 1176 / 1500: loss 2.030682, accuracy 0.362000\n",
      "iteration 1177 / 1500: loss 2.041103, accuracy 0.353000\n",
      "iteration 1178 / 1500: loss 2.046261, accuracy 0.344000\n",
      "iteration 1179 / 1500: loss 2.043971, accuracy 0.368000\n",
      "iteration 1180 / 1500: loss 2.016815, accuracy 0.368000\n",
      "iteration 1181 / 1500: loss 2.062500, accuracy 0.352000\n",
      "iteration 1182 / 1500: loss 2.088172, accuracy 0.298000\n",
      "iteration 1183 / 1500: loss 2.066190, accuracy 0.339000\n",
      "iteration 1184 / 1500: loss 2.052203, accuracy 0.361000\n",
      "iteration 1185 / 1500: loss 2.043797, accuracy 0.355000\n",
      "iteration 1186 / 1500: loss 2.046338, accuracy 0.335000\n",
      "iteration 1187 / 1500: loss 2.045200, accuracy 0.358000\n",
      "iteration 1188 / 1500: loss 2.041440, accuracy 0.350000\n",
      "iteration 1189 / 1500: loss 2.055357, accuracy 0.352000\n",
      "iteration 1190 / 1500: loss 2.049277, accuracy 0.363000\n",
      "iteration 1191 / 1500: loss 2.052492, accuracy 0.361000\n",
      "iteration 1192 / 1500: loss 2.063279, accuracy 0.337000\n",
      "iteration 1193 / 1500: loss 2.065250, accuracy 0.328000\n",
      "iteration 1194 / 1500: loss 2.033238, accuracy 0.360000\n",
      "iteration 1195 / 1500: loss 2.060636, accuracy 0.351000\n",
      "iteration 1196 / 1500: loss 2.036789, accuracy 0.356000\n",
      "iteration 1197 / 1500: loss 2.037017, accuracy 0.351000\n",
      "iteration 1198 / 1500: loss 2.068763, accuracy 0.338000\n",
      "iteration 1199 / 1500: loss 2.092810, accuracy 0.301000\n",
      "iteration 1200 / 1500: loss 2.056000, accuracy 0.344000\n",
      "iteration 1201 / 1500: loss 2.061855, accuracy 0.329000\n",
      "iteration 1202 / 1500: loss 2.048574, accuracy 0.349000\n",
      "iteration 1203 / 1500: loss 2.036195, accuracy 0.342000\n",
      "iteration 1204 / 1500: loss 2.009723, accuracy 0.382000\n",
      "iteration 1205 / 1500: loss 2.059073, accuracy 0.324000\n",
      "iteration 1206 / 1500: loss 2.032051, accuracy 0.339000\n",
      "iteration 1207 / 1500: loss 2.066483, accuracy 0.349000\n",
      "iteration 1208 / 1500: loss 2.036445, accuracy 0.357000\n",
      "iteration 1209 / 1500: loss 2.009749, accuracy 0.367000\n",
      "iteration 1210 / 1500: loss 2.076837, accuracy 0.313000\n",
      "iteration 1211 / 1500: loss 2.065752, accuracy 0.349000\n",
      "iteration 1212 / 1500: loss 2.030934, accuracy 0.356000\n",
      "iteration 1213 / 1500: loss 2.021117, accuracy 0.359000\n",
      "iteration 1214 / 1500: loss 2.066823, accuracy 0.335000\n",
      "iteration 1215 / 1500: loss 2.059054, accuracy 0.363000\n",
      "iteration 1216 / 1500: loss 2.077905, accuracy 0.333000\n",
      "iteration 1217 / 1500: loss 2.081663, accuracy 0.332000\n",
      "iteration 1218 / 1500: loss 2.063404, accuracy 0.316000\n",
      "iteration 1219 / 1500: loss 2.049234, accuracy 0.324000\n",
      "iteration 1220 / 1500: loss 2.073611, accuracy 0.321000\n",
      "iteration 1221 / 1500: loss 2.044243, accuracy 0.340000\n",
      "iteration 1222 / 1500: loss 2.078285, accuracy 0.327000\n",
      "iteration 1223 / 1500: loss 2.079719, accuracy 0.324000\n",
      "iteration 1224 / 1500: loss 2.048151, accuracy 0.362000\n",
      "iteration 1225 / 1500: loss 2.068104, accuracy 0.338000\n",
      "iteration 1226 / 1500: loss 2.075492, accuracy 0.311000\n",
      "iteration 1227 / 1500: loss 2.032233, accuracy 0.351000\n",
      "iteration 1228 / 1500: loss 2.045463, accuracy 0.353000\n",
      "iteration 1229 / 1500: loss 2.050669, accuracy 0.340000\n",
      "iteration 1230 / 1500: loss 2.059602, accuracy 0.336000\n",
      "iteration 1231 / 1500: loss 2.042368, accuracy 0.348000\n",
      "iteration 1232 / 1500: loss 2.046016, accuracy 0.351000\n",
      "iteration 1233 / 1500: loss 2.028832, accuracy 0.379000\n",
      "iteration 1234 / 1500: loss 2.059250, accuracy 0.347000\n",
      "iteration 1235 / 1500: loss 2.067939, accuracy 0.357000\n",
      "iteration 1236 / 1500: loss 2.023902, accuracy 0.361000\n",
      "iteration 1237 / 1500: loss 2.080723, accuracy 0.329000\n",
      "iteration 1238 / 1500: loss 2.069493, accuracy 0.347000\n",
      "iteration 1239 / 1500: loss 2.045225, accuracy 0.354000\n",
      "iteration 1240 / 1500: loss 2.044795, accuracy 0.326000\n",
      "iteration 1241 / 1500: loss 2.064250, accuracy 0.335000\n",
      "iteration 1242 / 1500: loss 2.044486, accuracy 0.361000\n",
      "iteration 1243 / 1500: loss 2.061914, accuracy 0.332000\n",
      "iteration 1244 / 1500: loss 2.052724, accuracy 0.344000\n",
      "iteration 1245 / 1500: loss 2.048604, accuracy 0.315000\n",
      "iteration 1246 / 1500: loss 2.033156, accuracy 0.345000\n",
      "iteration 1247 / 1500: loss 2.055061, accuracy 0.343000\n",
      "iteration 1248 / 1500: loss 2.058371, accuracy 0.344000\n",
      "iteration 1249 / 1500: loss 2.086583, accuracy 0.339000\n",
      "iteration 1250 / 1500: loss 2.045784, accuracy 0.358000\n",
      "iteration 1251 / 1500: loss 2.036655, accuracy 0.352000\n",
      "iteration 1252 / 1500: loss 2.027629, accuracy 0.346000\n",
      "iteration 1253 / 1500: loss 2.044594, accuracy 0.329000\n",
      "iteration 1254 / 1500: loss 2.059977, accuracy 0.343000\n",
      "iteration 1255 / 1500: loss 2.059377, accuracy 0.357000\n",
      "iteration 1256 / 1500: loss 2.062046, accuracy 0.346000\n",
      "iteration 1257 / 1500: loss 2.020834, accuracy 0.356000\n",
      "iteration 1258 / 1500: loss 2.056988, accuracy 0.340000\n",
      "iteration 1259 / 1500: loss 2.026078, accuracy 0.363000\n",
      "iteration 1260 / 1500: loss 2.068234, accuracy 0.304000\n",
      "iteration 1261 / 1500: loss 2.025995, accuracy 0.350000\n",
      "iteration 1262 / 1500: loss 2.041205, accuracy 0.347000\n",
      "iteration 1263 / 1500: loss 2.012136, accuracy 0.380000\n",
      "iteration 1264 / 1500: loss 2.041414, accuracy 0.331000\n",
      "iteration 1265 / 1500: loss 2.049535, accuracy 0.347000\n",
      "iteration 1266 / 1500: loss 2.037716, accuracy 0.341000\n",
      "iteration 1267 / 1500: loss 2.022655, accuracy 0.358000\n",
      "iteration 1268 / 1500: loss 2.076939, accuracy 0.347000\n",
      "iteration 1269 / 1500: loss 2.029543, accuracy 0.365000\n",
      "iteration 1270 / 1500: loss 2.067247, accuracy 0.337000\n",
      "iteration 1271 / 1500: loss 2.060467, accuracy 0.331000\n",
      "iteration 1272 / 1500: loss 2.062942, accuracy 0.320000\n",
      "iteration 1273 / 1500: loss 2.041298, accuracy 0.350000\n",
      "iteration 1274 / 1500: loss 2.062757, accuracy 0.345000\n",
      "iteration 1275 / 1500: loss 2.047256, accuracy 0.333000\n",
      "iteration 1276 / 1500: loss 2.043590, accuracy 0.365000\n",
      "iteration 1277 / 1500: loss 2.049491, accuracy 0.346000\n",
      "iteration 1278 / 1500: loss 2.028890, accuracy 0.354000\n",
      "iteration 1279 / 1500: loss 2.074309, accuracy 0.330000\n",
      "iteration 1280 / 1500: loss 2.057807, accuracy 0.342000\n",
      "iteration 1281 / 1500: loss 2.076004, accuracy 0.320000\n",
      "iteration 1282 / 1500: loss 2.082202, accuracy 0.341000\n",
      "iteration 1283 / 1500: loss 2.047687, accuracy 0.338000\n",
      "iteration 1284 / 1500: loss 2.057608, accuracy 0.335000\n",
      "iteration 1285 / 1500: loss 2.040621, accuracy 0.341000\n",
      "iteration 1286 / 1500: loss 2.028890, accuracy 0.343000\n",
      "iteration 1287 / 1500: loss 2.046725, accuracy 0.335000\n",
      "iteration 1288 / 1500: loss 2.063360, accuracy 0.329000\n",
      "iteration 1289 / 1500: loss 2.051945, accuracy 0.358000\n",
      "iteration 1290 / 1500: loss 2.034059, accuracy 0.349000\n",
      "iteration 1291 / 1500: loss 2.036317, accuracy 0.354000\n",
      "iteration 1292 / 1500: loss 2.024835, accuracy 0.359000\n",
      "iteration 1293 / 1500: loss 2.041116, accuracy 0.325000\n",
      "iteration 1294 / 1500: loss 2.067010, accuracy 0.337000\n",
      "iteration 1295 / 1500: loss 2.037663, accuracy 0.353000\n",
      "iteration 1296 / 1500: loss 2.059437, accuracy 0.338000\n",
      "iteration 1297 / 1500: loss 2.049300, accuracy 0.351000\n",
      "iteration 1298 / 1500: loss 2.066213, accuracy 0.329000\n",
      "iteration 1299 / 1500: loss 2.070202, accuracy 0.331000\n",
      "iteration 1300 / 1500: loss 2.072542, accuracy 0.357000\n",
      "iteration 1301 / 1500: loss 2.027899, accuracy 0.355000\n",
      "iteration 1302 / 1500: loss 2.036310, accuracy 0.348000\n",
      "iteration 1303 / 1500: loss 2.072301, accuracy 0.311000\n",
      "iteration 1304 / 1500: loss 2.078448, accuracy 0.318000\n",
      "iteration 1305 / 1500: loss 2.053669, accuracy 0.331000\n",
      "iteration 1306 / 1500: loss 2.044405, accuracy 0.337000\n",
      "iteration 1307 / 1500: loss 2.044569, accuracy 0.346000\n",
      "iteration 1308 / 1500: loss 2.065256, accuracy 0.328000\n",
      "iteration 1309 / 1500: loss 2.058765, accuracy 0.351000\n",
      "iteration 1310 / 1500: loss 2.065500, accuracy 0.331000\n",
      "iteration 1311 / 1500: loss 2.050902, accuracy 0.329000\n",
      "iteration 1312 / 1500: loss 2.051705, accuracy 0.342000\n",
      "iteration 1313 / 1500: loss 2.077009, accuracy 0.336000\n",
      "iteration 1314 / 1500: loss 2.070714, accuracy 0.353000\n",
      "iteration 1315 / 1500: loss 2.060730, accuracy 0.340000\n",
      "iteration 1316 / 1500: loss 2.042303, accuracy 0.336000\n",
      "iteration 1317 / 1500: loss 2.071338, accuracy 0.325000\n",
      "iteration 1318 / 1500: loss 2.073493, accuracy 0.337000\n",
      "iteration 1319 / 1500: loss 2.067935, accuracy 0.326000\n",
      "iteration 1320 / 1500: loss 2.059390, accuracy 0.344000\n",
      "iteration 1321 / 1500: loss 2.059486, accuracy 0.342000\n",
      "iteration 1322 / 1500: loss 2.031280, accuracy 0.345000\n",
      "iteration 1323 / 1500: loss 2.040990, accuracy 0.340000\n",
      "iteration 1324 / 1500: loss 2.037735, accuracy 0.355000\n",
      "iteration 1325 / 1500: loss 2.058546, accuracy 0.352000\n",
      "iteration 1326 / 1500: loss 2.064323, accuracy 0.340000\n",
      "iteration 1327 / 1500: loss 2.036171, accuracy 0.344000\n",
      "iteration 1328 / 1500: loss 2.071171, accuracy 0.323000\n",
      "iteration 1329 / 1500: loss 2.053926, accuracy 0.324000\n",
      "iteration 1330 / 1500: loss 2.046980, accuracy 0.318000\n",
      "iteration 1331 / 1500: loss 2.024107, accuracy 0.355000\n",
      "iteration 1332 / 1500: loss 2.070420, accuracy 0.317000\n",
      "iteration 1333 / 1500: loss 2.058359, accuracy 0.349000\n",
      "iteration 1334 / 1500: loss 2.068690, accuracy 0.339000\n",
      "iteration 1335 / 1500: loss 2.023761, accuracy 0.368000\n",
      "iteration 1336 / 1500: loss 2.051128, accuracy 0.334000\n",
      "iteration 1337 / 1500: loss 2.064767, accuracy 0.346000\n",
      "iteration 1338 / 1500: loss 2.051653, accuracy 0.334000\n",
      "iteration 1339 / 1500: loss 2.054413, accuracy 0.317000\n",
      "iteration 1340 / 1500: loss 2.065261, accuracy 0.351000\n",
      "iteration 1341 / 1500: loss 2.038038, accuracy 0.332000\n",
      "iteration 1342 / 1500: loss 2.052991, accuracy 0.321000\n",
      "iteration 1343 / 1500: loss 2.043443, accuracy 0.358000\n",
      "iteration 1344 / 1500: loss 2.037336, accuracy 0.367000\n",
      "iteration 1345 / 1500: loss 2.033285, accuracy 0.355000\n",
      "iteration 1346 / 1500: loss 2.039453, accuracy 0.355000\n",
      "iteration 1347 / 1500: loss 2.053668, accuracy 0.329000\n",
      "iteration 1348 / 1500: loss 2.069762, accuracy 0.330000\n",
      "iteration 1349 / 1500: loss 2.047997, accuracy 0.359000\n",
      "iteration 1350 / 1500: loss 2.063506, accuracy 0.336000\n",
      "iteration 1351 / 1500: loss 2.051617, accuracy 0.341000\n",
      "iteration 1352 / 1500: loss 2.057990, accuracy 0.346000\n",
      "iteration 1353 / 1500: loss 2.048691, accuracy 0.339000\n",
      "iteration 1354 / 1500: loss 2.051948, accuracy 0.337000\n",
      "iteration 1355 / 1500: loss 2.068524, accuracy 0.341000\n",
      "iteration 1356 / 1500: loss 2.071272, accuracy 0.330000\n",
      "iteration 1357 / 1500: loss 2.048376, accuracy 0.366000\n",
      "iteration 1358 / 1500: loss 2.063766, accuracy 0.353000\n",
      "iteration 1359 / 1500: loss 2.044636, accuracy 0.343000\n",
      "iteration 1360 / 1500: loss 2.049597, accuracy 0.352000\n",
      "iteration 1361 / 1500: loss 2.066093, accuracy 0.319000\n",
      "iteration 1362 / 1500: loss 2.041578, accuracy 0.329000\n",
      "iteration 1363 / 1500: loss 2.056706, accuracy 0.352000\n",
      "iteration 1364 / 1500: loss 2.085751, accuracy 0.310000\n",
      "iteration 1365 / 1500: loss 2.068687, accuracy 0.332000\n",
      "iteration 1366 / 1500: loss 2.049255, accuracy 0.349000\n",
      "iteration 1367 / 1500: loss 2.067484, accuracy 0.351000\n",
      "iteration 1368 / 1500: loss 2.053716, accuracy 0.333000\n",
      "iteration 1369 / 1500: loss 2.024866, accuracy 0.362000\n",
      "iteration 1370 / 1500: loss 2.063885, accuracy 0.336000\n",
      "iteration 1371 / 1500: loss 2.067183, accuracy 0.326000\n",
      "iteration 1372 / 1500: loss 2.047281, accuracy 0.349000\n",
      "iteration 1373 / 1500: loss 2.069705, accuracy 0.327000\n",
      "iteration 1374 / 1500: loss 2.033837, accuracy 0.356000\n",
      "iteration 1375 / 1500: loss 2.028580, accuracy 0.355000\n",
      "iteration 1376 / 1500: loss 2.055512, accuracy 0.328000\n",
      "iteration 1377 / 1500: loss 2.065747, accuracy 0.323000\n",
      "iteration 1378 / 1500: loss 2.059768, accuracy 0.327000\n",
      "iteration 1379 / 1500: loss 2.073746, accuracy 0.340000\n",
      "iteration 1380 / 1500: loss 2.031155, accuracy 0.385000\n",
      "iteration 1381 / 1500: loss 2.050295, accuracy 0.333000\n",
      "iteration 1382 / 1500: loss 2.070852, accuracy 0.324000\n",
      "iteration 1383 / 1500: loss 2.033970, accuracy 0.357000\n",
      "iteration 1384 / 1500: loss 2.051666, accuracy 0.330000\n",
      "iteration 1385 / 1500: loss 2.054255, accuracy 0.348000\n",
      "iteration 1386 / 1500: loss 2.062453, accuracy 0.315000\n",
      "iteration 1387 / 1500: loss 2.045460, accuracy 0.349000\n",
      "iteration 1388 / 1500: loss 2.034022, accuracy 0.353000\n",
      "iteration 1389 / 1500: loss 2.069636, accuracy 0.314000\n",
      "iteration 1390 / 1500: loss 2.056213, accuracy 0.334000\n",
      "iteration 1391 / 1500: loss 2.020362, accuracy 0.345000\n",
      "iteration 1392 / 1500: loss 2.052774, accuracy 0.319000\n",
      "iteration 1393 / 1500: loss 2.068793, accuracy 0.337000\n",
      "iteration 1394 / 1500: loss 2.069195, accuracy 0.344000\n",
      "iteration 1395 / 1500: loss 2.043612, accuracy 0.359000\n",
      "iteration 1396 / 1500: loss 2.053327, accuracy 0.331000\n",
      "iteration 1397 / 1500: loss 2.061816, accuracy 0.301000\n",
      "iteration 1398 / 1500: loss 2.047401, accuracy 0.351000\n",
      "iteration 1399 / 1500: loss 2.056127, accuracy 0.338000\n",
      "iteration 1400 / 1500: loss 2.060188, accuracy 0.359000\n",
      "iteration 1401 / 1500: loss 2.046083, accuracy 0.350000\n",
      "iteration 1402 / 1500: loss 2.059789, accuracy 0.342000\n",
      "iteration 1403 / 1500: loss 2.047118, accuracy 0.327000\n",
      "iteration 1404 / 1500: loss 2.051327, accuracy 0.344000\n",
      "iteration 1405 / 1500: loss 2.041064, accuracy 0.334000\n",
      "iteration 1406 / 1500: loss 2.056567, accuracy 0.338000\n",
      "iteration 1407 / 1500: loss 2.062021, accuracy 0.330000\n",
      "iteration 1408 / 1500: loss 2.058261, accuracy 0.345000\n",
      "iteration 1409 / 1500: loss 2.044788, accuracy 0.348000\n",
      "iteration 1410 / 1500: loss 2.082978, accuracy 0.320000\n",
      "iteration 1411 / 1500: loss 2.066122, accuracy 0.344000\n",
      "iteration 1412 / 1500: loss 2.067426, accuracy 0.327000\n",
      "iteration 1413 / 1500: loss 2.079349, accuracy 0.327000\n",
      "iteration 1414 / 1500: loss 2.057178, accuracy 0.339000\n",
      "iteration 1415 / 1500: loss 2.051722, accuracy 0.360000\n",
      "iteration 1416 / 1500: loss 2.038944, accuracy 0.354000\n",
      "iteration 1417 / 1500: loss 2.037353, accuracy 0.336000\n",
      "iteration 1418 / 1500: loss 2.074968, accuracy 0.321000\n",
      "iteration 1419 / 1500: loss 2.050856, accuracy 0.346000\n",
      "iteration 1420 / 1500: loss 2.049641, accuracy 0.345000\n",
      "iteration 1421 / 1500: loss 2.037383, accuracy 0.366000\n",
      "iteration 1422 / 1500: loss 2.039041, accuracy 0.338000\n",
      "iteration 1423 / 1500: loss 2.050899, accuracy 0.364000\n",
      "iteration 1424 / 1500: loss 2.038732, accuracy 0.349000\n",
      "iteration 1425 / 1500: loss 2.061963, accuracy 0.322000\n",
      "iteration 1426 / 1500: loss 2.049831, accuracy 0.314000\n",
      "iteration 1427 / 1500: loss 2.040619, accuracy 0.337000\n",
      "iteration 1428 / 1500: loss 2.054937, accuracy 0.333000\n",
      "iteration 1429 / 1500: loss 2.079203, accuracy 0.324000\n",
      "iteration 1430 / 1500: loss 2.044557, accuracy 0.356000\n",
      "iteration 1431 / 1500: loss 2.031382, accuracy 0.337000\n",
      "iteration 1432 / 1500: loss 2.062295, accuracy 0.335000\n",
      "iteration 1433 / 1500: loss 2.052860, accuracy 0.344000\n",
      "iteration 1434 / 1500: loss 2.057175, accuracy 0.348000\n",
      "iteration 1435 / 1500: loss 2.037151, accuracy 0.338000\n",
      "iteration 1436 / 1500: loss 2.064691, accuracy 0.323000\n",
      "iteration 1437 / 1500: loss 2.035775, accuracy 0.349000\n",
      "iteration 1438 / 1500: loss 2.072063, accuracy 0.337000\n",
      "iteration 1439 / 1500: loss 2.067377, accuracy 0.324000\n",
      "iteration 1440 / 1500: loss 2.073941, accuracy 0.329000\n",
      "iteration 1441 / 1500: loss 2.044777, accuracy 0.335000\n",
      "iteration 1442 / 1500: loss 2.054026, accuracy 0.323000\n",
      "iteration 1443 / 1500: loss 2.065748, accuracy 0.331000\n",
      "iteration 1444 / 1500: loss 2.038029, accuracy 0.356000\n",
      "iteration 1445 / 1500: loss 2.035638, accuracy 0.353000\n",
      "iteration 1446 / 1500: loss 2.028973, accuracy 0.349000\n",
      "iteration 1447 / 1500: loss 2.063148, accuracy 0.327000\n",
      "iteration 1448 / 1500: loss 2.062171, accuracy 0.350000\n",
      "iteration 1449 / 1500: loss 2.021756, accuracy 0.378000\n",
      "iteration 1450 / 1500: loss 2.060673, accuracy 0.341000\n",
      "iteration 1451 / 1500: loss 2.061446, accuracy 0.333000\n",
      "iteration 1452 / 1500: loss 2.038276, accuracy 0.340000\n",
      "iteration 1453 / 1500: loss 2.039692, accuracy 0.337000\n",
      "iteration 1454 / 1500: loss 2.048534, accuracy 0.351000\n",
      "iteration 1455 / 1500: loss 2.053027, accuracy 0.340000\n",
      "iteration 1456 / 1500: loss 2.032136, accuracy 0.349000\n",
      "iteration 1457 / 1500: loss 2.039996, accuracy 0.368000\n",
      "iteration 1458 / 1500: loss 2.044940, accuracy 0.336000\n",
      "iteration 1459 / 1500: loss 2.048485, accuracy 0.338000\n",
      "iteration 1460 / 1500: loss 2.040173, accuracy 0.359000\n",
      "iteration 1461 / 1500: loss 2.038009, accuracy 0.350000\n",
      "iteration 1462 / 1500: loss 2.064230, accuracy 0.331000\n",
      "iteration 1463 / 1500: loss 2.045626, accuracy 0.351000\n",
      "iteration 1464 / 1500: loss 2.026816, accuracy 0.352000\n",
      "iteration 1465 / 1500: loss 2.029756, accuracy 0.350000\n",
      "iteration 1466 / 1500: loss 2.056111, accuracy 0.337000\n",
      "iteration 1467 / 1500: loss 2.040214, accuracy 0.336000\n",
      "iteration 1468 / 1500: loss 2.045293, accuracy 0.340000\n",
      "iteration 1469 / 1500: loss 2.061568, accuracy 0.324000\n",
      "iteration 1470 / 1500: loss 2.035797, accuracy 0.346000\n",
      "iteration 1471 / 1500: loss 2.075858, accuracy 0.312000\n",
      "iteration 1472 / 1500: loss 2.082105, accuracy 0.310000\n",
      "iteration 1473 / 1500: loss 2.042376, accuracy 0.333000\n",
      "iteration 1474 / 1500: loss 2.080172, accuracy 0.334000\n",
      "iteration 1475 / 1500: loss 2.070585, accuracy 0.336000\n",
      "iteration 1476 / 1500: loss 2.053683, accuracy 0.337000\n",
      "iteration 1477 / 1500: loss 2.046497, accuracy 0.349000\n",
      "iteration 1478 / 1500: loss 2.061428, accuracy 0.334000\n",
      "iteration 1479 / 1500: loss 2.052940, accuracy 0.334000\n",
      "iteration 1480 / 1500: loss 2.070061, accuracy 0.349000\n",
      "iteration 1481 / 1500: loss 2.030728, accuracy 0.358000\n",
      "iteration 1482 / 1500: loss 2.021244, accuracy 0.366000\n",
      "iteration 1483 / 1500: loss 2.070831, accuracy 0.328000\n",
      "iteration 1484 / 1500: loss 2.059040, accuracy 0.336000\n",
      "iteration 1485 / 1500: loss 2.084001, accuracy 0.309000\n",
      "iteration 1486 / 1500: loss 2.046145, accuracy 0.335000\n",
      "iteration 1487 / 1500: loss 2.079975, accuracy 0.334000\n",
      "iteration 1488 / 1500: loss 2.078018, accuracy 0.325000\n",
      "iteration 1489 / 1500: loss 2.066170, accuracy 0.341000\n",
      "iteration 1490 / 1500: loss 2.060276, accuracy 0.352000\n",
      "iteration 1491 / 1500: loss 2.063759, accuracy 0.331000\n",
      "iteration 1492 / 1500: loss 2.068561, accuracy 0.356000\n",
      "iteration 1493 / 1500: loss 2.074205, accuracy 0.334000\n",
      "iteration 1494 / 1500: loss 2.044013, accuracy 0.355000\n",
      "iteration 1495 / 1500: loss 2.079720, accuracy 0.354000\n",
      "iteration 1496 / 1500: loss 2.087691, accuracy 0.319000\n",
      "iteration 1497 / 1500: loss 2.054950, accuracy 0.351000\n",
      "iteration 1498 / 1500: loss 2.050289, accuracy 0.353000\n",
      "iteration 1499 / 1500: loss 2.035489, accuracy 0.369000\n",
      "iteration 0 / 1500: loss 587.921674, accuracy 0.098000\n",
      "iteration 1 / 1500: loss 582.306953, accuracy 0.087000\n",
      "iteration 2 / 1500: loss 576.732393, accuracy 0.109000\n",
      "iteration 3 / 1500: loss 571.268320, accuracy 0.109000\n",
      "iteration 4 / 1500: loss 565.737676, accuracy 0.119000\n",
      "iteration 5 / 1500: loss 560.631214, accuracy 0.100000\n",
      "iteration 6 / 1500: loss 555.407116, accuracy 0.109000\n",
      "iteration 7 / 1500: loss 550.278513, accuracy 0.092000\n",
      "iteration 8 / 1500: loss 544.931930, accuracy 0.110000\n",
      "iteration 9 / 1500: loss 539.936801, accuracy 0.097000\n",
      "iteration 10 / 1500: loss 534.839616, accuracy 0.110000\n",
      "iteration 11 / 1500: loss 529.861559, accuracy 0.100000\n",
      "iteration 12 / 1500: loss 524.807750, accuracy 0.099000\n",
      "iteration 13 / 1500: loss 520.026008, accuracy 0.092000\n",
      "iteration 14 / 1500: loss 515.076137, accuracy 0.111000\n",
      "iteration 15 / 1500: loss 510.201513, accuracy 0.099000\n",
      "iteration 16 / 1500: loss 505.647557, accuracy 0.084000\n",
      "iteration 17 / 1500: loss 500.851553, accuracy 0.104000\n",
      "iteration 18 / 1500: loss 495.953154, accuracy 0.111000\n",
      "iteration 19 / 1500: loss 491.418887, accuracy 0.092000\n",
      "iteration 20 / 1500: loss 486.861017, accuracy 0.108000\n",
      "iteration 21 / 1500: loss 482.111897, accuracy 0.103000\n",
      "iteration 22 / 1500: loss 477.650493, accuracy 0.117000\n",
      "iteration 23 / 1500: loss 473.160162, accuracy 0.104000\n",
      "iteration 24 / 1500: loss 468.641073, accuracy 0.110000\n",
      "iteration 25 / 1500: loss 464.241224, accuracy 0.150000\n",
      "iteration 26 / 1500: loss 459.986440, accuracy 0.102000\n",
      "iteration 27 / 1500: loss 455.771672, accuracy 0.107000\n",
      "iteration 28 / 1500: loss 451.519180, accuracy 0.104000\n",
      "iteration 29 / 1500: loss 447.315874, accuracy 0.096000\n",
      "iteration 30 / 1500: loss 442.994603, accuracy 0.122000\n",
      "iteration 31 / 1500: loss 438.998731, accuracy 0.102000\n",
      "iteration 32 / 1500: loss 434.883885, accuracy 0.103000\n",
      "iteration 33 / 1500: loss 430.773470, accuracy 0.111000\n",
      "iteration 34 / 1500: loss 426.851339, accuracy 0.089000\n",
      "iteration 35 / 1500: loss 422.794934, accuracy 0.099000\n",
      "iteration 36 / 1500: loss 418.798685, accuracy 0.115000\n",
      "iteration 37 / 1500: loss 414.844038, accuracy 0.124000\n",
      "iteration 38 / 1500: loss 410.863561, accuracy 0.107000\n",
      "iteration 39 / 1500: loss 407.017736, accuracy 0.120000\n",
      "iteration 40 / 1500: loss 403.363384, accuracy 0.085000\n",
      "iteration 41 / 1500: loss 399.507280, accuracy 0.128000\n",
      "iteration 42 / 1500: loss 395.718105, accuracy 0.125000\n",
      "iteration 43 / 1500: loss 392.128113, accuracy 0.122000\n",
      "iteration 44 / 1500: loss 388.457058, accuracy 0.127000\n",
      "iteration 45 / 1500: loss 384.695949, accuracy 0.131000\n",
      "iteration 46 / 1500: loss 381.117301, accuracy 0.145000\n",
      "iteration 47 / 1500: loss 377.576459, accuracy 0.120000\n",
      "iteration 48 / 1500: loss 374.135886, accuracy 0.125000\n",
      "iteration 49 / 1500: loss 370.593070, accuracy 0.109000\n",
      "iteration 50 / 1500: loss 367.138512, accuracy 0.124000\n",
      "iteration 51 / 1500: loss 363.681814, accuracy 0.133000\n",
      "iteration 52 / 1500: loss 360.390328, accuracy 0.103000\n",
      "iteration 53 / 1500: loss 356.953786, accuracy 0.122000\n",
      "iteration 54 / 1500: loss 353.563505, accuracy 0.119000\n",
      "iteration 55 / 1500: loss 350.350680, accuracy 0.127000\n",
      "iteration 56 / 1500: loss 346.941522, accuracy 0.124000\n",
      "iteration 57 / 1500: loss 343.621863, accuracy 0.145000\n",
      "iteration 58 / 1500: loss 340.631757, accuracy 0.114000\n",
      "iteration 59 / 1500: loss 337.536442, accuracy 0.127000\n",
      "iteration 60 / 1500: loss 334.249540, accuracy 0.117000\n",
      "iteration 61 / 1500: loss 331.127620, accuracy 0.136000\n",
      "iteration 62 / 1500: loss 327.981979, accuracy 0.158000\n",
      "iteration 63 / 1500: loss 325.094564, accuracy 0.117000\n",
      "iteration 64 / 1500: loss 321.958803, accuracy 0.129000\n",
      "iteration 65 / 1500: loss 318.978332, accuracy 0.132000\n",
      "iteration 66 / 1500: loss 315.913940, accuracy 0.139000\n",
      "iteration 67 / 1500: loss 313.013346, accuracy 0.126000\n",
      "iteration 68 / 1500: loss 310.196866, accuracy 0.104000\n",
      "iteration 69 / 1500: loss 307.311587, accuracy 0.123000\n",
      "iteration 70 / 1500: loss 304.296944, accuracy 0.142000\n",
      "iteration 71 / 1500: loss 301.464667, accuracy 0.123000\n",
      "iteration 72 / 1500: loss 298.664941, accuracy 0.125000\n",
      "iteration 73 / 1500: loss 295.968752, accuracy 0.127000\n",
      "iteration 74 / 1500: loss 293.223763, accuracy 0.122000\n",
      "iteration 75 / 1500: loss 290.355425, accuracy 0.147000\n",
      "iteration 76 / 1500: loss 287.697251, accuracy 0.139000\n",
      "iteration 77 / 1500: loss 284.942442, accuracy 0.143000\n",
      "iteration 78 / 1500: loss 282.331033, accuracy 0.149000\n",
      "iteration 79 / 1500: loss 279.771622, accuracy 0.136000\n",
      "iteration 80 / 1500: loss 277.067649, accuracy 0.142000\n",
      "iteration 81 / 1500: loss 274.463894, accuracy 0.145000\n",
      "iteration 82 / 1500: loss 271.933958, accuracy 0.129000\n",
      "iteration 83 / 1500: loss 269.398328, accuracy 0.130000\n",
      "iteration 84 / 1500: loss 266.911715, accuracy 0.143000\n",
      "iteration 85 / 1500: loss 264.394605, accuracy 0.148000\n",
      "iteration 86 / 1500: loss 261.994044, accuracy 0.143000\n",
      "iteration 87 / 1500: loss 259.530004, accuracy 0.133000\n",
      "iteration 88 / 1500: loss 257.057593, accuracy 0.148000\n",
      "iteration 89 / 1500: loss 254.728705, accuracy 0.152000\n",
      "iteration 90 / 1500: loss 252.334933, accuracy 0.121000\n",
      "iteration 91 / 1500: loss 249.992611, accuracy 0.136000\n",
      "iteration 92 / 1500: loss 247.741058, accuracy 0.116000\n",
      "iteration 93 / 1500: loss 245.353660, accuracy 0.155000\n",
      "iteration 94 / 1500: loss 243.150796, accuracy 0.140000\n",
      "iteration 95 / 1500: loss 240.746019, accuracy 0.148000\n",
      "iteration 96 / 1500: loss 238.519567, accuracy 0.153000\n",
      "iteration 97 / 1500: loss 236.373125, accuracy 0.134000\n",
      "iteration 98 / 1500: loss 234.052185, accuracy 0.156000\n",
      "iteration 99 / 1500: loss 231.900460, accuracy 0.162000\n",
      "iteration 100 / 1500: loss 229.785608, accuracy 0.147000\n",
      "iteration 101 / 1500: loss 227.681380, accuracy 0.158000\n",
      "iteration 102 / 1500: loss 225.551313, accuracy 0.143000\n",
      "iteration 103 / 1500: loss 223.470841, accuracy 0.130000\n",
      "iteration 104 / 1500: loss 221.363455, accuracy 0.158000\n",
      "iteration 105 / 1500: loss 219.263073, accuracy 0.165000\n",
      "iteration 106 / 1500: loss 217.299126, accuracy 0.149000\n",
      "iteration 107 / 1500: loss 215.224702, accuracy 0.136000\n",
      "iteration 108 / 1500: loss 213.263633, accuracy 0.152000\n",
      "iteration 109 / 1500: loss 211.304734, accuracy 0.155000\n",
      "iteration 110 / 1500: loss 209.323241, accuracy 0.150000\n",
      "iteration 111 / 1500: loss 207.336441, accuracy 0.159000\n",
      "iteration 112 / 1500: loss 205.385502, accuracy 0.163000\n",
      "iteration 113 / 1500: loss 203.472872, accuracy 0.154000\n",
      "iteration 114 / 1500: loss 201.668690, accuracy 0.157000\n",
      "iteration 115 / 1500: loss 199.657693, accuracy 0.157000\n",
      "iteration 116 / 1500: loss 197.834071, accuracy 0.175000\n",
      "iteration 117 / 1500: loss 196.040724, accuracy 0.162000\n",
      "iteration 118 / 1500: loss 194.256840, accuracy 0.133000\n",
      "iteration 119 / 1500: loss 192.403776, accuracy 0.168000\n",
      "iteration 120 / 1500: loss 190.671054, accuracy 0.164000\n",
      "iteration 121 / 1500: loss 188.908255, accuracy 0.153000\n",
      "iteration 122 / 1500: loss 187.155850, accuracy 0.156000\n",
      "iteration 123 / 1500: loss 185.343652, accuracy 0.157000\n",
      "iteration 124 / 1500: loss 183.719065, accuracy 0.143000\n",
      "iteration 125 / 1500: loss 181.890166, accuracy 0.173000\n",
      "iteration 126 / 1500: loss 180.265013, accuracy 0.172000\n",
      "iteration 127 / 1500: loss 178.606019, accuracy 0.151000\n",
      "iteration 128 / 1500: loss 176.959890, accuracy 0.152000\n",
      "iteration 129 / 1500: loss 175.316670, accuracy 0.148000\n",
      "iteration 130 / 1500: loss 173.621669, accuracy 0.174000\n",
      "iteration 131 / 1500: loss 172.074785, accuracy 0.174000\n",
      "iteration 132 / 1500: loss 170.468592, accuracy 0.150000\n",
      "iteration 133 / 1500: loss 168.867740, accuracy 0.185000\n",
      "iteration 134 / 1500: loss 167.402632, accuracy 0.166000\n",
      "iteration 135 / 1500: loss 165.791572, accuracy 0.171000\n",
      "iteration 136 / 1500: loss 164.246759, accuracy 0.150000\n",
      "iteration 137 / 1500: loss 162.757534, accuracy 0.147000\n",
      "iteration 138 / 1500: loss 161.195227, accuracy 0.164000\n",
      "iteration 139 / 1500: loss 159.683487, accuracy 0.161000\n",
      "iteration 140 / 1500: loss 158.295530, accuracy 0.166000\n",
      "iteration 141 / 1500: loss 156.745832, accuracy 0.170000\n",
      "iteration 142 / 1500: loss 155.340108, accuracy 0.167000\n",
      "iteration 143 / 1500: loss 153.869964, accuracy 0.167000\n",
      "iteration 144 / 1500: loss 152.369152, accuracy 0.188000\n",
      "iteration 145 / 1500: loss 151.041486, accuracy 0.173000\n",
      "iteration 146 / 1500: loss 149.649070, accuracy 0.197000\n",
      "iteration 147 / 1500: loss 148.216822, accuracy 0.166000\n",
      "iteration 148 / 1500: loss 146.911843, accuracy 0.168000\n",
      "iteration 149 / 1500: loss 145.524243, accuracy 0.197000\n",
      "iteration 150 / 1500: loss 144.103199, accuracy 0.197000\n",
      "iteration 151 / 1500: loss 142.869354, accuracy 0.159000\n",
      "iteration 152 / 1500: loss 141.532979, accuracy 0.180000\n",
      "iteration 153 / 1500: loss 140.254993, accuracy 0.161000\n",
      "iteration 154 / 1500: loss 138.946418, accuracy 0.156000\n",
      "iteration 155 / 1500: loss 137.647468, accuracy 0.175000\n",
      "iteration 156 / 1500: loss 136.374193, accuracy 0.161000\n",
      "iteration 157 / 1500: loss 135.078309, accuracy 0.163000\n",
      "iteration 158 / 1500: loss 133.851404, accuracy 0.168000\n",
      "iteration 159 / 1500: loss 132.651440, accuracy 0.150000\n",
      "iteration 160 / 1500: loss 131.371228, accuracy 0.184000\n",
      "iteration 161 / 1500: loss 130.219725, accuracy 0.176000\n",
      "iteration 162 / 1500: loss 128.948532, accuracy 0.170000\n",
      "iteration 163 / 1500: loss 127.785807, accuracy 0.187000\n",
      "iteration 164 / 1500: loss 126.602447, accuracy 0.167000\n",
      "iteration 165 / 1500: loss 125.384637, accuracy 0.179000\n",
      "iteration 166 / 1500: loss 124.249554, accuracy 0.193000\n",
      "iteration 167 / 1500: loss 123.142418, accuracy 0.170000\n",
      "iteration 168 / 1500: loss 121.947193, accuracy 0.201000\n",
      "iteration 169 / 1500: loss 120.872320, accuracy 0.176000\n",
      "iteration 170 / 1500: loss 119.676800, accuracy 0.202000\n",
      "iteration 171 / 1500: loss 118.649317, accuracy 0.186000\n",
      "iteration 172 / 1500: loss 117.512809, accuracy 0.190000\n",
      "iteration 173 / 1500: loss 116.414471, accuracy 0.193000\n",
      "iteration 174 / 1500: loss 115.351193, accuracy 0.199000\n",
      "iteration 175 / 1500: loss 114.314651, accuracy 0.193000\n",
      "iteration 176 / 1500: loss 113.268450, accuracy 0.194000\n",
      "iteration 177 / 1500: loss 112.152608, accuracy 0.196000\n",
      "iteration 178 / 1500: loss 111.229372, accuracy 0.194000\n",
      "iteration 179 / 1500: loss 110.142757, accuracy 0.198000\n",
      "iteration 180 / 1500: loss 109.138239, accuracy 0.203000\n",
      "iteration 181 / 1500: loss 108.114937, accuracy 0.219000\n",
      "iteration 182 / 1500: loss 107.145938, accuracy 0.194000\n",
      "iteration 183 / 1500: loss 106.199903, accuracy 0.177000\n",
      "iteration 184 / 1500: loss 105.192224, accuracy 0.196000\n",
      "iteration 185 / 1500: loss 104.229438, accuracy 0.201000\n",
      "iteration 186 / 1500: loss 103.271664, accuracy 0.190000\n",
      "iteration 187 / 1500: loss 102.360503, accuracy 0.195000\n",
      "iteration 188 / 1500: loss 101.384324, accuracy 0.184000\n",
      "iteration 189 / 1500: loss 100.427694, accuracy 0.198000\n",
      "iteration 190 / 1500: loss 99.447654, accuracy 0.218000\n",
      "iteration 191 / 1500: loss 98.603673, accuracy 0.186000\n",
      "iteration 192 / 1500: loss 97.710203, accuracy 0.182000\n",
      "iteration 193 / 1500: loss 96.776944, accuracy 0.206000\n",
      "iteration 194 / 1500: loss 95.941203, accuracy 0.181000\n",
      "iteration 195 / 1500: loss 95.000421, accuracy 0.204000\n",
      "iteration 196 / 1500: loss 94.117217, accuracy 0.198000\n",
      "iteration 197 / 1500: loss 93.253810, accuracy 0.201000\n",
      "iteration 198 / 1500: loss 92.443294, accuracy 0.179000\n",
      "iteration 199 / 1500: loss 91.587150, accuracy 0.204000\n",
      "iteration 200 / 1500: loss 90.775660, accuracy 0.179000\n",
      "iteration 201 / 1500: loss 89.853181, accuracy 0.218000\n",
      "iteration 202 / 1500: loss 89.141694, accuracy 0.175000\n",
      "iteration 203 / 1500: loss 88.225162, accuracy 0.215000\n",
      "iteration 204 / 1500: loss 87.461627, accuracy 0.215000\n",
      "iteration 205 / 1500: loss 86.659379, accuracy 0.209000\n",
      "iteration 206 / 1500: loss 85.871060, accuracy 0.213000\n",
      "iteration 207 / 1500: loss 85.122808, accuracy 0.178000\n",
      "iteration 208 / 1500: loss 84.305149, accuracy 0.200000\n",
      "iteration 209 / 1500: loss 83.484491, accuracy 0.236000\n",
      "iteration 210 / 1500: loss 82.789029, accuracy 0.191000\n",
      "iteration 211 / 1500: loss 82.009090, accuracy 0.199000\n",
      "iteration 212 / 1500: loss 81.300553, accuracy 0.189000\n",
      "iteration 213 / 1500: loss 80.461261, accuracy 0.231000\n",
      "iteration 214 / 1500: loss 79.756398, accuracy 0.224000\n",
      "iteration 215 / 1500: loss 79.027844, accuracy 0.207000\n",
      "iteration 216 / 1500: loss 78.311925, accuracy 0.200000\n",
      "iteration 217 / 1500: loss 77.566149, accuracy 0.196000\n",
      "iteration 218 / 1500: loss 76.911917, accuracy 0.205000\n",
      "iteration 219 / 1500: loss 76.182371, accuracy 0.216000\n",
      "iteration 220 / 1500: loss 75.467057, accuracy 0.220000\n",
      "iteration 221 / 1500: loss 74.774264, accuracy 0.219000\n",
      "iteration 222 / 1500: loss 74.131829, accuracy 0.220000\n",
      "iteration 223 / 1500: loss 73.422893, accuracy 0.200000\n",
      "iteration 224 / 1500: loss 72.781329, accuracy 0.217000\n",
      "iteration 225 / 1500: loss 72.122553, accuracy 0.213000\n",
      "iteration 226 / 1500: loss 71.475648, accuracy 0.219000\n",
      "iteration 227 / 1500: loss 70.766851, accuracy 0.225000\n",
      "iteration 228 / 1500: loss 70.178571, accuracy 0.211000\n",
      "iteration 229 / 1500: loss 69.567792, accuracy 0.205000\n",
      "iteration 230 / 1500: loss 68.846990, accuracy 0.231000\n",
      "iteration 231 / 1500: loss 68.261247, accuracy 0.223000\n",
      "iteration 232 / 1500: loss 67.647154, accuracy 0.199000\n",
      "iteration 233 / 1500: loss 67.009813, accuracy 0.223000\n",
      "iteration 234 / 1500: loss 66.415370, accuracy 0.200000\n",
      "iteration 235 / 1500: loss 65.831631, accuracy 0.199000\n",
      "iteration 236 / 1500: loss 65.194061, accuracy 0.241000\n",
      "iteration 237 / 1500: loss 64.605541, accuracy 0.231000\n",
      "iteration 238 / 1500: loss 63.997692, accuracy 0.232000\n",
      "iteration 239 / 1500: loss 63.454491, accuracy 0.226000\n",
      "iteration 240 / 1500: loss 62.848066, accuracy 0.236000\n",
      "iteration 241 / 1500: loss 62.335942, accuracy 0.219000\n",
      "iteration 242 / 1500: loss 61.765214, accuracy 0.227000\n",
      "iteration 243 / 1500: loss 61.187511, accuracy 0.225000\n",
      "iteration 244 / 1500: loss 60.596301, accuracy 0.225000\n",
      "iteration 245 / 1500: loss 60.064803, accuracy 0.220000\n",
      "iteration 246 / 1500: loss 59.535845, accuracy 0.232000\n",
      "iteration 247 / 1500: loss 58.987662, accuracy 0.218000\n",
      "iteration 248 / 1500: loss 58.468242, accuracy 0.211000\n",
      "iteration 249 / 1500: loss 57.934174, accuracy 0.220000\n",
      "iteration 250 / 1500: loss 57.361599, accuracy 0.243000\n",
      "iteration 251 / 1500: loss 56.876546, accuracy 0.234000\n",
      "iteration 252 / 1500: loss 56.364344, accuracy 0.230000\n",
      "iteration 253 / 1500: loss 55.891784, accuracy 0.226000\n",
      "iteration 254 / 1500: loss 55.298751, accuracy 0.242000\n",
      "iteration 255 / 1500: loss 54.846128, accuracy 0.230000\n",
      "iteration 256 / 1500: loss 54.372281, accuracy 0.213000\n",
      "iteration 257 / 1500: loss 53.882893, accuracy 0.217000\n",
      "iteration 258 / 1500: loss 53.382712, accuracy 0.225000\n",
      "iteration 259 / 1500: loss 52.860341, accuracy 0.240000\n",
      "iteration 260 / 1500: loss 52.423196, accuracy 0.225000\n",
      "iteration 261 / 1500: loss 51.978691, accuracy 0.208000\n",
      "iteration 262 / 1500: loss 51.479294, accuracy 0.225000\n",
      "iteration 263 / 1500: loss 51.069374, accuracy 0.227000\n",
      "iteration 264 / 1500: loss 50.549662, accuracy 0.232000\n",
      "iteration 265 / 1500: loss 50.097435, accuracy 0.243000\n",
      "iteration 266 / 1500: loss 49.611341, accuracy 0.252000\n",
      "iteration 267 / 1500: loss 49.219878, accuracy 0.218000\n",
      "iteration 268 / 1500: loss 48.752015, accuracy 0.260000\n",
      "iteration 269 / 1500: loss 48.323144, accuracy 0.234000\n",
      "iteration 270 / 1500: loss 47.877409, accuracy 0.256000\n",
      "iteration 271 / 1500: loss 47.471255, accuracy 0.229000\n",
      "iteration 272 / 1500: loss 47.052126, accuracy 0.238000\n",
      "iteration 273 / 1500: loss 46.601020, accuracy 0.236000\n",
      "iteration 274 / 1500: loss 46.273999, accuracy 0.205000\n",
      "iteration 275 / 1500: loss 45.772137, accuracy 0.236000\n",
      "iteration 276 / 1500: loss 45.361668, accuracy 0.236000\n",
      "iteration 277 / 1500: loss 44.924522, accuracy 0.256000\n",
      "iteration 278 / 1500: loss 44.586408, accuracy 0.234000\n",
      "iteration 279 / 1500: loss 44.173744, accuracy 0.239000\n",
      "iteration 280 / 1500: loss 43.737963, accuracy 0.255000\n",
      "iteration 281 / 1500: loss 43.391131, accuracy 0.233000\n",
      "iteration 282 / 1500: loss 43.009747, accuracy 0.226000\n",
      "iteration 283 / 1500: loss 42.590343, accuracy 0.242000\n",
      "iteration 284 / 1500: loss 42.251429, accuracy 0.211000\n",
      "iteration 285 / 1500: loss 41.873288, accuracy 0.239000\n",
      "iteration 286 / 1500: loss 41.494798, accuracy 0.245000\n",
      "iteration 287 / 1500: loss 41.117670, accuracy 0.236000\n",
      "iteration 288 / 1500: loss 40.752365, accuracy 0.248000\n",
      "iteration 289 / 1500: loss 40.367562, accuracy 0.238000\n",
      "iteration 290 / 1500: loss 40.061168, accuracy 0.255000\n",
      "iteration 291 / 1500: loss 39.725625, accuracy 0.218000\n",
      "iteration 292 / 1500: loss 39.311805, accuracy 0.238000\n",
      "iteration 293 / 1500: loss 38.968341, accuracy 0.240000\n",
      "iteration 294 / 1500: loss 38.643984, accuracy 0.238000\n",
      "iteration 295 / 1500: loss 38.264141, accuracy 0.247000\n",
      "iteration 296 / 1500: loss 37.952715, accuracy 0.249000\n",
      "iteration 297 / 1500: loss 37.606306, accuracy 0.246000\n",
      "iteration 298 / 1500: loss 37.242836, accuracy 0.252000\n",
      "iteration 299 / 1500: loss 36.900679, accuracy 0.256000\n",
      "iteration 300 / 1500: loss 36.637306, accuracy 0.223000\n",
      "iteration 301 / 1500: loss 36.277767, accuracy 0.261000\n",
      "iteration 302 / 1500: loss 35.965374, accuracy 0.242000\n",
      "iteration 303 / 1500: loss 35.640933, accuracy 0.254000\n",
      "iteration 304 / 1500: loss 35.330182, accuracy 0.254000\n",
      "iteration 305 / 1500: loss 34.994862, accuracy 0.244000\n",
      "iteration 306 / 1500: loss 34.707954, accuracy 0.254000\n",
      "iteration 307 / 1500: loss 34.409986, accuracy 0.246000\n",
      "iteration 308 / 1500: loss 34.110538, accuracy 0.243000\n",
      "iteration 309 / 1500: loss 33.825894, accuracy 0.254000\n",
      "iteration 310 / 1500: loss 33.515699, accuracy 0.261000\n",
      "iteration 311 / 1500: loss 33.202145, accuracy 0.258000\n",
      "iteration 312 / 1500: loss 32.876964, accuracy 0.272000\n",
      "iteration 313 / 1500: loss 32.599053, accuracy 0.265000\n",
      "iteration 314 / 1500: loss 32.332652, accuracy 0.262000\n",
      "iteration 315 / 1500: loss 32.057446, accuracy 0.264000\n",
      "iteration 316 / 1500: loss 31.781507, accuracy 0.251000\n",
      "iteration 317 / 1500: loss 31.492239, accuracy 0.252000\n",
      "iteration 318 / 1500: loss 31.215331, accuracy 0.258000\n",
      "iteration 319 / 1500: loss 30.913991, accuracy 0.254000\n",
      "iteration 320 / 1500: loss 30.663639, accuracy 0.275000\n",
      "iteration 321 / 1500: loss 30.385237, accuracy 0.244000\n",
      "iteration 322 / 1500: loss 30.134775, accuracy 0.267000\n",
      "iteration 323 / 1500: loss 29.894073, accuracy 0.237000\n",
      "iteration 324 / 1500: loss 29.602253, accuracy 0.260000\n",
      "iteration 325 / 1500: loss 29.376432, accuracy 0.237000\n",
      "iteration 326 / 1500: loss 29.077405, accuracy 0.256000\n",
      "iteration 327 / 1500: loss 28.853089, accuracy 0.258000\n",
      "iteration 328 / 1500: loss 28.537760, accuracy 0.292000\n",
      "iteration 329 / 1500: loss 28.344035, accuracy 0.260000\n",
      "iteration 330 / 1500: loss 28.079582, accuracy 0.271000\n",
      "iteration 331 / 1500: loss 27.852956, accuracy 0.262000\n",
      "iteration 332 / 1500: loss 27.622590, accuracy 0.262000\n",
      "iteration 333 / 1500: loss 27.352815, accuracy 0.267000\n",
      "iteration 334 / 1500: loss 27.125328, accuracy 0.266000\n",
      "iteration 335 / 1500: loss 26.931785, accuracy 0.252000\n",
      "iteration 336 / 1500: loss 26.672988, accuracy 0.271000\n",
      "iteration 337 / 1500: loss 26.429997, accuracy 0.263000\n",
      "iteration 338 / 1500: loss 26.191723, accuracy 0.273000\n",
      "iteration 339 / 1500: loss 25.967914, accuracy 0.269000\n",
      "iteration 340 / 1500: loss 25.791765, accuracy 0.243000\n",
      "iteration 341 / 1500: loss 25.537558, accuracy 0.255000\n",
      "iteration 342 / 1500: loss 25.312775, accuracy 0.256000\n",
      "iteration 343 / 1500: loss 25.092156, accuracy 0.267000\n",
      "iteration 344 / 1500: loss 24.855474, accuracy 0.282000\n",
      "iteration 345 / 1500: loss 24.650957, accuracy 0.250000\n",
      "iteration 346 / 1500: loss 24.438703, accuracy 0.281000\n",
      "iteration 347 / 1500: loss 24.228648, accuracy 0.277000\n",
      "iteration 348 / 1500: loss 24.021449, accuracy 0.276000\n",
      "iteration 349 / 1500: loss 23.797583, accuracy 0.285000\n",
      "iteration 350 / 1500: loss 23.644192, accuracy 0.267000\n",
      "iteration 351 / 1500: loss 23.421373, accuracy 0.285000\n",
      "iteration 352 / 1500: loss 23.235719, accuracy 0.277000\n",
      "iteration 353 / 1500: loss 23.036959, accuracy 0.263000\n",
      "iteration 354 / 1500: loss 22.840903, accuracy 0.289000\n",
      "iteration 355 / 1500: loss 22.636776, accuracy 0.254000\n",
      "iteration 356 / 1500: loss 22.431601, accuracy 0.264000\n",
      "iteration 357 / 1500: loss 22.218571, accuracy 0.283000\n",
      "iteration 358 / 1500: loss 22.082829, accuracy 0.268000\n",
      "iteration 359 / 1500: loss 21.861470, accuracy 0.289000\n",
      "iteration 360 / 1500: loss 21.685547, accuracy 0.276000\n",
      "iteration 361 / 1500: loss 21.478446, accuracy 0.291000\n",
      "iteration 362 / 1500: loss 21.330957, accuracy 0.268000\n",
      "iteration 363 / 1500: loss 21.112238, accuracy 0.293000\n",
      "iteration 364 / 1500: loss 20.945626, accuracy 0.293000\n",
      "iteration 365 / 1500: loss 20.777153, accuracy 0.289000\n",
      "iteration 366 / 1500: loss 20.611184, accuracy 0.264000\n",
      "iteration 367 / 1500: loss 20.425752, accuracy 0.277000\n",
      "iteration 368 / 1500: loss 20.236267, accuracy 0.294000\n",
      "iteration 369 / 1500: loss 20.121042, accuracy 0.251000\n",
      "iteration 370 / 1500: loss 19.903193, accuracy 0.279000\n",
      "iteration 371 / 1500: loss 19.734001, accuracy 0.301000\n",
      "iteration 372 / 1500: loss 19.582099, accuracy 0.281000\n",
      "iteration 373 / 1500: loss 19.413821, accuracy 0.292000\n",
      "iteration 374 / 1500: loss 19.263348, accuracy 0.281000\n",
      "iteration 375 / 1500: loss 19.121554, accuracy 0.247000\n",
      "iteration 376 / 1500: loss 18.912252, accuracy 0.295000\n",
      "iteration 377 / 1500: loss 18.787283, accuracy 0.287000\n",
      "iteration 378 / 1500: loss 18.618175, accuracy 0.268000\n",
      "iteration 379 / 1500: loss 18.474153, accuracy 0.284000\n",
      "iteration 380 / 1500: loss 18.302232, accuracy 0.301000\n",
      "iteration 381 / 1500: loss 18.134672, accuracy 0.291000\n",
      "iteration 382 / 1500: loss 18.007548, accuracy 0.274000\n",
      "iteration 383 / 1500: loss 17.855963, accuracy 0.295000\n",
      "iteration 384 / 1500: loss 17.705394, accuracy 0.286000\n",
      "iteration 385 / 1500: loss 17.548145, accuracy 0.275000\n",
      "iteration 386 / 1500: loss 17.426348, accuracy 0.251000\n",
      "iteration 387 / 1500: loss 17.299148, accuracy 0.275000\n",
      "iteration 388 / 1500: loss 17.108273, accuracy 0.287000\n",
      "iteration 389 / 1500: loss 16.984667, accuracy 0.294000\n",
      "iteration 390 / 1500: loss 16.848000, accuracy 0.301000\n",
      "iteration 391 / 1500: loss 16.688157, accuracy 0.303000\n",
      "iteration 392 / 1500: loss 16.586428, accuracy 0.271000\n",
      "iteration 393 / 1500: loss 16.437634, accuracy 0.281000\n",
      "iteration 394 / 1500: loss 16.289986, accuracy 0.302000\n",
      "iteration 395 / 1500: loss 16.169430, accuracy 0.284000\n",
      "iteration 396 / 1500: loss 16.063879, accuracy 0.263000\n",
      "iteration 397 / 1500: loss 15.934872, accuracy 0.289000\n",
      "iteration 398 / 1500: loss 15.747638, accuracy 0.309000\n",
      "iteration 399 / 1500: loss 15.646000, accuracy 0.304000\n",
      "iteration 400 / 1500: loss 15.514053, accuracy 0.306000\n",
      "iteration 401 / 1500: loss 15.363590, accuracy 0.309000\n",
      "iteration 402 / 1500: loss 15.272182, accuracy 0.277000\n",
      "iteration 403 / 1500: loss 15.152495, accuracy 0.300000\n",
      "iteration 404 / 1500: loss 14.987622, accuracy 0.328000\n",
      "iteration 405 / 1500: loss 14.926759, accuracy 0.274000\n",
      "iteration 406 / 1500: loss 14.767783, accuracy 0.288000\n",
      "iteration 407 / 1500: loss 14.687325, accuracy 0.283000\n",
      "iteration 408 / 1500: loss 14.552800, accuracy 0.271000\n",
      "iteration 409 / 1500: loss 14.420095, accuracy 0.290000\n",
      "iteration 410 / 1500: loss 14.308948, accuracy 0.286000\n",
      "iteration 411 / 1500: loss 14.169877, accuracy 0.285000\n",
      "iteration 412 / 1500: loss 14.083332, accuracy 0.285000\n",
      "iteration 413 / 1500: loss 13.997013, accuracy 0.286000\n",
      "iteration 414 / 1500: loss 13.828294, accuracy 0.293000\n",
      "iteration 415 / 1500: loss 13.735508, accuracy 0.300000\n",
      "iteration 416 / 1500: loss 13.638728, accuracy 0.271000\n",
      "iteration 417 / 1500: loss 13.525938, accuracy 0.305000\n",
      "iteration 418 / 1500: loss 13.429825, accuracy 0.277000\n",
      "iteration 419 / 1500: loss 13.307180, accuracy 0.305000\n",
      "iteration 420 / 1500: loss 13.255702, accuracy 0.268000\n",
      "iteration 421 / 1500: loss 13.084045, accuracy 0.309000\n",
      "iteration 422 / 1500: loss 12.998994, accuracy 0.284000\n",
      "iteration 423 / 1500: loss 12.897550, accuracy 0.306000\n",
      "iteration 424 / 1500: loss 12.832651, accuracy 0.277000\n",
      "iteration 425 / 1500: loss 12.732230, accuracy 0.266000\n",
      "iteration 426 / 1500: loss 12.576557, accuracy 0.308000\n",
      "iteration 427 / 1500: loss 12.483534, accuracy 0.302000\n",
      "iteration 428 / 1500: loss 12.415385, accuracy 0.287000\n",
      "iteration 429 / 1500: loss 12.296299, accuracy 0.297000\n",
      "iteration 430 / 1500: loss 12.221560, accuracy 0.290000\n",
      "iteration 431 / 1500: loss 12.100693, accuracy 0.319000\n",
      "iteration 432 / 1500: loss 12.015826, accuracy 0.300000\n",
      "iteration 433 / 1500: loss 11.916503, accuracy 0.327000\n",
      "iteration 434 / 1500: loss 11.833516, accuracy 0.313000\n",
      "iteration 435 / 1500: loss 11.727955, accuracy 0.308000\n",
      "iteration 436 / 1500: loss 11.653241, accuracy 0.281000\n",
      "iteration 437 / 1500: loss 11.558133, accuracy 0.297000\n",
      "iteration 438 / 1500: loss 11.466378, accuracy 0.313000\n",
      "iteration 439 / 1500: loss 11.374773, accuracy 0.306000\n",
      "iteration 440 / 1500: loss 11.314799, accuracy 0.296000\n",
      "iteration 441 / 1500: loss 11.223271, accuracy 0.307000\n",
      "iteration 442 / 1500: loss 11.092783, accuracy 0.313000\n",
      "iteration 443 / 1500: loss 11.060329, accuracy 0.300000\n",
      "iteration 444 / 1500: loss 10.962926, accuracy 0.300000\n",
      "iteration 445 / 1500: loss 10.878424, accuracy 0.300000\n",
      "iteration 446 / 1500: loss 10.781355, accuracy 0.310000\n",
      "iteration 447 / 1500: loss 10.704928, accuracy 0.290000\n",
      "iteration 448 / 1500: loss 10.590431, accuracy 0.319000\n",
      "iteration 449 / 1500: loss 10.563950, accuracy 0.289000\n",
      "iteration 450 / 1500: loss 10.455968, accuracy 0.317000\n",
      "iteration 451 / 1500: loss 10.410903, accuracy 0.283000\n",
      "iteration 452 / 1500: loss 10.317921, accuracy 0.295000\n",
      "iteration 453 / 1500: loss 10.241118, accuracy 0.305000\n",
      "iteration 454 / 1500: loss 10.114066, accuracy 0.331000\n",
      "iteration 455 / 1500: loss 10.100584, accuracy 0.302000\n",
      "iteration 456 / 1500: loss 9.989895, accuracy 0.304000\n",
      "iteration 457 / 1500: loss 9.950727, accuracy 0.284000\n",
      "iteration 458 / 1500: loss 9.858779, accuracy 0.288000\n",
      "iteration 459 / 1500: loss 9.774822, accuracy 0.313000\n",
      "iteration 460 / 1500: loss 9.730751, accuracy 0.308000\n",
      "iteration 461 / 1500: loss 9.655246, accuracy 0.321000\n",
      "iteration 462 / 1500: loss 9.586511, accuracy 0.304000\n",
      "iteration 463 / 1500: loss 9.515911, accuracy 0.313000\n",
      "iteration 464 / 1500: loss 9.417704, accuracy 0.338000\n",
      "iteration 465 / 1500: loss 9.362356, accuracy 0.292000\n",
      "iteration 466 / 1500: loss 9.262644, accuracy 0.335000\n",
      "iteration 467 / 1500: loss 9.232393, accuracy 0.302000\n",
      "iteration 468 / 1500: loss 9.167753, accuracy 0.318000\n",
      "iteration 469 / 1500: loss 9.072845, accuracy 0.324000\n",
      "iteration 470 / 1500: loss 9.018007, accuracy 0.304000\n",
      "iteration 471 / 1500: loss 8.941454, accuracy 0.324000\n",
      "iteration 472 / 1500: loss 8.883124, accuracy 0.317000\n",
      "iteration 473 / 1500: loss 8.807603, accuracy 0.311000\n",
      "iteration 474 / 1500: loss 8.764349, accuracy 0.311000\n",
      "iteration 475 / 1500: loss 8.691084, accuracy 0.306000\n",
      "iteration 476 / 1500: loss 8.669459, accuracy 0.277000\n",
      "iteration 477 / 1500: loss 8.581474, accuracy 0.307000\n",
      "iteration 478 / 1500: loss 8.505783, accuracy 0.312000\n",
      "iteration 479 / 1500: loss 8.456023, accuracy 0.319000\n",
      "iteration 480 / 1500: loss 8.370942, accuracy 0.332000\n",
      "iteration 481 / 1500: loss 8.303261, accuracy 0.326000\n",
      "iteration 482 / 1500: loss 8.276132, accuracy 0.303000\n",
      "iteration 483 / 1500: loss 8.197513, accuracy 0.316000\n",
      "iteration 484 / 1500: loss 8.145225, accuracy 0.335000\n",
      "iteration 485 / 1500: loss 8.070600, accuracy 0.357000\n",
      "iteration 486 / 1500: loss 8.027273, accuracy 0.318000\n",
      "iteration 487 / 1500: loss 7.973160, accuracy 0.320000\n",
      "iteration 488 / 1500: loss 7.907321, accuracy 0.329000\n",
      "iteration 489 / 1500: loss 7.899686, accuracy 0.311000\n",
      "iteration 490 / 1500: loss 7.810529, accuracy 0.329000\n",
      "iteration 491 / 1500: loss 7.773827, accuracy 0.310000\n",
      "iteration 492 / 1500: loss 7.736492, accuracy 0.297000\n",
      "iteration 493 / 1500: loss 7.669535, accuracy 0.328000\n",
      "iteration 494 / 1500: loss 7.606496, accuracy 0.304000\n",
      "iteration 495 / 1500: loss 7.585337, accuracy 0.295000\n",
      "iteration 496 / 1500: loss 7.505092, accuracy 0.322000\n",
      "iteration 497 / 1500: loss 7.447173, accuracy 0.323000\n",
      "iteration 498 / 1500: loss 7.375518, accuracy 0.329000\n",
      "iteration 499 / 1500: loss 7.372680, accuracy 0.316000\n",
      "iteration 500 / 1500: loss 7.296349, accuracy 0.325000\n",
      "iteration 501 / 1500: loss 7.272540, accuracy 0.294000\n",
      "iteration 502 / 1500: loss 7.232108, accuracy 0.303000\n",
      "iteration 503 / 1500: loss 7.168657, accuracy 0.312000\n",
      "iteration 504 / 1500: loss 7.119312, accuracy 0.316000\n",
      "iteration 505 / 1500: loss 7.093781, accuracy 0.302000\n",
      "iteration 506 / 1500: loss 7.003648, accuracy 0.337000\n",
      "iteration 507 / 1500: loss 6.971849, accuracy 0.327000\n",
      "iteration 508 / 1500: loss 6.929329, accuracy 0.317000\n",
      "iteration 509 / 1500: loss 6.892687, accuracy 0.315000\n",
      "iteration 510 / 1500: loss 6.824065, accuracy 0.323000\n",
      "iteration 511 / 1500: loss 6.776088, accuracy 0.341000\n",
      "iteration 512 / 1500: loss 6.760514, accuracy 0.296000\n",
      "iteration 513 / 1500: loss 6.697598, accuracy 0.345000\n",
      "iteration 514 / 1500: loss 6.642415, accuracy 0.327000\n",
      "iteration 515 / 1500: loss 6.620737, accuracy 0.320000\n",
      "iteration 516 / 1500: loss 6.579133, accuracy 0.326000\n",
      "iteration 517 / 1500: loss 6.527462, accuracy 0.320000\n",
      "iteration 518 / 1500: loss 6.491174, accuracy 0.325000\n",
      "iteration 519 / 1500: loss 6.451461, accuracy 0.305000\n",
      "iteration 520 / 1500: loss 6.422836, accuracy 0.287000\n",
      "iteration 521 / 1500: loss 6.375320, accuracy 0.307000\n",
      "iteration 522 / 1500: loss 6.357793, accuracy 0.277000\n",
      "iteration 523 / 1500: loss 6.278340, accuracy 0.316000\n",
      "iteration 524 / 1500: loss 6.268184, accuracy 0.283000\n",
      "iteration 525 / 1500: loss 6.177528, accuracy 0.325000\n",
      "iteration 526 / 1500: loss 6.182251, accuracy 0.319000\n",
      "iteration 527 / 1500: loss 6.139503, accuracy 0.306000\n",
      "iteration 528 / 1500: loss 6.079507, accuracy 0.349000\n",
      "iteration 529 / 1500: loss 6.044293, accuracy 0.316000\n",
      "iteration 530 / 1500: loss 5.994693, accuracy 0.328000\n",
      "iteration 531 / 1500: loss 5.995842, accuracy 0.296000\n",
      "iteration 532 / 1500: loss 5.923685, accuracy 0.337000\n",
      "iteration 533 / 1500: loss 5.904101, accuracy 0.308000\n",
      "iteration 534 / 1500: loss 5.861049, accuracy 0.315000\n",
      "iteration 535 / 1500: loss 5.820068, accuracy 0.325000\n",
      "iteration 536 / 1500: loss 5.804045, accuracy 0.317000\n",
      "iteration 537 / 1500: loss 5.765341, accuracy 0.328000\n",
      "iteration 538 / 1500: loss 5.777202, accuracy 0.306000\n",
      "iteration 539 / 1500: loss 5.656872, accuracy 0.336000\n",
      "iteration 540 / 1500: loss 5.653058, accuracy 0.316000\n",
      "iteration 541 / 1500: loss 5.632824, accuracy 0.307000\n",
      "iteration 542 / 1500: loss 5.607513, accuracy 0.329000\n",
      "iteration 543 / 1500: loss 5.584770, accuracy 0.298000\n",
      "iteration 544 / 1500: loss 5.523998, accuracy 0.338000\n",
      "iteration 545 / 1500: loss 5.486940, accuracy 0.329000\n",
      "iteration 546 / 1500: loss 5.423518, accuracy 0.343000\n",
      "iteration 547 / 1500: loss 5.446449, accuracy 0.317000\n",
      "iteration 548 / 1500: loss 5.410091, accuracy 0.330000\n",
      "iteration 549 / 1500: loss 5.380677, accuracy 0.306000\n",
      "iteration 550 / 1500: loss 5.330925, accuracy 0.303000\n",
      "iteration 551 / 1500: loss 5.315396, accuracy 0.315000\n",
      "iteration 552 / 1500: loss 5.269580, accuracy 0.315000\n",
      "iteration 553 / 1500: loss 5.249873, accuracy 0.331000\n",
      "iteration 554 / 1500: loss 5.206275, accuracy 0.337000\n",
      "iteration 555 / 1500: loss 5.186681, accuracy 0.335000\n",
      "iteration 556 / 1500: loss 5.125732, accuracy 0.346000\n",
      "iteration 557 / 1500: loss 5.140070, accuracy 0.318000\n",
      "iteration 558 / 1500: loss 5.078404, accuracy 0.337000\n",
      "iteration 559 / 1500: loss 5.073729, accuracy 0.327000\n",
      "iteration 560 / 1500: loss 5.085148, accuracy 0.281000\n",
      "iteration 561 / 1500: loss 5.020072, accuracy 0.312000\n",
      "iteration 562 / 1500: loss 4.993601, accuracy 0.331000\n",
      "iteration 563 / 1500: loss 4.964008, accuracy 0.334000\n",
      "iteration 564 / 1500: loss 4.911889, accuracy 0.349000\n",
      "iteration 565 / 1500: loss 4.896345, accuracy 0.324000\n",
      "iteration 566 / 1500: loss 4.864815, accuracy 0.335000\n",
      "iteration 567 / 1500: loss 4.861086, accuracy 0.319000\n",
      "iteration 568 / 1500: loss 4.814602, accuracy 0.335000\n",
      "iteration 569 / 1500: loss 4.820716, accuracy 0.316000\n",
      "iteration 570 / 1500: loss 4.768026, accuracy 0.314000\n",
      "iteration 571 / 1500: loss 4.754219, accuracy 0.323000\n",
      "iteration 572 / 1500: loss 4.702663, accuracy 0.341000\n",
      "iteration 573 / 1500: loss 4.732320, accuracy 0.304000\n",
      "iteration 574 / 1500: loss 4.676534, accuracy 0.327000\n",
      "iteration 575 / 1500: loss 4.650124, accuracy 0.324000\n",
      "iteration 576 / 1500: loss 4.637834, accuracy 0.314000\n",
      "iteration 577 / 1500: loss 4.589115, accuracy 0.322000\n",
      "iteration 578 / 1500: loss 4.545449, accuracy 0.322000\n",
      "iteration 579 / 1500: loss 4.524249, accuracy 0.343000\n",
      "iteration 580 / 1500: loss 4.542427, accuracy 0.312000\n",
      "iteration 581 / 1500: loss 4.498486, accuracy 0.320000\n",
      "iteration 582 / 1500: loss 4.465090, accuracy 0.341000\n",
      "iteration 583 / 1500: loss 4.454389, accuracy 0.303000\n",
      "iteration 584 / 1500: loss 4.412702, accuracy 0.361000\n",
      "iteration 585 / 1500: loss 4.457473, accuracy 0.296000\n",
      "iteration 586 / 1500: loss 4.394667, accuracy 0.327000\n",
      "iteration 587 / 1500: loss 4.364614, accuracy 0.340000\n",
      "iteration 588 / 1500: loss 4.330820, accuracy 0.368000\n",
      "iteration 589 / 1500: loss 4.282065, accuracy 0.357000\n",
      "iteration 590 / 1500: loss 4.290057, accuracy 0.326000\n",
      "iteration 591 / 1500: loss 4.245151, accuracy 0.352000\n",
      "iteration 592 / 1500: loss 4.259996, accuracy 0.335000\n",
      "iteration 593 / 1500: loss 4.272497, accuracy 0.313000\n",
      "iteration 594 / 1500: loss 4.230815, accuracy 0.356000\n",
      "iteration 595 / 1500: loss 4.201117, accuracy 0.327000\n",
      "iteration 596 / 1500: loss 4.166103, accuracy 0.329000\n",
      "iteration 597 / 1500: loss 4.195348, accuracy 0.316000\n",
      "iteration 598 / 1500: loss 4.158591, accuracy 0.318000\n",
      "iteration 599 / 1500: loss 4.129814, accuracy 0.321000\n",
      "iteration 600 / 1500: loss 4.113056, accuracy 0.321000\n",
      "iteration 601 / 1500: loss 4.089523, accuracy 0.319000\n",
      "iteration 602 / 1500: loss 4.077866, accuracy 0.334000\n",
      "iteration 603 / 1500: loss 4.039985, accuracy 0.331000\n",
      "iteration 604 / 1500: loss 4.030204, accuracy 0.342000\n",
      "iteration 605 / 1500: loss 4.004931, accuracy 0.332000\n",
      "iteration 606 / 1500: loss 3.993502, accuracy 0.332000\n",
      "iteration 607 / 1500: loss 3.973648, accuracy 0.331000\n",
      "iteration 608 / 1500: loss 3.926651, accuracy 0.378000\n",
      "iteration 609 / 1500: loss 3.933966, accuracy 0.336000\n",
      "iteration 610 / 1500: loss 3.947706, accuracy 0.313000\n",
      "iteration 611 / 1500: loss 3.895261, accuracy 0.318000\n",
      "iteration 612 / 1500: loss 3.865271, accuracy 0.333000\n",
      "iteration 613 / 1500: loss 3.858612, accuracy 0.363000\n",
      "iteration 614 / 1500: loss 3.868246, accuracy 0.304000\n",
      "iteration 615 / 1500: loss 3.833519, accuracy 0.332000\n",
      "iteration 616 / 1500: loss 3.802244, accuracy 0.354000\n",
      "iteration 617 / 1500: loss 3.818842, accuracy 0.297000\n",
      "iteration 618 / 1500: loss 3.795228, accuracy 0.314000\n",
      "iteration 619 / 1500: loss 3.801825, accuracy 0.314000\n",
      "iteration 620 / 1500: loss 3.769065, accuracy 0.316000\n",
      "iteration 621 / 1500: loss 3.717182, accuracy 0.347000\n",
      "iteration 622 / 1500: loss 3.732295, accuracy 0.312000\n",
      "iteration 623 / 1500: loss 3.697469, accuracy 0.348000\n",
      "iteration 624 / 1500: loss 3.684203, accuracy 0.331000\n",
      "iteration 625 / 1500: loss 3.666352, accuracy 0.348000\n",
      "iteration 626 / 1500: loss 3.635528, accuracy 0.353000\n",
      "iteration 627 / 1500: loss 3.636583, accuracy 0.344000\n",
      "iteration 628 / 1500: loss 3.669832, accuracy 0.322000\n",
      "iteration 629 / 1500: loss 3.616604, accuracy 0.318000\n",
      "iteration 630 / 1500: loss 3.605054, accuracy 0.336000\n",
      "iteration 631 / 1500: loss 3.606528, accuracy 0.306000\n",
      "iteration 632 / 1500: loss 3.558805, accuracy 0.322000\n",
      "iteration 633 / 1500: loss 3.566403, accuracy 0.325000\n",
      "iteration 634 / 1500: loss 3.561393, accuracy 0.316000\n",
      "iteration 635 / 1500: loss 3.518182, accuracy 0.342000\n",
      "iteration 636 / 1500: loss 3.520488, accuracy 0.318000\n",
      "iteration 637 / 1500: loss 3.487718, accuracy 0.338000\n",
      "iteration 638 / 1500: loss 3.492812, accuracy 0.346000\n",
      "iteration 639 / 1500: loss 3.461782, accuracy 0.349000\n",
      "iteration 640 / 1500: loss 3.479942, accuracy 0.317000\n",
      "iteration 641 / 1500: loss 3.448459, accuracy 0.336000\n",
      "iteration 642 / 1500: loss 3.406229, accuracy 0.372000\n",
      "iteration 643 / 1500: loss 3.405494, accuracy 0.362000\n",
      "iteration 644 / 1500: loss 3.435860, accuracy 0.329000\n",
      "iteration 645 / 1500: loss 3.407184, accuracy 0.315000\n",
      "iteration 646 / 1500: loss 3.387079, accuracy 0.338000\n",
      "iteration 647 / 1500: loss 3.334718, accuracy 0.353000\n",
      "iteration 648 / 1500: loss 3.370756, accuracy 0.324000\n",
      "iteration 649 / 1500: loss 3.364073, accuracy 0.326000\n",
      "iteration 650 / 1500: loss 3.365374, accuracy 0.338000\n",
      "iteration 651 / 1500: loss 3.327691, accuracy 0.328000\n",
      "iteration 652 / 1500: loss 3.311252, accuracy 0.321000\n",
      "iteration 653 / 1500: loss 3.293245, accuracy 0.319000\n",
      "iteration 654 / 1500: loss 3.250837, accuracy 0.348000\n",
      "iteration 655 / 1500: loss 3.285991, accuracy 0.316000\n",
      "iteration 656 / 1500: loss 3.281435, accuracy 0.314000\n",
      "iteration 657 / 1500: loss 3.260012, accuracy 0.316000\n",
      "iteration 658 / 1500: loss 3.244587, accuracy 0.335000\n",
      "iteration 659 / 1500: loss 3.205882, accuracy 0.342000\n",
      "iteration 660 / 1500: loss 3.191109, accuracy 0.321000\n",
      "iteration 661 / 1500: loss 3.213533, accuracy 0.310000\n",
      "iteration 662 / 1500: loss 3.171361, accuracy 0.331000\n",
      "iteration 663 / 1500: loss 3.167266, accuracy 0.340000\n",
      "iteration 664 / 1500: loss 3.189771, accuracy 0.304000\n",
      "iteration 665 / 1500: loss 3.179419, accuracy 0.321000\n",
      "iteration 666 / 1500: loss 3.142035, accuracy 0.336000\n",
      "iteration 667 / 1500: loss 3.140507, accuracy 0.356000\n",
      "iteration 668 / 1500: loss 3.106279, accuracy 0.353000\n",
      "iteration 669 / 1500: loss 3.130100, accuracy 0.326000\n",
      "iteration 670 / 1500: loss 3.136895, accuracy 0.323000\n",
      "iteration 671 / 1500: loss 3.125425, accuracy 0.322000\n",
      "iteration 672 / 1500: loss 3.098882, accuracy 0.340000\n",
      "iteration 673 / 1500: loss 3.079621, accuracy 0.316000\n",
      "iteration 674 / 1500: loss 3.071983, accuracy 0.325000\n",
      "iteration 675 / 1500: loss 3.085552, accuracy 0.330000\n",
      "iteration 676 / 1500: loss 3.064684, accuracy 0.341000\n",
      "iteration 677 / 1500: loss 3.045491, accuracy 0.328000\n",
      "iteration 678 / 1500: loss 3.028779, accuracy 0.350000\n",
      "iteration 679 / 1500: loss 3.043821, accuracy 0.322000\n",
      "iteration 680 / 1500: loss 3.020068, accuracy 0.329000\n",
      "iteration 681 / 1500: loss 3.022312, accuracy 0.340000\n",
      "iteration 682 / 1500: loss 3.030138, accuracy 0.316000\n",
      "iteration 683 / 1500: loss 3.022897, accuracy 0.321000\n",
      "iteration 684 / 1500: loss 2.991287, accuracy 0.321000\n",
      "iteration 685 / 1500: loss 2.971284, accuracy 0.331000\n",
      "iteration 686 / 1500: loss 2.991666, accuracy 0.317000\n",
      "iteration 687 / 1500: loss 2.968851, accuracy 0.316000\n",
      "iteration 688 / 1500: loss 2.943216, accuracy 0.347000\n",
      "iteration 689 / 1500: loss 2.964123, accuracy 0.313000\n",
      "iteration 690 / 1500: loss 2.942147, accuracy 0.331000\n",
      "iteration 691 / 1500: loss 2.947763, accuracy 0.352000\n",
      "iteration 692 / 1500: loss 2.926424, accuracy 0.330000\n",
      "iteration 693 / 1500: loss 2.883342, accuracy 0.348000\n",
      "iteration 694 / 1500: loss 2.933170, accuracy 0.322000\n",
      "iteration 695 / 1500: loss 2.923651, accuracy 0.344000\n",
      "iteration 696 / 1500: loss 2.871724, accuracy 0.344000\n",
      "iteration 697 / 1500: loss 2.873866, accuracy 0.334000\n",
      "iteration 698 / 1500: loss 2.859315, accuracy 0.334000\n",
      "iteration 699 / 1500: loss 2.832221, accuracy 0.352000\n",
      "iteration 700 / 1500: loss 2.852223, accuracy 0.342000\n",
      "iteration 701 / 1500: loss 2.845623, accuracy 0.330000\n",
      "iteration 702 / 1500: loss 2.816648, accuracy 0.349000\n",
      "iteration 703 / 1500: loss 2.854002, accuracy 0.313000\n",
      "iteration 704 / 1500: loss 2.817184, accuracy 0.344000\n",
      "iteration 705 / 1500: loss 2.813798, accuracy 0.327000\n",
      "iteration 706 / 1500: loss 2.802869, accuracy 0.326000\n",
      "iteration 707 / 1500: loss 2.830993, accuracy 0.332000\n",
      "iteration 708 / 1500: loss 2.798049, accuracy 0.346000\n",
      "iteration 709 / 1500: loss 2.792558, accuracy 0.335000\n",
      "iteration 710 / 1500: loss 2.817831, accuracy 0.330000\n",
      "iteration 711 / 1500: loss 2.769956, accuracy 0.351000\n",
      "iteration 712 / 1500: loss 2.750963, accuracy 0.342000\n",
      "iteration 713 / 1500: loss 2.737792, accuracy 0.358000\n",
      "iteration 714 / 1500: loss 2.739728, accuracy 0.362000\n",
      "iteration 715 / 1500: loss 2.732132, accuracy 0.345000\n",
      "iteration 716 / 1500: loss 2.763400, accuracy 0.335000\n",
      "iteration 717 / 1500: loss 2.746815, accuracy 0.334000\n",
      "iteration 718 / 1500: loss 2.735685, accuracy 0.345000\n",
      "iteration 719 / 1500: loss 2.685435, accuracy 0.368000\n",
      "iteration 720 / 1500: loss 2.716766, accuracy 0.325000\n",
      "iteration 721 / 1500: loss 2.701829, accuracy 0.336000\n",
      "iteration 722 / 1500: loss 2.721463, accuracy 0.306000\n",
      "iteration 723 / 1500: loss 2.698264, accuracy 0.333000\n",
      "iteration 724 / 1500: loss 2.688150, accuracy 0.331000\n",
      "iteration 725 / 1500: loss 2.670299, accuracy 0.332000\n",
      "iteration 726 / 1500: loss 2.672079, accuracy 0.331000\n",
      "iteration 727 / 1500: loss 2.667391, accuracy 0.357000\n",
      "iteration 728 / 1500: loss 2.689142, accuracy 0.327000\n",
      "iteration 729 / 1500: loss 2.688589, accuracy 0.296000\n",
      "iteration 730 / 1500: loss 2.676590, accuracy 0.343000\n",
      "iteration 731 / 1500: loss 2.689122, accuracy 0.314000\n",
      "iteration 732 / 1500: loss 2.651002, accuracy 0.315000\n",
      "iteration 733 / 1500: loss 2.668804, accuracy 0.307000\n",
      "iteration 734 / 1500: loss 2.608051, accuracy 0.366000\n",
      "iteration 735 / 1500: loss 2.627185, accuracy 0.328000\n",
      "iteration 736 / 1500: loss 2.636105, accuracy 0.318000\n",
      "iteration 737 / 1500: loss 2.619763, accuracy 0.346000\n",
      "iteration 738 / 1500: loss 2.604140, accuracy 0.343000\n",
      "iteration 739 / 1500: loss 2.628588, accuracy 0.358000\n",
      "iteration 740 / 1500: loss 2.597099, accuracy 0.359000\n",
      "iteration 741 / 1500: loss 2.578107, accuracy 0.333000\n",
      "iteration 742 / 1500: loss 2.580586, accuracy 0.331000\n",
      "iteration 743 / 1500: loss 2.586428, accuracy 0.341000\n",
      "iteration 744 / 1500: loss 2.566766, accuracy 0.358000\n",
      "iteration 745 / 1500: loss 2.598919, accuracy 0.326000\n",
      "iteration 746 / 1500: loss 2.547872, accuracy 0.369000\n",
      "iteration 747 / 1500: loss 2.570593, accuracy 0.347000\n",
      "iteration 748 / 1500: loss 2.552876, accuracy 0.355000\n",
      "iteration 749 / 1500: loss 2.568317, accuracy 0.342000\n",
      "iteration 750 / 1500: loss 2.531443, accuracy 0.365000\n",
      "iteration 751 / 1500: loss 2.528830, accuracy 0.332000\n",
      "iteration 752 / 1500: loss 2.532906, accuracy 0.355000\n",
      "iteration 753 / 1500: loss 2.546648, accuracy 0.336000\n",
      "iteration 754 / 1500: loss 2.547080, accuracy 0.327000\n",
      "iteration 755 / 1500: loss 2.531916, accuracy 0.334000\n",
      "iteration 756 / 1500: loss 2.509090, accuracy 0.349000\n",
      "iteration 757 / 1500: loss 2.492217, accuracy 0.353000\n",
      "iteration 758 / 1500: loss 2.503666, accuracy 0.347000\n",
      "iteration 759 / 1500: loss 2.523680, accuracy 0.337000\n",
      "iteration 760 / 1500: loss 2.515399, accuracy 0.338000\n",
      "iteration 761 / 1500: loss 2.514688, accuracy 0.332000\n",
      "iteration 762 / 1500: loss 2.492031, accuracy 0.333000\n",
      "iteration 763 / 1500: loss 2.504416, accuracy 0.321000\n",
      "iteration 764 / 1500: loss 2.527018, accuracy 0.328000\n",
      "iteration 765 / 1500: loss 2.487526, accuracy 0.363000\n",
      "iteration 766 / 1500: loss 2.482772, accuracy 0.332000\n",
      "iteration 767 / 1500: loss 2.477151, accuracy 0.335000\n",
      "iteration 768 / 1500: loss 2.497806, accuracy 0.316000\n",
      "iteration 769 / 1500: loss 2.495767, accuracy 0.329000\n",
      "iteration 770 / 1500: loss 2.468076, accuracy 0.325000\n",
      "iteration 771 / 1500: loss 2.451601, accuracy 0.356000\n",
      "iteration 772 / 1500: loss 2.456450, accuracy 0.339000\n",
      "iteration 773 / 1500: loss 2.458539, accuracy 0.319000\n",
      "iteration 774 / 1500: loss 2.460625, accuracy 0.344000\n",
      "iteration 775 / 1500: loss 2.454507, accuracy 0.329000\n",
      "iteration 776 / 1500: loss 2.436169, accuracy 0.340000\n",
      "iteration 777 / 1500: loss 2.457432, accuracy 0.298000\n",
      "iteration 778 / 1500: loss 2.440475, accuracy 0.349000\n",
      "iteration 779 / 1500: loss 2.445711, accuracy 0.330000\n",
      "iteration 780 / 1500: loss 2.427070, accuracy 0.326000\n",
      "iteration 781 / 1500: loss 2.423439, accuracy 0.340000\n",
      "iteration 782 / 1500: loss 2.428922, accuracy 0.339000\n",
      "iteration 783 / 1500: loss 2.431622, accuracy 0.337000\n",
      "iteration 784 / 1500: loss 2.421282, accuracy 0.348000\n",
      "iteration 785 / 1500: loss 2.395964, accuracy 0.332000\n",
      "iteration 786 / 1500: loss 2.409507, accuracy 0.333000\n",
      "iteration 787 / 1500: loss 2.429400, accuracy 0.336000\n",
      "iteration 788 / 1500: loss 2.407507, accuracy 0.314000\n",
      "iteration 789 / 1500: loss 2.409992, accuracy 0.332000\n",
      "iteration 790 / 1500: loss 2.402374, accuracy 0.347000\n",
      "iteration 791 / 1500: loss 2.386083, accuracy 0.323000\n",
      "iteration 792 / 1500: loss 2.400250, accuracy 0.349000\n",
      "iteration 793 / 1500: loss 2.380529, accuracy 0.351000\n",
      "iteration 794 / 1500: loss 2.387610, accuracy 0.346000\n",
      "iteration 795 / 1500: loss 2.377439, accuracy 0.357000\n",
      "iteration 796 / 1500: loss 2.393999, accuracy 0.316000\n",
      "iteration 797 / 1500: loss 2.411491, accuracy 0.288000\n",
      "iteration 798 / 1500: loss 2.366627, accuracy 0.365000\n",
      "iteration 799 / 1500: loss 2.374852, accuracy 0.339000\n",
      "iteration 800 / 1500: loss 2.379513, accuracy 0.320000\n",
      "iteration 801 / 1500: loss 2.348002, accuracy 0.356000\n",
      "iteration 802 / 1500: loss 2.337870, accuracy 0.334000\n",
      "iteration 803 / 1500: loss 2.347212, accuracy 0.345000\n",
      "iteration 804 / 1500: loss 2.367393, accuracy 0.320000\n",
      "iteration 805 / 1500: loss 2.375393, accuracy 0.340000\n",
      "iteration 806 / 1500: loss 2.328220, accuracy 0.342000\n",
      "iteration 807 / 1500: loss 2.339732, accuracy 0.331000\n",
      "iteration 808 / 1500: loss 2.348231, accuracy 0.336000\n",
      "iteration 809 / 1500: loss 2.358501, accuracy 0.315000\n",
      "iteration 810 / 1500: loss 2.338837, accuracy 0.359000\n",
      "iteration 811 / 1500: loss 2.314631, accuracy 0.347000\n",
      "iteration 812 / 1500: loss 2.339994, accuracy 0.332000\n",
      "iteration 813 / 1500: loss 2.323382, accuracy 0.348000\n",
      "iteration 814 / 1500: loss 2.325824, accuracy 0.343000\n",
      "iteration 815 / 1500: loss 2.334215, accuracy 0.343000\n",
      "iteration 816 / 1500: loss 2.306378, accuracy 0.348000\n",
      "iteration 817 / 1500: loss 2.347549, accuracy 0.326000\n",
      "iteration 818 / 1500: loss 2.348735, accuracy 0.324000\n",
      "iteration 819 / 1500: loss 2.295330, accuracy 0.357000\n",
      "iteration 820 / 1500: loss 2.327481, accuracy 0.336000\n",
      "iteration 821 / 1500: loss 2.272892, accuracy 0.364000\n",
      "iteration 822 / 1500: loss 2.318497, accuracy 0.337000\n",
      "iteration 823 / 1500: loss 2.313138, accuracy 0.326000\n",
      "iteration 824 / 1500: loss 2.319884, accuracy 0.321000\n",
      "iteration 825 / 1500: loss 2.280290, accuracy 0.353000\n",
      "iteration 826 / 1500: loss 2.302693, accuracy 0.351000\n",
      "iteration 827 / 1500: loss 2.291742, accuracy 0.346000\n",
      "iteration 828 / 1500: loss 2.304917, accuracy 0.324000\n",
      "iteration 829 / 1500: loss 2.323709, accuracy 0.316000\n",
      "iteration 830 / 1500: loss 2.306697, accuracy 0.333000\n",
      "iteration 831 / 1500: loss 2.296396, accuracy 0.326000\n",
      "iteration 832 / 1500: loss 2.301173, accuracy 0.333000\n",
      "iteration 833 / 1500: loss 2.312000, accuracy 0.307000\n",
      "iteration 834 / 1500: loss 2.284621, accuracy 0.344000\n",
      "iteration 835 / 1500: loss 2.262850, accuracy 0.341000\n",
      "iteration 836 / 1500: loss 2.277407, accuracy 0.330000\n",
      "iteration 837 / 1500: loss 2.264173, accuracy 0.328000\n",
      "iteration 838 / 1500: loss 2.268489, accuracy 0.337000\n",
      "iteration 839 / 1500: loss 2.270441, accuracy 0.327000\n",
      "iteration 840 / 1500: loss 2.244031, accuracy 0.361000\n",
      "iteration 841 / 1500: loss 2.279066, accuracy 0.309000\n",
      "iteration 842 / 1500: loss 2.275487, accuracy 0.338000\n",
      "iteration 843 / 1500: loss 2.257095, accuracy 0.346000\n",
      "iteration 844 / 1500: loss 2.269094, accuracy 0.333000\n",
      "iteration 845 / 1500: loss 2.265634, accuracy 0.357000\n",
      "iteration 846 / 1500: loss 2.280427, accuracy 0.336000\n",
      "iteration 847 / 1500: loss 2.258498, accuracy 0.324000\n",
      "iteration 848 / 1500: loss 2.252828, accuracy 0.344000\n",
      "iteration 849 / 1500: loss 2.247740, accuracy 0.345000\n",
      "iteration 850 / 1500: loss 2.270547, accuracy 0.312000\n",
      "iteration 851 / 1500: loss 2.242790, accuracy 0.347000\n",
      "iteration 852 / 1500: loss 2.254337, accuracy 0.312000\n",
      "iteration 853 / 1500: loss 2.245191, accuracy 0.358000\n",
      "iteration 854 / 1500: loss 2.250914, accuracy 0.330000\n",
      "iteration 855 / 1500: loss 2.216375, accuracy 0.358000\n",
      "iteration 856 / 1500: loss 2.238059, accuracy 0.347000\n",
      "iteration 857 / 1500: loss 2.236832, accuracy 0.343000\n",
      "iteration 858 / 1500: loss 2.246094, accuracy 0.329000\n",
      "iteration 859 / 1500: loss 2.237281, accuracy 0.333000\n",
      "iteration 860 / 1500: loss 2.263626, accuracy 0.339000\n",
      "iteration 861 / 1500: loss 2.244593, accuracy 0.314000\n",
      "iteration 862 / 1500: loss 2.226993, accuracy 0.334000\n",
      "iteration 863 / 1500: loss 2.219837, accuracy 0.344000\n",
      "iteration 864 / 1500: loss 2.216873, accuracy 0.332000\n",
      "iteration 865 / 1500: loss 2.232123, accuracy 0.321000\n",
      "iteration 866 / 1500: loss 2.193906, accuracy 0.369000\n",
      "iteration 867 / 1500: loss 2.253924, accuracy 0.311000\n",
      "iteration 868 / 1500: loss 2.223738, accuracy 0.333000\n",
      "iteration 869 / 1500: loss 2.229037, accuracy 0.342000\n",
      "iteration 870 / 1500: loss 2.217640, accuracy 0.335000\n",
      "iteration 871 / 1500: loss 2.218800, accuracy 0.331000\n",
      "iteration 872 / 1500: loss 2.233193, accuracy 0.320000\n",
      "iteration 873 / 1500: loss 2.240677, accuracy 0.326000\n",
      "iteration 874 / 1500: loss 2.214281, accuracy 0.345000\n",
      "iteration 875 / 1500: loss 2.193391, accuracy 0.340000\n",
      "iteration 876 / 1500: loss 2.233741, accuracy 0.315000\n",
      "iteration 877 / 1500: loss 2.226530, accuracy 0.346000\n",
      "iteration 878 / 1500: loss 2.208898, accuracy 0.334000\n",
      "iteration 879 / 1500: loss 2.202088, accuracy 0.343000\n",
      "iteration 880 / 1500: loss 2.210471, accuracy 0.331000\n",
      "iteration 881 / 1500: loss 2.185574, accuracy 0.327000\n",
      "iteration 882 / 1500: loss 2.211053, accuracy 0.353000\n",
      "iteration 883 / 1500: loss 2.210422, accuracy 0.345000\n",
      "iteration 884 / 1500: loss 2.230752, accuracy 0.328000\n",
      "iteration 885 / 1500: loss 2.203178, accuracy 0.330000\n",
      "iteration 886 / 1500: loss 2.207570, accuracy 0.352000\n",
      "iteration 887 / 1500: loss 2.194915, accuracy 0.331000\n",
      "iteration 888 / 1500: loss 2.216857, accuracy 0.320000\n",
      "iteration 889 / 1500: loss 2.186489, accuracy 0.342000\n",
      "iteration 890 / 1500: loss 2.202146, accuracy 0.336000\n",
      "iteration 891 / 1500: loss 2.188460, accuracy 0.345000\n",
      "iteration 892 / 1500: loss 2.179216, accuracy 0.337000\n",
      "iteration 893 / 1500: loss 2.152050, accuracy 0.353000\n",
      "iteration 894 / 1500: loss 2.172083, accuracy 0.353000\n",
      "iteration 895 / 1500: loss 2.163989, accuracy 0.333000\n",
      "iteration 896 / 1500: loss 2.199957, accuracy 0.329000\n",
      "iteration 897 / 1500: loss 2.179419, accuracy 0.330000\n",
      "iteration 898 / 1500: loss 2.192176, accuracy 0.334000\n",
      "iteration 899 / 1500: loss 2.196778, accuracy 0.315000\n",
      "iteration 900 / 1500: loss 2.165179, accuracy 0.339000\n",
      "iteration 901 / 1500: loss 2.177340, accuracy 0.341000\n",
      "iteration 902 / 1500: loss 2.191716, accuracy 0.331000\n",
      "iteration 903 / 1500: loss 2.169532, accuracy 0.339000\n",
      "iteration 904 / 1500: loss 2.199232, accuracy 0.334000\n",
      "iteration 905 / 1500: loss 2.186210, accuracy 0.322000\n",
      "iteration 906 / 1500: loss 2.148723, accuracy 0.350000\n",
      "iteration 907 / 1500: loss 2.189352, accuracy 0.323000\n",
      "iteration 908 / 1500: loss 2.159802, accuracy 0.348000\n",
      "iteration 909 / 1500: loss 2.225049, accuracy 0.320000\n",
      "iteration 910 / 1500: loss 2.176208, accuracy 0.327000\n",
      "iteration 911 / 1500: loss 2.152639, accuracy 0.345000\n",
      "iteration 912 / 1500: loss 2.200486, accuracy 0.299000\n",
      "iteration 913 / 1500: loss 2.198624, accuracy 0.319000\n",
      "iteration 914 / 1500: loss 2.169262, accuracy 0.329000\n",
      "iteration 915 / 1500: loss 2.154640, accuracy 0.338000\n",
      "iteration 916 / 1500: loss 2.139025, accuracy 0.341000\n",
      "iteration 917 / 1500: loss 2.153629, accuracy 0.341000\n",
      "iteration 918 / 1500: loss 2.156608, accuracy 0.348000\n",
      "iteration 919 / 1500: loss 2.152455, accuracy 0.332000\n",
      "iteration 920 / 1500: loss 2.163284, accuracy 0.328000\n",
      "iteration 921 / 1500: loss 2.164791, accuracy 0.328000\n",
      "iteration 922 / 1500: loss 2.158315, accuracy 0.329000\n",
      "iteration 923 / 1500: loss 2.135647, accuracy 0.331000\n",
      "iteration 924 / 1500: loss 2.146114, accuracy 0.336000\n",
      "iteration 925 / 1500: loss 2.129091, accuracy 0.366000\n",
      "iteration 926 / 1500: loss 2.183266, accuracy 0.314000\n",
      "iteration 927 / 1500: loss 2.163621, accuracy 0.319000\n",
      "iteration 928 / 1500: loss 2.188037, accuracy 0.325000\n",
      "iteration 929 / 1500: loss 2.134516, accuracy 0.348000\n",
      "iteration 930 / 1500: loss 2.140852, accuracy 0.329000\n",
      "iteration 931 / 1500: loss 2.143288, accuracy 0.360000\n",
      "iteration 932 / 1500: loss 2.129482, accuracy 0.349000\n",
      "iteration 933 / 1500: loss 2.153472, accuracy 0.336000\n",
      "iteration 934 / 1500: loss 2.160643, accuracy 0.331000\n",
      "iteration 935 / 1500: loss 2.168408, accuracy 0.308000\n",
      "iteration 936 / 1500: loss 2.139383, accuracy 0.348000\n",
      "iteration 937 / 1500: loss 2.137416, accuracy 0.333000\n",
      "iteration 938 / 1500: loss 2.142240, accuracy 0.325000\n",
      "iteration 939 / 1500: loss 2.132915, accuracy 0.340000\n",
      "iteration 940 / 1500: loss 2.142657, accuracy 0.333000\n",
      "iteration 941 / 1500: loss 2.103097, accuracy 0.352000\n",
      "iteration 942 / 1500: loss 2.123512, accuracy 0.341000\n",
      "iteration 943 / 1500: loss 2.138522, accuracy 0.331000\n",
      "iteration 944 / 1500: loss 2.139531, accuracy 0.352000\n",
      "iteration 945 / 1500: loss 2.150802, accuracy 0.336000\n",
      "iteration 946 / 1500: loss 2.139982, accuracy 0.331000\n",
      "iteration 947 / 1500: loss 2.150085, accuracy 0.324000\n",
      "iteration 948 / 1500: loss 2.119772, accuracy 0.343000\n",
      "iteration 949 / 1500: loss 2.133617, accuracy 0.320000\n",
      "iteration 950 / 1500: loss 2.154111, accuracy 0.348000\n",
      "iteration 951 / 1500: loss 2.143946, accuracy 0.335000\n",
      "iteration 952 / 1500: loss 2.130436, accuracy 0.340000\n",
      "iteration 953 / 1500: loss 2.131991, accuracy 0.345000\n",
      "iteration 954 / 1500: loss 2.133496, accuracy 0.340000\n",
      "iteration 955 / 1500: loss 2.101327, accuracy 0.353000\n",
      "iteration 956 / 1500: loss 2.146236, accuracy 0.342000\n",
      "iteration 957 / 1500: loss 2.157096, accuracy 0.315000\n",
      "iteration 958 / 1500: loss 2.141699, accuracy 0.334000\n",
      "iteration 959 / 1500: loss 2.140599, accuracy 0.315000\n",
      "iteration 960 / 1500: loss 2.172870, accuracy 0.290000\n",
      "iteration 961 / 1500: loss 2.124151, accuracy 0.358000\n",
      "iteration 962 / 1500: loss 2.127594, accuracy 0.348000\n",
      "iteration 963 / 1500: loss 2.108725, accuracy 0.336000\n",
      "iteration 964 / 1500: loss 2.136278, accuracy 0.322000\n",
      "iteration 965 / 1500: loss 2.126577, accuracy 0.343000\n",
      "iteration 966 / 1500: loss 2.140283, accuracy 0.317000\n",
      "iteration 967 / 1500: loss 2.138354, accuracy 0.333000\n",
      "iteration 968 / 1500: loss 2.101458, accuracy 0.361000\n",
      "iteration 969 / 1500: loss 2.104203, accuracy 0.321000\n",
      "iteration 970 / 1500: loss 2.123555, accuracy 0.344000\n",
      "iteration 971 / 1500: loss 2.098063, accuracy 0.364000\n",
      "iteration 972 / 1500: loss 2.139127, accuracy 0.312000\n",
      "iteration 973 / 1500: loss 2.124506, accuracy 0.329000\n",
      "iteration 974 / 1500: loss 2.115658, accuracy 0.349000\n",
      "iteration 975 / 1500: loss 2.094540, accuracy 0.317000\n",
      "iteration 976 / 1500: loss 2.098765, accuracy 0.351000\n",
      "iteration 977 / 1500: loss 2.120091, accuracy 0.328000\n",
      "iteration 978 / 1500: loss 2.113542, accuracy 0.329000\n",
      "iteration 979 / 1500: loss 2.103277, accuracy 0.337000\n",
      "iteration 980 / 1500: loss 2.107253, accuracy 0.336000\n",
      "iteration 981 / 1500: loss 2.118034, accuracy 0.335000\n",
      "iteration 982 / 1500: loss 2.125530, accuracy 0.347000\n",
      "iteration 983 / 1500: loss 2.116034, accuracy 0.345000\n",
      "iteration 984 / 1500: loss 2.092502, accuracy 0.358000\n",
      "iteration 985 / 1500: loss 2.086913, accuracy 0.354000\n",
      "iteration 986 / 1500: loss 2.111762, accuracy 0.327000\n",
      "iteration 987 / 1500: loss 2.086513, accuracy 0.333000\n",
      "iteration 988 / 1500: loss 2.096964, accuracy 0.336000\n",
      "iteration 989 / 1500: loss 2.109529, accuracy 0.345000\n",
      "iteration 990 / 1500: loss 2.116968, accuracy 0.339000\n",
      "iteration 991 / 1500: loss 2.136909, accuracy 0.306000\n",
      "iteration 992 / 1500: loss 2.095845, accuracy 0.353000\n",
      "iteration 993 / 1500: loss 2.091395, accuracy 0.342000\n",
      "iteration 994 / 1500: loss 2.114419, accuracy 0.322000\n",
      "iteration 995 / 1500: loss 2.095964, accuracy 0.354000\n",
      "iteration 996 / 1500: loss 2.088829, accuracy 0.319000\n",
      "iteration 997 / 1500: loss 2.113456, accuracy 0.338000\n",
      "iteration 998 / 1500: loss 2.111784, accuracy 0.343000\n",
      "iteration 999 / 1500: loss 2.127998, accuracy 0.311000\n",
      "iteration 1000 / 1500: loss 2.103456, accuracy 0.329000\n",
      "iteration 1001 / 1500: loss 2.071727, accuracy 0.367000\n",
      "iteration 1002 / 1500: loss 2.103989, accuracy 0.336000\n",
      "iteration 1003 / 1500: loss 2.110891, accuracy 0.370000\n",
      "iteration 1004 / 1500: loss 2.100869, accuracy 0.337000\n",
      "iteration 1005 / 1500: loss 2.120639, accuracy 0.310000\n",
      "iteration 1006 / 1500: loss 2.115057, accuracy 0.319000\n",
      "iteration 1007 / 1500: loss 2.086541, accuracy 0.341000\n",
      "iteration 1008 / 1500: loss 2.090642, accuracy 0.347000\n",
      "iteration 1009 / 1500: loss 2.075807, accuracy 0.349000\n",
      "iteration 1010 / 1500: loss 2.116586, accuracy 0.325000\n",
      "iteration 1011 / 1500: loss 2.100307, accuracy 0.313000\n",
      "iteration 1012 / 1500: loss 2.084645, accuracy 0.344000\n",
      "iteration 1013 / 1500: loss 2.098503, accuracy 0.334000\n",
      "iteration 1014 / 1500: loss 2.108845, accuracy 0.334000\n",
      "iteration 1015 / 1500: loss 2.096177, accuracy 0.361000\n",
      "iteration 1016 / 1500: loss 2.108256, accuracy 0.338000\n",
      "iteration 1017 / 1500: loss 2.100267, accuracy 0.340000\n",
      "iteration 1018 / 1500: loss 2.112772, accuracy 0.337000\n",
      "iteration 1019 / 1500: loss 2.098518, accuracy 0.327000\n",
      "iteration 1020 / 1500: loss 2.082653, accuracy 0.345000\n",
      "iteration 1021 / 1500: loss 2.084571, accuracy 0.338000\n",
      "iteration 1022 / 1500: loss 2.090644, accuracy 0.335000\n",
      "iteration 1023 / 1500: loss 2.073781, accuracy 0.339000\n",
      "iteration 1024 / 1500: loss 2.101558, accuracy 0.329000\n",
      "iteration 1025 / 1500: loss 2.089775, accuracy 0.352000\n",
      "iteration 1026 / 1500: loss 2.069519, accuracy 0.361000\n",
      "iteration 1027 / 1500: loss 2.058091, accuracy 0.352000\n",
      "iteration 1028 / 1500: loss 2.060801, accuracy 0.360000\n",
      "iteration 1029 / 1500: loss 2.085802, accuracy 0.346000\n",
      "iteration 1030 / 1500: loss 2.128964, accuracy 0.313000\n",
      "iteration 1031 / 1500: loss 2.065257, accuracy 0.359000\n",
      "iteration 1032 / 1500: loss 2.098372, accuracy 0.347000\n",
      "iteration 1033 / 1500: loss 2.109273, accuracy 0.323000\n",
      "iteration 1034 / 1500: loss 2.080004, accuracy 0.341000\n",
      "iteration 1035 / 1500: loss 2.090555, accuracy 0.330000\n",
      "iteration 1036 / 1500: loss 2.090420, accuracy 0.338000\n",
      "iteration 1037 / 1500: loss 2.101232, accuracy 0.341000\n",
      "iteration 1038 / 1500: loss 2.099039, accuracy 0.337000\n",
      "iteration 1039 / 1500: loss 2.077657, accuracy 0.345000\n",
      "iteration 1040 / 1500: loss 2.098270, accuracy 0.316000\n",
      "iteration 1041 / 1500: loss 2.082025, accuracy 0.343000\n",
      "iteration 1042 / 1500: loss 2.082621, accuracy 0.351000\n",
      "iteration 1043 / 1500: loss 2.088649, accuracy 0.337000\n",
      "iteration 1044 / 1500: loss 2.060637, accuracy 0.365000\n",
      "iteration 1045 / 1500: loss 2.126768, accuracy 0.321000\n",
      "iteration 1046 / 1500: loss 2.103973, accuracy 0.324000\n",
      "iteration 1047 / 1500: loss 2.076447, accuracy 0.337000\n",
      "iteration 1048 / 1500: loss 2.090200, accuracy 0.347000\n",
      "iteration 1049 / 1500: loss 2.117958, accuracy 0.310000\n",
      "iteration 1050 / 1500: loss 2.059394, accuracy 0.369000\n",
      "iteration 1051 / 1500: loss 2.095755, accuracy 0.335000\n",
      "iteration 1052 / 1500: loss 2.075944, accuracy 0.343000\n",
      "iteration 1053 / 1500: loss 2.102326, accuracy 0.326000\n",
      "iteration 1054 / 1500: loss 2.090708, accuracy 0.350000\n",
      "iteration 1055 / 1500: loss 2.087899, accuracy 0.331000\n",
      "iteration 1056 / 1500: loss 2.113606, accuracy 0.308000\n",
      "iteration 1057 / 1500: loss 2.088406, accuracy 0.329000\n",
      "iteration 1058 / 1500: loss 2.109312, accuracy 0.331000\n",
      "iteration 1059 / 1500: loss 2.071493, accuracy 0.360000\n",
      "iteration 1060 / 1500: loss 2.078662, accuracy 0.354000\n",
      "iteration 1061 / 1500: loss 2.097885, accuracy 0.324000\n",
      "iteration 1062 / 1500: loss 2.109989, accuracy 0.319000\n",
      "iteration 1063 / 1500: loss 2.077323, accuracy 0.316000\n",
      "iteration 1064 / 1500: loss 2.089717, accuracy 0.329000\n",
      "iteration 1065 / 1500: loss 2.075886, accuracy 0.344000\n",
      "iteration 1066 / 1500: loss 2.087253, accuracy 0.321000\n",
      "iteration 1067 / 1500: loss 2.068960, accuracy 0.355000\n",
      "iteration 1068 / 1500: loss 2.090180, accuracy 0.347000\n",
      "iteration 1069 / 1500: loss 2.074785, accuracy 0.359000\n",
      "iteration 1070 / 1500: loss 2.109908, accuracy 0.332000\n",
      "iteration 1071 / 1500: loss 2.083711, accuracy 0.344000\n",
      "iteration 1072 / 1500: loss 2.084012, accuracy 0.338000\n",
      "iteration 1073 / 1500: loss 2.089618, accuracy 0.332000\n",
      "iteration 1074 / 1500: loss 2.085148, accuracy 0.343000\n",
      "iteration 1075 / 1500: loss 2.104754, accuracy 0.334000\n",
      "iteration 1076 / 1500: loss 2.080133, accuracy 0.352000\n",
      "iteration 1077 / 1500: loss 2.076806, accuracy 0.330000\n",
      "iteration 1078 / 1500: loss 2.096093, accuracy 0.353000\n",
      "iteration 1079 / 1500: loss 2.066674, accuracy 0.361000\n",
      "iteration 1080 / 1500: loss 2.089418, accuracy 0.323000\n",
      "iteration 1081 / 1500: loss 2.089033, accuracy 0.342000\n",
      "iteration 1082 / 1500: loss 2.090834, accuracy 0.357000\n",
      "iteration 1083 / 1500: loss 2.069196, accuracy 0.357000\n",
      "iteration 1084 / 1500: loss 2.083970, accuracy 0.324000\n",
      "iteration 1085 / 1500: loss 2.059010, accuracy 0.343000\n",
      "iteration 1086 / 1500: loss 2.069815, accuracy 0.352000\n",
      "iteration 1087 / 1500: loss 2.070945, accuracy 0.350000\n",
      "iteration 1088 / 1500: loss 2.097243, accuracy 0.315000\n",
      "iteration 1089 / 1500: loss 2.088512, accuracy 0.325000\n",
      "iteration 1090 / 1500: loss 2.089365, accuracy 0.326000\n",
      "iteration 1091 / 1500: loss 2.070983, accuracy 0.347000\n",
      "iteration 1092 / 1500: loss 2.076536, accuracy 0.335000\n",
      "iteration 1093 / 1500: loss 2.079341, accuracy 0.342000\n",
      "iteration 1094 / 1500: loss 2.073290, accuracy 0.343000\n",
      "iteration 1095 / 1500: loss 2.068910, accuracy 0.340000\n",
      "iteration 1096 / 1500: loss 2.082536, accuracy 0.332000\n",
      "iteration 1097 / 1500: loss 2.070168, accuracy 0.363000\n",
      "iteration 1098 / 1500: loss 2.083926, accuracy 0.341000\n",
      "iteration 1099 / 1500: loss 2.061724, accuracy 0.365000\n",
      "iteration 1100 / 1500: loss 2.047488, accuracy 0.338000\n",
      "iteration 1101 / 1500: loss 2.075704, accuracy 0.332000\n",
      "iteration 1102 / 1500: loss 2.072618, accuracy 0.353000\n",
      "iteration 1103 / 1500: loss 2.070159, accuracy 0.346000\n",
      "iteration 1104 / 1500: loss 2.087532, accuracy 0.323000\n",
      "iteration 1105 / 1500: loss 2.074821, accuracy 0.340000\n",
      "iteration 1106 / 1500: loss 2.081216, accuracy 0.314000\n",
      "iteration 1107 / 1500: loss 2.070683, accuracy 0.344000\n",
      "iteration 1108 / 1500: loss 2.070406, accuracy 0.334000\n",
      "iteration 1109 / 1500: loss 2.075183, accuracy 0.332000\n",
      "iteration 1110 / 1500: loss 2.126017, accuracy 0.314000\n",
      "iteration 1111 / 1500: loss 2.063223, accuracy 0.375000\n",
      "iteration 1112 / 1500: loss 2.067340, accuracy 0.313000\n",
      "iteration 1113 / 1500: loss 2.065429, accuracy 0.346000\n",
      "iteration 1114 / 1500: loss 2.077613, accuracy 0.333000\n",
      "iteration 1115 / 1500: loss 2.076750, accuracy 0.362000\n",
      "iteration 1116 / 1500: loss 2.083759, accuracy 0.348000\n",
      "iteration 1117 / 1500: loss 2.076736, accuracy 0.338000\n",
      "iteration 1118 / 1500: loss 2.054054, accuracy 0.347000\n",
      "iteration 1119 / 1500: loss 2.096172, accuracy 0.322000\n",
      "iteration 1120 / 1500: loss 2.101264, accuracy 0.326000\n",
      "iteration 1121 / 1500: loss 2.089216, accuracy 0.327000\n",
      "iteration 1122 / 1500: loss 2.107707, accuracy 0.303000\n",
      "iteration 1123 / 1500: loss 2.076091, accuracy 0.317000\n",
      "iteration 1124 / 1500: loss 2.087962, accuracy 0.343000\n",
      "iteration 1125 / 1500: loss 2.073354, accuracy 0.322000\n",
      "iteration 1126 / 1500: loss 2.063655, accuracy 0.353000\n",
      "iteration 1127 / 1500: loss 2.058831, accuracy 0.346000\n",
      "iteration 1128 / 1500: loss 2.073974, accuracy 0.321000\n",
      "iteration 1129 / 1500: loss 2.110026, accuracy 0.315000\n",
      "iteration 1130 / 1500: loss 2.061824, accuracy 0.352000\n",
      "iteration 1131 / 1500: loss 2.063826, accuracy 0.330000\n",
      "iteration 1132 / 1500: loss 2.075074, accuracy 0.349000\n",
      "iteration 1133 / 1500: loss 2.071310, accuracy 0.335000\n",
      "iteration 1134 / 1500: loss 2.075877, accuracy 0.317000\n",
      "iteration 1135 / 1500: loss 2.076519, accuracy 0.317000\n",
      "iteration 1136 / 1500: loss 2.097237, accuracy 0.298000\n",
      "iteration 1137 / 1500: loss 2.103337, accuracy 0.310000\n",
      "iteration 1138 / 1500: loss 2.087089, accuracy 0.339000\n",
      "iteration 1139 / 1500: loss 2.072421, accuracy 0.363000\n",
      "iteration 1140 / 1500: loss 2.049122, accuracy 0.364000\n",
      "iteration 1141 / 1500: loss 2.073602, accuracy 0.347000\n",
      "iteration 1142 / 1500: loss 2.076653, accuracy 0.321000\n",
      "iteration 1143 / 1500: loss 2.045626, accuracy 0.365000\n",
      "iteration 1144 / 1500: loss 2.064032, accuracy 0.357000\n",
      "iteration 1145 / 1500: loss 2.092481, accuracy 0.320000\n",
      "iteration 1146 / 1500: loss 2.077531, accuracy 0.334000\n",
      "iteration 1147 / 1500: loss 2.052555, accuracy 0.369000\n",
      "iteration 1148 / 1500: loss 2.047230, accuracy 0.370000\n",
      "iteration 1149 / 1500: loss 2.066377, accuracy 0.346000\n",
      "iteration 1150 / 1500: loss 2.091396, accuracy 0.327000\n",
      "iteration 1151 / 1500: loss 2.028875, accuracy 0.352000\n",
      "iteration 1152 / 1500: loss 2.066967, accuracy 0.340000\n",
      "iteration 1153 / 1500: loss 2.054560, accuracy 0.349000\n",
      "iteration 1154 / 1500: loss 2.082710, accuracy 0.341000\n",
      "iteration 1155 / 1500: loss 2.085104, accuracy 0.311000\n",
      "iteration 1156 / 1500: loss 2.069674, accuracy 0.330000\n",
      "iteration 1157 / 1500: loss 2.086801, accuracy 0.312000\n",
      "iteration 1158 / 1500: loss 2.055100, accuracy 0.350000\n",
      "iteration 1159 / 1500: loss 2.077488, accuracy 0.324000\n",
      "iteration 1160 / 1500: loss 2.074073, accuracy 0.322000\n",
      "iteration 1161 / 1500: loss 2.080301, accuracy 0.333000\n",
      "iteration 1162 / 1500: loss 2.057705, accuracy 0.366000\n",
      "iteration 1163 / 1500: loss 2.071706, accuracy 0.319000\n",
      "iteration 1164 / 1500: loss 2.084308, accuracy 0.321000\n",
      "iteration 1165 / 1500: loss 2.070283, accuracy 0.334000\n",
      "iteration 1166 / 1500: loss 2.084941, accuracy 0.330000\n",
      "iteration 1167 / 1500: loss 2.071201, accuracy 0.340000\n",
      "iteration 1168 / 1500: loss 2.049721, accuracy 0.363000\n",
      "iteration 1169 / 1500: loss 2.059927, accuracy 0.336000\n",
      "iteration 1170 / 1500: loss 2.056479, accuracy 0.331000\n",
      "iteration 1171 / 1500: loss 2.062062, accuracy 0.371000\n",
      "iteration 1172 / 1500: loss 2.059091, accuracy 0.343000\n",
      "iteration 1173 / 1500: loss 2.079776, accuracy 0.331000\n",
      "iteration 1174 / 1500: loss 2.065454, accuracy 0.333000\n",
      "iteration 1175 / 1500: loss 2.065836, accuracy 0.345000\n",
      "iteration 1176 / 1500: loss 2.055237, accuracy 0.350000\n",
      "iteration 1177 / 1500: loss 2.091050, accuracy 0.335000\n",
      "iteration 1178 / 1500: loss 2.074205, accuracy 0.315000\n",
      "iteration 1179 / 1500: loss 2.088991, accuracy 0.345000\n",
      "iteration 1180 / 1500: loss 2.098846, accuracy 0.348000\n",
      "iteration 1181 / 1500: loss 2.057723, accuracy 0.352000\n",
      "iteration 1182 / 1500: loss 2.094254, accuracy 0.329000\n",
      "iteration 1183 / 1500: loss 2.080488, accuracy 0.310000\n",
      "iteration 1184 / 1500: loss 2.101052, accuracy 0.314000\n",
      "iteration 1185 / 1500: loss 2.071712, accuracy 0.319000\n",
      "iteration 1186 / 1500: loss 2.079239, accuracy 0.323000\n",
      "iteration 1187 / 1500: loss 2.069281, accuracy 0.341000\n",
      "iteration 1188 / 1500: loss 2.078427, accuracy 0.338000\n",
      "iteration 1189 / 1500: loss 2.096420, accuracy 0.311000\n",
      "iteration 1190 / 1500: loss 2.063347, accuracy 0.320000\n",
      "iteration 1191 / 1500: loss 2.086597, accuracy 0.333000\n",
      "iteration 1192 / 1500: loss 2.086176, accuracy 0.329000\n",
      "iteration 1193 / 1500: loss 2.077824, accuracy 0.327000\n",
      "iteration 1194 / 1500: loss 2.077209, accuracy 0.310000\n",
      "iteration 1195 / 1500: loss 2.070178, accuracy 0.320000\n",
      "iteration 1196 / 1500: loss 2.069465, accuracy 0.332000\n",
      "iteration 1197 / 1500: loss 2.081962, accuracy 0.332000\n",
      "iteration 1198 / 1500: loss 2.069883, accuracy 0.344000\n",
      "iteration 1199 / 1500: loss 2.068791, accuracy 0.346000\n",
      "iteration 1200 / 1500: loss 2.096275, accuracy 0.312000\n",
      "iteration 1201 / 1500: loss 2.079607, accuracy 0.320000\n",
      "iteration 1202 / 1500: loss 2.107692, accuracy 0.291000\n",
      "iteration 1203 / 1500: loss 2.063214, accuracy 0.333000\n",
      "iteration 1204 / 1500: loss 2.052357, accuracy 0.370000\n",
      "iteration 1205 / 1500: loss 2.063354, accuracy 0.348000\n",
      "iteration 1206 / 1500: loss 2.081619, accuracy 0.343000\n",
      "iteration 1207 / 1500: loss 2.073344, accuracy 0.340000\n",
      "iteration 1208 / 1500: loss 2.068176, accuracy 0.319000\n",
      "iteration 1209 / 1500: loss 2.112223, accuracy 0.294000\n",
      "iteration 1210 / 1500: loss 2.042870, accuracy 0.351000\n",
      "iteration 1211 / 1500: loss 2.077188, accuracy 0.316000\n",
      "iteration 1212 / 1500: loss 2.073338, accuracy 0.342000\n",
      "iteration 1213 / 1500: loss 2.060810, accuracy 0.339000\n",
      "iteration 1214 / 1500: loss 2.065002, accuracy 0.327000\n",
      "iteration 1215 / 1500: loss 2.033331, accuracy 0.368000\n",
      "iteration 1216 / 1500: loss 2.044473, accuracy 0.375000\n",
      "iteration 1217 / 1500: loss 2.071926, accuracy 0.344000\n",
      "iteration 1218 / 1500: loss 2.077106, accuracy 0.336000\n",
      "iteration 1219 / 1500: loss 2.068361, accuracy 0.320000\n",
      "iteration 1220 / 1500: loss 2.058426, accuracy 0.344000\n",
      "iteration 1221 / 1500: loss 2.062738, accuracy 0.337000\n",
      "iteration 1222 / 1500: loss 2.083974, accuracy 0.315000\n",
      "iteration 1223 / 1500: loss 2.054102, accuracy 0.342000\n",
      "iteration 1224 / 1500: loss 2.086747, accuracy 0.321000\n",
      "iteration 1225 / 1500: loss 2.071974, accuracy 0.345000\n",
      "iteration 1226 / 1500: loss 2.061860, accuracy 0.348000\n",
      "iteration 1227 / 1500: loss 2.091033, accuracy 0.324000\n",
      "iteration 1228 / 1500: loss 2.043353, accuracy 0.367000\n",
      "iteration 1229 / 1500: loss 2.079006, accuracy 0.335000\n",
      "iteration 1230 / 1500: loss 2.059235, accuracy 0.340000\n",
      "iteration 1231 / 1500: loss 2.077243, accuracy 0.334000\n",
      "iteration 1232 / 1500: loss 2.090444, accuracy 0.345000\n",
      "iteration 1233 / 1500: loss 2.070159, accuracy 0.336000\n",
      "iteration 1234 / 1500: loss 2.065737, accuracy 0.324000\n",
      "iteration 1235 / 1500: loss 2.038965, accuracy 0.334000\n",
      "iteration 1236 / 1500: loss 2.066248, accuracy 0.353000\n",
      "iteration 1237 / 1500: loss 2.025180, accuracy 0.371000\n",
      "iteration 1238 / 1500: loss 2.066953, accuracy 0.344000\n",
      "iteration 1239 / 1500: loss 2.082287, accuracy 0.324000\n",
      "iteration 1240 / 1500: loss 2.054518, accuracy 0.359000\n",
      "iteration 1241 / 1500: loss 2.045673, accuracy 0.345000\n",
      "iteration 1242 / 1500: loss 2.066761, accuracy 0.335000\n",
      "iteration 1243 / 1500: loss 2.053680, accuracy 0.351000\n",
      "iteration 1244 / 1500: loss 2.092930, accuracy 0.331000\n",
      "iteration 1245 / 1500: loss 2.046412, accuracy 0.333000\n",
      "iteration 1246 / 1500: loss 2.041460, accuracy 0.350000\n",
      "iteration 1247 / 1500: loss 2.053038, accuracy 0.333000\n",
      "iteration 1248 / 1500: loss 2.053668, accuracy 0.337000\n",
      "iteration 1249 / 1500: loss 2.050709, accuracy 0.351000\n",
      "iteration 1250 / 1500: loss 2.045789, accuracy 0.349000\n",
      "iteration 1251 / 1500: loss 2.069084, accuracy 0.331000\n",
      "iteration 1252 / 1500: loss 2.055388, accuracy 0.343000\n",
      "iteration 1253 / 1500: loss 2.063766, accuracy 0.340000\n",
      "iteration 1254 / 1500: loss 2.053422, accuracy 0.347000\n",
      "iteration 1255 / 1500: loss 2.059472, accuracy 0.328000\n",
      "iteration 1256 / 1500: loss 2.040918, accuracy 0.367000\n",
      "iteration 1257 / 1500: loss 2.034960, accuracy 0.342000\n",
      "iteration 1258 / 1500: loss 2.045590, accuracy 0.330000\n",
      "iteration 1259 / 1500: loss 2.084074, accuracy 0.327000\n",
      "iteration 1260 / 1500: loss 2.068094, accuracy 0.320000\n",
      "iteration 1261 / 1500: loss 2.045530, accuracy 0.346000\n",
      "iteration 1262 / 1500: loss 2.054668, accuracy 0.349000\n",
      "iteration 1263 / 1500: loss 2.061703, accuracy 0.344000\n",
      "iteration 1264 / 1500: loss 2.072576, accuracy 0.344000\n",
      "iteration 1265 / 1500: loss 2.060186, accuracy 0.330000\n",
      "iteration 1266 / 1500: loss 2.064721, accuracy 0.348000\n",
      "iteration 1267 / 1500: loss 2.074589, accuracy 0.326000\n",
      "iteration 1268 / 1500: loss 2.097692, accuracy 0.317000\n",
      "iteration 1269 / 1500: loss 2.047417, accuracy 0.338000\n",
      "iteration 1270 / 1500: loss 2.070585, accuracy 0.330000\n",
      "iteration 1271 / 1500: loss 2.062995, accuracy 0.349000\n",
      "iteration 1272 / 1500: loss 2.055581, accuracy 0.332000\n",
      "iteration 1273 / 1500: loss 2.077424, accuracy 0.347000\n",
      "iteration 1274 / 1500: loss 2.069398, accuracy 0.340000\n",
      "iteration 1275 / 1500: loss 2.079017, accuracy 0.314000\n",
      "iteration 1276 / 1500: loss 2.043941, accuracy 0.354000\n",
      "iteration 1277 / 1500: loss 2.048990, accuracy 0.374000\n",
      "iteration 1278 / 1500: loss 2.049875, accuracy 0.342000\n",
      "iteration 1279 / 1500: loss 2.081037, accuracy 0.332000\n",
      "iteration 1280 / 1500: loss 2.075713, accuracy 0.330000\n",
      "iteration 1281 / 1500: loss 2.060857, accuracy 0.370000\n",
      "iteration 1282 / 1500: loss 2.056893, accuracy 0.335000\n",
      "iteration 1283 / 1500: loss 2.044547, accuracy 0.347000\n",
      "iteration 1284 / 1500: loss 2.046229, accuracy 0.338000\n",
      "iteration 1285 / 1500: loss 2.097790, accuracy 0.315000\n",
      "iteration 1286 / 1500: loss 2.061110, accuracy 0.353000\n",
      "iteration 1287 / 1500: loss 2.078869, accuracy 0.320000\n",
      "iteration 1288 / 1500: loss 2.068307, accuracy 0.331000\n",
      "iteration 1289 / 1500: loss 2.070168, accuracy 0.338000\n",
      "iteration 1290 / 1500: loss 2.045008, accuracy 0.352000\n",
      "iteration 1291 / 1500: loss 2.067229, accuracy 0.331000\n",
      "iteration 1292 / 1500: loss 2.059761, accuracy 0.348000\n",
      "iteration 1293 / 1500: loss 2.040467, accuracy 0.346000\n",
      "iteration 1294 / 1500: loss 2.044314, accuracy 0.368000\n",
      "iteration 1295 / 1500: loss 2.082510, accuracy 0.331000\n",
      "iteration 1296 / 1500: loss 2.073136, accuracy 0.321000\n",
      "iteration 1297 / 1500: loss 2.042534, accuracy 0.327000\n",
      "iteration 1298 / 1500: loss 2.039309, accuracy 0.343000\n",
      "iteration 1299 / 1500: loss 2.075897, accuracy 0.329000\n",
      "iteration 1300 / 1500: loss 2.066793, accuracy 0.344000\n",
      "iteration 1301 / 1500: loss 2.072225, accuracy 0.331000\n",
      "iteration 1302 / 1500: loss 2.049129, accuracy 0.351000\n",
      "iteration 1303 / 1500: loss 2.060761, accuracy 0.333000\n",
      "iteration 1304 / 1500: loss 2.042029, accuracy 0.347000\n",
      "iteration 1305 / 1500: loss 2.060254, accuracy 0.347000\n",
      "iteration 1306 / 1500: loss 2.055604, accuracy 0.338000\n",
      "iteration 1307 / 1500: loss 2.056857, accuracy 0.334000\n",
      "iteration 1308 / 1500: loss 2.068702, accuracy 0.325000\n",
      "iteration 1309 / 1500: loss 2.055585, accuracy 0.357000\n",
      "iteration 1310 / 1500: loss 2.068225, accuracy 0.334000\n",
      "iteration 1311 / 1500: loss 2.073646, accuracy 0.318000\n",
      "iteration 1312 / 1500: loss 2.057441, accuracy 0.331000\n",
      "iteration 1313 / 1500: loss 2.076304, accuracy 0.339000\n",
      "iteration 1314 / 1500: loss 2.057142, accuracy 0.326000\n",
      "iteration 1315 / 1500: loss 2.064006, accuracy 0.328000\n",
      "iteration 1316 / 1500: loss 2.071955, accuracy 0.328000\n",
      "iteration 1317 / 1500: loss 2.077527, accuracy 0.326000\n",
      "iteration 1318 / 1500: loss 2.064625, accuracy 0.334000\n",
      "iteration 1319 / 1500: loss 2.054092, accuracy 0.340000\n",
      "iteration 1320 / 1500: loss 2.077519, accuracy 0.320000\n",
      "iteration 1321 / 1500: loss 2.068337, accuracy 0.333000\n",
      "iteration 1322 / 1500: loss 2.066032, accuracy 0.327000\n",
      "iteration 1323 / 1500: loss 2.059983, accuracy 0.338000\n",
      "iteration 1324 / 1500: loss 2.042259, accuracy 0.349000\n",
      "iteration 1325 / 1500: loss 2.039264, accuracy 0.340000\n",
      "iteration 1326 / 1500: loss 2.037639, accuracy 0.355000\n",
      "iteration 1327 / 1500: loss 2.037035, accuracy 0.351000\n",
      "iteration 1328 / 1500: loss 2.056048, accuracy 0.353000\n",
      "iteration 1329 / 1500: loss 2.086849, accuracy 0.323000\n",
      "iteration 1330 / 1500: loss 2.056494, accuracy 0.348000\n",
      "iteration 1331 / 1500: loss 2.067737, accuracy 0.317000\n",
      "iteration 1332 / 1500: loss 2.059440, accuracy 0.353000\n",
      "iteration 1333 / 1500: loss 2.043500, accuracy 0.354000\n",
      "iteration 1334 / 1500: loss 2.071054, accuracy 0.324000\n",
      "iteration 1335 / 1500: loss 2.021788, accuracy 0.400000\n",
      "iteration 1336 / 1500: loss 2.077155, accuracy 0.311000\n",
      "iteration 1337 / 1500: loss 2.037111, accuracy 0.348000\n",
      "iteration 1338 / 1500: loss 2.059453, accuracy 0.343000\n",
      "iteration 1339 / 1500: loss 2.077371, accuracy 0.323000\n",
      "iteration 1340 / 1500: loss 2.084839, accuracy 0.326000\n",
      "iteration 1341 / 1500: loss 2.048365, accuracy 0.332000\n",
      "iteration 1342 / 1500: loss 2.041364, accuracy 0.360000\n",
      "iteration 1343 / 1500: loss 2.056690, accuracy 0.330000\n",
      "iteration 1344 / 1500: loss 2.050256, accuracy 0.335000\n",
      "iteration 1345 / 1500: loss 2.069098, accuracy 0.333000\n",
      "iteration 1346 / 1500: loss 2.039171, accuracy 0.359000\n",
      "iteration 1347 / 1500: loss 2.061390, accuracy 0.310000\n",
      "iteration 1348 / 1500: loss 2.066966, accuracy 0.328000\n",
      "iteration 1349 / 1500: loss 2.074281, accuracy 0.326000\n",
      "iteration 1350 / 1500: loss 2.047960, accuracy 0.367000\n",
      "iteration 1351 / 1500: loss 2.049596, accuracy 0.351000\n",
      "iteration 1352 / 1500: loss 2.096633, accuracy 0.291000\n",
      "iteration 1353 / 1500: loss 2.023392, accuracy 0.379000\n",
      "iteration 1354 / 1500: loss 2.061608, accuracy 0.340000\n",
      "iteration 1355 / 1500: loss 2.070556, accuracy 0.328000\n",
      "iteration 1356 / 1500: loss 2.047705, accuracy 0.339000\n",
      "iteration 1357 / 1500: loss 2.088123, accuracy 0.335000\n",
      "iteration 1358 / 1500: loss 2.057342, accuracy 0.339000\n",
      "iteration 1359 / 1500: loss 2.076081, accuracy 0.323000\n",
      "iteration 1360 / 1500: loss 2.061265, accuracy 0.337000\n",
      "iteration 1361 / 1500: loss 2.052146, accuracy 0.334000\n",
      "iteration 1362 / 1500: loss 2.080541, accuracy 0.345000\n",
      "iteration 1363 / 1500: loss 2.100551, accuracy 0.321000\n",
      "iteration 1364 / 1500: loss 2.052394, accuracy 0.354000\n",
      "iteration 1365 / 1500: loss 2.073703, accuracy 0.331000\n",
      "iteration 1366 / 1500: loss 2.039749, accuracy 0.355000\n",
      "iteration 1367 / 1500: loss 2.062942, accuracy 0.315000\n",
      "iteration 1368 / 1500: loss 2.035901, accuracy 0.359000\n",
      "iteration 1369 / 1500: loss 2.076059, accuracy 0.314000\n",
      "iteration 1370 / 1500: loss 2.043387, accuracy 0.333000\n",
      "iteration 1371 / 1500: loss 2.086322, accuracy 0.323000\n",
      "iteration 1372 / 1500: loss 2.058672, accuracy 0.347000\n",
      "iteration 1373 / 1500: loss 2.059501, accuracy 0.336000\n",
      "iteration 1374 / 1500: loss 2.081018, accuracy 0.311000\n",
      "iteration 1375 / 1500: loss 2.066708, accuracy 0.329000\n",
      "iteration 1376 / 1500: loss 2.052023, accuracy 0.343000\n",
      "iteration 1377 / 1500: loss 2.052708, accuracy 0.351000\n",
      "iteration 1378 / 1500: loss 2.039594, accuracy 0.348000\n",
      "iteration 1379 / 1500: loss 2.054299, accuracy 0.326000\n",
      "iteration 1380 / 1500: loss 2.078261, accuracy 0.348000\n",
      "iteration 1381 / 1500: loss 2.063471, accuracy 0.357000\n",
      "iteration 1382 / 1500: loss 2.055803, accuracy 0.352000\n",
      "iteration 1383 / 1500: loss 2.053733, accuracy 0.335000\n",
      "iteration 1384 / 1500: loss 2.046206, accuracy 0.346000\n",
      "iteration 1385 / 1500: loss 2.084895, accuracy 0.325000\n",
      "iteration 1386 / 1500: loss 2.059838, accuracy 0.333000\n",
      "iteration 1387 / 1500: loss 2.071205, accuracy 0.331000\n",
      "iteration 1388 / 1500: loss 2.061662, accuracy 0.325000\n",
      "iteration 1389 / 1500: loss 2.059804, accuracy 0.346000\n",
      "iteration 1390 / 1500: loss 2.052071, accuracy 0.364000\n",
      "iteration 1391 / 1500: loss 2.065722, accuracy 0.339000\n",
      "iteration 1392 / 1500: loss 2.074481, accuracy 0.318000\n",
      "iteration 1393 / 1500: loss 2.056433, accuracy 0.333000\n",
      "iteration 1394 / 1500: loss 2.069832, accuracy 0.351000\n",
      "iteration 1395 / 1500: loss 2.050227, accuracy 0.350000\n",
      "iteration 1396 / 1500: loss 2.082842, accuracy 0.328000\n",
      "iteration 1397 / 1500: loss 2.032486, accuracy 0.354000\n",
      "iteration 1398 / 1500: loss 2.049079, accuracy 0.370000\n",
      "iteration 1399 / 1500: loss 2.050942, accuracy 0.345000\n",
      "iteration 1400 / 1500: loss 2.036167, accuracy 0.360000\n",
      "iteration 1401 / 1500: loss 2.043841, accuracy 0.348000\n",
      "iteration 1402 / 1500: loss 2.050492, accuracy 0.352000\n",
      "iteration 1403 / 1500: loss 2.084244, accuracy 0.322000\n",
      "iteration 1404 / 1500: loss 2.070095, accuracy 0.339000\n",
      "iteration 1405 / 1500: loss 2.063947, accuracy 0.350000\n",
      "iteration 1406 / 1500: loss 2.042045, accuracy 0.337000\n",
      "iteration 1407 / 1500: loss 2.075733, accuracy 0.323000\n",
      "iteration 1408 / 1500: loss 2.084780, accuracy 0.311000\n",
      "iteration 1409 / 1500: loss 2.043439, accuracy 0.367000\n",
      "iteration 1410 / 1500: loss 2.066293, accuracy 0.322000\n",
      "iteration 1411 / 1500: loss 2.065610, accuracy 0.325000\n",
      "iteration 1412 / 1500: loss 2.035900, accuracy 0.360000\n",
      "iteration 1413 / 1500: loss 2.073217, accuracy 0.346000\n",
      "iteration 1414 / 1500: loss 2.066756, accuracy 0.332000\n",
      "iteration 1415 / 1500: loss 2.049405, accuracy 0.337000\n",
      "iteration 1416 / 1500: loss 2.103644, accuracy 0.335000\n",
      "iteration 1417 / 1500: loss 2.056469, accuracy 0.348000\n",
      "iteration 1418 / 1500: loss 2.053490, accuracy 0.337000\n",
      "iteration 1419 / 1500: loss 2.074825, accuracy 0.324000\n",
      "iteration 1420 / 1500: loss 2.092497, accuracy 0.314000\n",
      "iteration 1421 / 1500: loss 2.033232, accuracy 0.354000\n",
      "iteration 1422 / 1500: loss 2.070928, accuracy 0.338000\n",
      "iteration 1423 / 1500: loss 2.030445, accuracy 0.364000\n",
      "iteration 1424 / 1500: loss 2.036466, accuracy 0.340000\n",
      "iteration 1425 / 1500: loss 2.006416, accuracy 0.368000\n",
      "iteration 1426 / 1500: loss 2.070341, accuracy 0.332000\n",
      "iteration 1427 / 1500: loss 2.055137, accuracy 0.343000\n",
      "iteration 1428 / 1500: loss 2.079794, accuracy 0.319000\n",
      "iteration 1429 / 1500: loss 2.073052, accuracy 0.327000\n",
      "iteration 1430 / 1500: loss 2.060907, accuracy 0.352000\n",
      "iteration 1431 / 1500: loss 2.048316, accuracy 0.349000\n",
      "iteration 1432 / 1500: loss 2.073921, accuracy 0.337000\n",
      "iteration 1433 / 1500: loss 2.078689, accuracy 0.335000\n",
      "iteration 1434 / 1500: loss 2.041994, accuracy 0.344000\n",
      "iteration 1435 / 1500: loss 2.053368, accuracy 0.335000\n",
      "iteration 1436 / 1500: loss 2.059138, accuracy 0.343000\n",
      "iteration 1437 / 1500: loss 2.082132, accuracy 0.325000\n",
      "iteration 1438 / 1500: loss 2.057045, accuracy 0.343000\n",
      "iteration 1439 / 1500: loss 2.022834, accuracy 0.366000\n",
      "iteration 1440 / 1500: loss 2.072946, accuracy 0.359000\n",
      "iteration 1441 / 1500: loss 2.048684, accuracy 0.364000\n",
      "iteration 1442 / 1500: loss 2.090112, accuracy 0.319000\n",
      "iteration 1443 / 1500: loss 2.071043, accuracy 0.338000\n",
      "iteration 1444 / 1500: loss 2.071689, accuracy 0.298000\n",
      "iteration 1445 / 1500: loss 2.080214, accuracy 0.320000\n",
      "iteration 1446 / 1500: loss 2.053199, accuracy 0.338000\n",
      "iteration 1447 / 1500: loss 2.088850, accuracy 0.329000\n",
      "iteration 1448 / 1500: loss 2.060584, accuracy 0.339000\n",
      "iteration 1449 / 1500: loss 2.043112, accuracy 0.343000\n",
      "iteration 1450 / 1500: loss 2.038567, accuracy 0.364000\n",
      "iteration 1451 / 1500: loss 2.062579, accuracy 0.334000\n",
      "iteration 1452 / 1500: loss 2.081332, accuracy 0.326000\n",
      "iteration 1453 / 1500: loss 2.047463, accuracy 0.343000\n",
      "iteration 1454 / 1500: loss 2.035038, accuracy 0.347000\n",
      "iteration 1455 / 1500: loss 2.082963, accuracy 0.334000\n",
      "iteration 1456 / 1500: loss 2.057391, accuracy 0.324000\n",
      "iteration 1457 / 1500: loss 2.057994, accuracy 0.336000\n",
      "iteration 1458 / 1500: loss 2.068582, accuracy 0.332000\n",
      "iteration 1459 / 1500: loss 2.041741, accuracy 0.340000\n",
      "iteration 1460 / 1500: loss 2.057295, accuracy 0.338000\n",
      "iteration 1461 / 1500: loss 2.054632, accuracy 0.339000\n",
      "iteration 1462 / 1500: loss 2.048666, accuracy 0.350000\n",
      "iteration 1463 / 1500: loss 2.096328, accuracy 0.337000\n",
      "iteration 1464 / 1500: loss 2.067735, accuracy 0.325000\n",
      "iteration 1465 / 1500: loss 2.055827, accuracy 0.363000\n",
      "iteration 1466 / 1500: loss 2.072951, accuracy 0.331000\n",
      "iteration 1467 / 1500: loss 2.056469, accuracy 0.355000\n",
      "iteration 1468 / 1500: loss 2.071155, accuracy 0.335000\n",
      "iteration 1469 / 1500: loss 2.027395, accuracy 0.349000\n",
      "iteration 1470 / 1500: loss 2.082225, accuracy 0.323000\n",
      "iteration 1471 / 1500: loss 2.067631, accuracy 0.341000\n",
      "iteration 1472 / 1500: loss 2.090430, accuracy 0.315000\n",
      "iteration 1473 / 1500: loss 2.031626, accuracy 0.341000\n",
      "iteration 1474 / 1500: loss 2.054119, accuracy 0.355000\n",
      "iteration 1475 / 1500: loss 2.053365, accuracy 0.315000\n",
      "iteration 1476 / 1500: loss 2.036380, accuracy 0.332000\n",
      "iteration 1477 / 1500: loss 2.072612, accuracy 0.332000\n",
      "iteration 1478 / 1500: loss 2.049773, accuracy 0.344000\n",
      "iteration 1479 / 1500: loss 2.063575, accuracy 0.325000\n",
      "iteration 1480 / 1500: loss 2.059657, accuracy 0.330000\n",
      "iteration 1481 / 1500: loss 2.082942, accuracy 0.336000\n",
      "iteration 1482 / 1500: loss 2.056815, accuracy 0.350000\n",
      "iteration 1483 / 1500: loss 2.048083, accuracy 0.359000\n",
      "iteration 1484 / 1500: loss 2.081584, accuracy 0.333000\n",
      "iteration 1485 / 1500: loss 2.047949, accuracy 0.371000\n",
      "iteration 1486 / 1500: loss 2.050685, accuracy 0.354000\n",
      "iteration 1487 / 1500: loss 2.049099, accuracy 0.332000\n",
      "iteration 1488 / 1500: loss 2.072957, accuracy 0.314000\n",
      "iteration 1489 / 1500: loss 2.064270, accuracy 0.327000\n",
      "iteration 1490 / 1500: loss 2.069513, accuracy 0.331000\n",
      "iteration 1491 / 1500: loss 2.058028, accuracy 0.347000\n",
      "iteration 1492 / 1500: loss 2.076757, accuracy 0.341000\n",
      "iteration 1493 / 1500: loss 2.081390, accuracy 0.334000\n",
      "iteration 1494 / 1500: loss 2.064787, accuracy 0.323000\n",
      "iteration 1495 / 1500: loss 2.079537, accuracy 0.328000\n",
      "iteration 1496 / 1500: loss 2.066390, accuracy 0.331000\n",
      "iteration 1497 / 1500: loss 2.061655, accuracy 0.335000\n",
      "iteration 1498 / 1500: loss 2.037218, accuracy 0.355000\n",
      "iteration 1499 / 1500: loss 2.076254, accuracy 0.345000\n",
      "iteration 0 / 1500: loss 622.506517, accuracy 0.134000\n",
      "iteration 1 / 1500: loss 618.849529, accuracy 0.122000\n",
      "iteration 2 / 1500: loss 615.269088, accuracy 0.141000\n",
      "iteration 3 / 1500: loss 611.572949, accuracy 0.131000\n",
      "iteration 4 / 1500: loss 607.759321, accuracy 0.129000\n",
      "iteration 5 / 1500: loss 604.226824, accuracy 0.138000\n",
      "iteration 6 / 1500: loss 600.408852, accuracy 0.133000\n",
      "iteration 7 / 1500: loss 597.044201, accuracy 0.122000\n",
      "iteration 8 / 1500: loss 593.214341, accuracy 0.121000\n",
      "iteration 9 / 1500: loss 589.806872, accuracy 0.144000\n",
      "iteration 10 / 1500: loss 586.424670, accuracy 0.131000\n",
      "iteration 11 / 1500: loss 582.726819, accuracy 0.132000\n",
      "iteration 12 / 1500: loss 579.341636, accuracy 0.120000\n",
      "iteration 13 / 1500: loss 575.744144, accuracy 0.116000\n",
      "iteration 14 / 1500: loss 572.470106, accuracy 0.122000\n",
      "iteration 15 / 1500: loss 569.050385, accuracy 0.124000\n",
      "iteration 16 / 1500: loss 565.338276, accuracy 0.122000\n",
      "iteration 17 / 1500: loss 562.110173, accuracy 0.129000\n",
      "iteration 18 / 1500: loss 558.545741, accuracy 0.143000\n",
      "iteration 19 / 1500: loss 555.292106, accuracy 0.111000\n",
      "iteration 20 / 1500: loss 551.922164, accuracy 0.117000\n",
      "iteration 21 / 1500: loss 548.784003, accuracy 0.105000\n",
      "iteration 22 / 1500: loss 545.348895, accuracy 0.140000\n",
      "iteration 23 / 1500: loss 542.125509, accuracy 0.121000\n",
      "iteration 24 / 1500: loss 538.958026, accuracy 0.118000\n",
      "iteration 25 / 1500: loss 535.614407, accuracy 0.114000\n",
      "iteration 26 / 1500: loss 532.252350, accuracy 0.138000\n",
      "iteration 27 / 1500: loss 529.215700, accuracy 0.125000\n",
      "iteration 28 / 1500: loss 526.022237, accuracy 0.109000\n",
      "iteration 29 / 1500: loss 522.919514, accuracy 0.129000\n",
      "iteration 30 / 1500: loss 519.630209, accuracy 0.122000\n",
      "iteration 31 / 1500: loss 516.592287, accuracy 0.128000\n",
      "iteration 32 / 1500: loss 513.597293, accuracy 0.110000\n",
      "iteration 33 / 1500: loss 510.333633, accuracy 0.129000\n",
      "iteration 34 / 1500: loss 507.277093, accuracy 0.125000\n",
      "iteration 35 / 1500: loss 504.372306, accuracy 0.125000\n",
      "iteration 36 / 1500: loss 501.225104, accuracy 0.142000\n",
      "iteration 37 / 1500: loss 498.298510, accuracy 0.114000\n",
      "iteration 38 / 1500: loss 495.326069, accuracy 0.115000\n",
      "iteration 39 / 1500: loss 492.199915, accuracy 0.118000\n",
      "iteration 40 / 1500: loss 489.316275, accuracy 0.130000\n",
      "iteration 41 / 1500: loss 486.415253, accuracy 0.124000\n",
      "iteration 42 / 1500: loss 483.428526, accuracy 0.115000\n",
      "iteration 43 / 1500: loss 480.594249, accuracy 0.120000\n",
      "iteration 44 / 1500: loss 477.748390, accuracy 0.131000\n",
      "iteration 45 / 1500: loss 474.956794, accuracy 0.127000\n",
      "iteration 46 / 1500: loss 472.004808, accuracy 0.124000\n",
      "iteration 47 / 1500: loss 469.161315, accuracy 0.123000\n",
      "iteration 48 / 1500: loss 466.393843, accuracy 0.129000\n",
      "iteration 49 / 1500: loss 463.508826, accuracy 0.117000\n",
      "iteration 50 / 1500: loss 460.619021, accuracy 0.147000\n",
      "iteration 51 / 1500: loss 458.056478, accuracy 0.132000\n",
      "iteration 52 / 1500: loss 455.228204, accuracy 0.113000\n",
      "iteration 53 / 1500: loss 452.531197, accuracy 0.121000\n",
      "iteration 54 / 1500: loss 449.737057, accuracy 0.141000\n",
      "iteration 55 / 1500: loss 447.140661, accuracy 0.130000\n",
      "iteration 56 / 1500: loss 444.377781, accuracy 0.136000\n",
      "iteration 57 / 1500: loss 441.895818, accuracy 0.130000\n",
      "iteration 58 / 1500: loss 439.269358, accuracy 0.139000\n",
      "iteration 59 / 1500: loss 436.544452, accuracy 0.133000\n",
      "iteration 60 / 1500: loss 434.013448, accuracy 0.116000\n",
      "iteration 61 / 1500: loss 431.346100, accuracy 0.131000\n",
      "iteration 62 / 1500: loss 428.654245, accuracy 0.119000\n",
      "iteration 63 / 1500: loss 426.092145, accuracy 0.146000\n",
      "iteration 64 / 1500: loss 423.481358, accuracy 0.133000\n",
      "iteration 65 / 1500: loss 421.060207, accuracy 0.132000\n",
      "iteration 66 / 1500: loss 418.571402, accuracy 0.122000\n",
      "iteration 67 / 1500: loss 416.126867, accuracy 0.144000\n",
      "iteration 68 / 1500: loss 413.488286, accuracy 0.120000\n",
      "iteration 69 / 1500: loss 411.075374, accuracy 0.119000\n",
      "iteration 70 / 1500: loss 408.631231, accuracy 0.116000\n",
      "iteration 71 / 1500: loss 406.222838, accuracy 0.133000\n",
      "iteration 72 / 1500: loss 403.674363, accuracy 0.133000\n",
      "iteration 73 / 1500: loss 401.294405, accuracy 0.132000\n",
      "iteration 74 / 1500: loss 398.788482, accuracy 0.125000\n",
      "iteration 75 / 1500: loss 396.573541, accuracy 0.119000\n",
      "iteration 76 / 1500: loss 394.107151, accuracy 0.134000\n",
      "iteration 77 / 1500: loss 391.621657, accuracy 0.152000\n",
      "iteration 78 / 1500: loss 389.437605, accuracy 0.136000\n",
      "iteration 79 / 1500: loss 387.109148, accuracy 0.131000\n",
      "iteration 80 / 1500: loss 384.825465, accuracy 0.127000\n",
      "iteration 81 / 1500: loss 382.527576, accuracy 0.126000\n",
      "iteration 82 / 1500: loss 380.296570, accuracy 0.125000\n",
      "iteration 83 / 1500: loss 377.951024, accuracy 0.145000\n",
      "iteration 84 / 1500: loss 375.484481, accuracy 0.132000\n",
      "iteration 85 / 1500: loss 373.423282, accuracy 0.117000\n",
      "iteration 86 / 1500: loss 371.140283, accuracy 0.145000\n",
      "iteration 87 / 1500: loss 368.984860, accuracy 0.133000\n",
      "iteration 88 / 1500: loss 366.662185, accuracy 0.147000\n",
      "iteration 89 / 1500: loss 364.453557, accuracy 0.144000\n",
      "iteration 90 / 1500: loss 362.348093, accuracy 0.143000\n",
      "iteration 91 / 1500: loss 360.146565, accuracy 0.141000\n",
      "iteration 92 / 1500: loss 358.011698, accuracy 0.142000\n",
      "iteration 93 / 1500: loss 355.811804, accuracy 0.164000\n",
      "iteration 94 / 1500: loss 353.770822, accuracy 0.147000\n",
      "iteration 95 / 1500: loss 351.639746, accuracy 0.144000\n",
      "iteration 96 / 1500: loss 349.452668, accuracy 0.152000\n",
      "iteration 97 / 1500: loss 347.404009, accuracy 0.138000\n",
      "iteration 98 / 1500: loss 345.405832, accuracy 0.136000\n",
      "iteration 99 / 1500: loss 343.270611, accuracy 0.130000\n",
      "iteration 100 / 1500: loss 341.220579, accuracy 0.142000\n",
      "iteration 101 / 1500: loss 339.138223, accuracy 0.140000\n",
      "iteration 102 / 1500: loss 337.188141, accuracy 0.144000\n",
      "iteration 103 / 1500: loss 335.146744, accuracy 0.146000\n",
      "iteration 104 / 1500: loss 333.196123, accuracy 0.153000\n",
      "iteration 105 / 1500: loss 331.136946, accuracy 0.156000\n",
      "iteration 106 / 1500: loss 329.138357, accuracy 0.160000\n",
      "iteration 107 / 1500: loss 327.159503, accuracy 0.181000\n",
      "iteration 108 / 1500: loss 325.275123, accuracy 0.150000\n",
      "iteration 109 / 1500: loss 323.312286, accuracy 0.148000\n",
      "iteration 110 / 1500: loss 321.435698, accuracy 0.142000\n",
      "iteration 111 / 1500: loss 319.498547, accuracy 0.156000\n",
      "iteration 112 / 1500: loss 317.690540, accuracy 0.124000\n",
      "iteration 113 / 1500: loss 315.712681, accuracy 0.136000\n",
      "iteration 114 / 1500: loss 313.846718, accuracy 0.129000\n",
      "iteration 115 / 1500: loss 311.958200, accuracy 0.147000\n",
      "iteration 116 / 1500: loss 310.098973, accuracy 0.148000\n",
      "iteration 117 / 1500: loss 308.146620, accuracy 0.159000\n",
      "iteration 118 / 1500: loss 306.338391, accuracy 0.155000\n",
      "iteration 119 / 1500: loss 304.424840, accuracy 0.163000\n",
      "iteration 120 / 1500: loss 302.703726, accuracy 0.157000\n",
      "iteration 121 / 1500: loss 300.915911, accuracy 0.154000\n",
      "iteration 122 / 1500: loss 299.046477, accuracy 0.152000\n",
      "iteration 123 / 1500: loss 297.099207, accuracy 0.155000\n",
      "iteration 124 / 1500: loss 295.568618, accuracy 0.151000\n",
      "iteration 125 / 1500: loss 293.792588, accuracy 0.142000\n",
      "iteration 126 / 1500: loss 291.999788, accuracy 0.158000\n",
      "iteration 127 / 1500: loss 290.142930, accuracy 0.168000\n",
      "iteration 128 / 1500: loss 288.537809, accuracy 0.144000\n",
      "iteration 129 / 1500: loss 286.792429, accuracy 0.141000\n",
      "iteration 130 / 1500: loss 285.047150, accuracy 0.150000\n",
      "iteration 131 / 1500: loss 283.325543, accuracy 0.171000\n",
      "iteration 132 / 1500: loss 281.765406, accuracy 0.148000\n",
      "iteration 133 / 1500: loss 280.001822, accuracy 0.147000\n",
      "iteration 134 / 1500: loss 278.350725, accuracy 0.141000\n",
      "iteration 135 / 1500: loss 276.705662, accuracy 0.144000\n",
      "iteration 136 / 1500: loss 274.992370, accuracy 0.146000\n",
      "iteration 137 / 1500: loss 273.338377, accuracy 0.152000\n",
      "iteration 138 / 1500: loss 271.768705, accuracy 0.148000\n",
      "iteration 139 / 1500: loss 270.184270, accuracy 0.142000\n",
      "iteration 140 / 1500: loss 268.491291, accuracy 0.166000\n",
      "iteration 141 / 1500: loss 266.948371, accuracy 0.144000\n",
      "iteration 142 / 1500: loss 265.304078, accuracy 0.156000\n",
      "iteration 143 / 1500: loss 263.698211, accuracy 0.162000\n",
      "iteration 144 / 1500: loss 262.106288, accuracy 0.166000\n",
      "iteration 145 / 1500: loss 260.574387, accuracy 0.169000\n",
      "iteration 146 / 1500: loss 258.968073, accuracy 0.154000\n",
      "iteration 147 / 1500: loss 257.436846, accuracy 0.158000\n",
      "iteration 148 / 1500: loss 255.949768, accuracy 0.158000\n",
      "iteration 149 / 1500: loss 254.496907, accuracy 0.156000\n",
      "iteration 150 / 1500: loss 252.888162, accuracy 0.143000\n",
      "iteration 151 / 1500: loss 251.472334, accuracy 0.160000\n",
      "iteration 152 / 1500: loss 249.899817, accuracy 0.148000\n",
      "iteration 153 / 1500: loss 248.433930, accuracy 0.156000\n",
      "iteration 154 / 1500: loss 246.886458, accuracy 0.138000\n",
      "iteration 155 / 1500: loss 245.482652, accuracy 0.157000\n",
      "iteration 156 / 1500: loss 243.951949, accuracy 0.174000\n",
      "iteration 157 / 1500: loss 242.506713, accuracy 0.157000\n",
      "iteration 158 / 1500: loss 241.039404, accuracy 0.167000\n",
      "iteration 159 / 1500: loss 239.626457, accuracy 0.151000\n",
      "iteration 160 / 1500: loss 238.244604, accuracy 0.166000\n",
      "iteration 161 / 1500: loss 236.799411, accuracy 0.184000\n",
      "iteration 162 / 1500: loss 235.463234, accuracy 0.175000\n",
      "iteration 163 / 1500: loss 233.986192, accuracy 0.160000\n",
      "iteration 164 / 1500: loss 232.565281, accuracy 0.176000\n",
      "iteration 165 / 1500: loss 231.219730, accuracy 0.173000\n",
      "iteration 166 / 1500: loss 229.794619, accuracy 0.178000\n",
      "iteration 167 / 1500: loss 228.535765, accuracy 0.158000\n",
      "iteration 168 / 1500: loss 227.074639, accuracy 0.160000\n",
      "iteration 169 / 1500: loss 225.775635, accuracy 0.170000\n",
      "iteration 170 / 1500: loss 224.405192, accuracy 0.154000\n",
      "iteration 171 / 1500: loss 223.102746, accuracy 0.163000\n",
      "iteration 172 / 1500: loss 221.717868, accuracy 0.162000\n",
      "iteration 173 / 1500: loss 220.438338, accuracy 0.152000\n",
      "iteration 174 / 1500: loss 219.030896, accuracy 0.173000\n",
      "iteration 175 / 1500: loss 217.796236, accuracy 0.167000\n",
      "iteration 176 / 1500: loss 216.448875, accuracy 0.172000\n",
      "iteration 177 / 1500: loss 215.249711, accuracy 0.159000\n",
      "iteration 178 / 1500: loss 213.896163, accuracy 0.179000\n",
      "iteration 179 / 1500: loss 212.612412, accuracy 0.158000\n",
      "iteration 180 / 1500: loss 211.504023, accuracy 0.141000\n",
      "iteration 181 / 1500: loss 210.141405, accuracy 0.175000\n",
      "iteration 182 / 1500: loss 208.919771, accuracy 0.176000\n",
      "iteration 183 / 1500: loss 207.734396, accuracy 0.167000\n",
      "iteration 184 / 1500: loss 206.478576, accuracy 0.157000\n",
      "iteration 185 / 1500: loss 205.208745, accuracy 0.172000\n",
      "iteration 186 / 1500: loss 203.926021, accuracy 0.159000\n",
      "iteration 187 / 1500: loss 202.651589, accuracy 0.190000\n",
      "iteration 188 / 1500: loss 201.514572, accuracy 0.169000\n",
      "iteration 189 / 1500: loss 200.380825, accuracy 0.178000\n",
      "iteration 190 / 1500: loss 199.121260, accuracy 0.159000\n",
      "iteration 191 / 1500: loss 198.019545, accuracy 0.146000\n",
      "iteration 192 / 1500: loss 196.808163, accuracy 0.180000\n",
      "iteration 193 / 1500: loss 195.603164, accuracy 0.166000\n",
      "iteration 194 / 1500: loss 194.414003, accuracy 0.172000\n",
      "iteration 195 / 1500: loss 193.266986, accuracy 0.179000\n",
      "iteration 196 / 1500: loss 192.130880, accuracy 0.166000\n",
      "iteration 197 / 1500: loss 191.040202, accuracy 0.150000\n",
      "iteration 198 / 1500: loss 189.871846, accuracy 0.163000\n",
      "iteration 199 / 1500: loss 188.763006, accuracy 0.178000\n",
      "iteration 200 / 1500: loss 187.597235, accuracy 0.164000\n",
      "iteration 201 / 1500: loss 186.461659, accuracy 0.174000\n",
      "iteration 202 / 1500: loss 185.347885, accuracy 0.170000\n",
      "iteration 203 / 1500: loss 184.340713, accuracy 0.163000\n",
      "iteration 204 / 1500: loss 183.207251, accuracy 0.171000\n",
      "iteration 205 / 1500: loss 182.025541, accuracy 0.186000\n",
      "iteration 206 / 1500: loss 181.003726, accuracy 0.192000\n",
      "iteration 207 / 1500: loss 179.878420, accuracy 0.188000\n",
      "iteration 208 / 1500: loss 178.852998, accuracy 0.162000\n",
      "iteration 209 / 1500: loss 177.781042, accuracy 0.174000\n",
      "iteration 210 / 1500: loss 176.673146, accuracy 0.184000\n",
      "iteration 211 / 1500: loss 175.707380, accuracy 0.179000\n",
      "iteration 212 / 1500: loss 174.649454, accuracy 0.179000\n",
      "iteration 213 / 1500: loss 173.659203, accuracy 0.165000\n",
      "iteration 214 / 1500: loss 172.550321, accuracy 0.176000\n",
      "iteration 215 / 1500: loss 171.489872, accuracy 0.207000\n",
      "iteration 216 / 1500: loss 170.504940, accuracy 0.183000\n",
      "iteration 217 / 1500: loss 169.557883, accuracy 0.193000\n",
      "iteration 218 / 1500: loss 168.460991, accuracy 0.183000\n",
      "iteration 219 / 1500: loss 167.482964, accuracy 0.195000\n",
      "iteration 220 / 1500: loss 166.483832, accuracy 0.178000\n",
      "iteration 221 / 1500: loss 165.598204, accuracy 0.170000\n",
      "iteration 222 / 1500: loss 164.519153, accuracy 0.198000\n",
      "iteration 223 / 1500: loss 163.575910, accuracy 0.175000\n",
      "iteration 224 / 1500: loss 162.591226, accuracy 0.199000\n",
      "iteration 225 / 1500: loss 161.673802, accuracy 0.164000\n",
      "iteration 226 / 1500: loss 160.719250, accuracy 0.176000\n",
      "iteration 227 / 1500: loss 159.698013, accuracy 0.184000\n",
      "iteration 228 / 1500: loss 158.804260, accuracy 0.172000\n",
      "iteration 229 / 1500: loss 157.814776, accuracy 0.180000\n",
      "iteration 230 / 1500: loss 156.866321, accuracy 0.179000\n",
      "iteration 231 / 1500: loss 155.956605, accuracy 0.188000\n",
      "iteration 232 / 1500: loss 155.013009, accuracy 0.183000\n",
      "iteration 233 / 1500: loss 154.166079, accuracy 0.168000\n",
      "iteration 234 / 1500: loss 153.180174, accuracy 0.194000\n",
      "iteration 235 / 1500: loss 152.237702, accuracy 0.194000\n",
      "iteration 236 / 1500: loss 151.389167, accuracy 0.175000\n",
      "iteration 237 / 1500: loss 150.421872, accuracy 0.210000\n",
      "iteration 238 / 1500: loss 149.618372, accuracy 0.188000\n",
      "iteration 239 / 1500: loss 148.702112, accuracy 0.175000\n",
      "iteration 240 / 1500: loss 147.874332, accuracy 0.173000\n",
      "iteration 241 / 1500: loss 146.991317, accuracy 0.186000\n",
      "iteration 242 / 1500: loss 146.109301, accuracy 0.181000\n",
      "iteration 243 / 1500: loss 145.241376, accuracy 0.180000\n",
      "iteration 244 / 1500: loss 144.371300, accuracy 0.177000\n",
      "iteration 245 / 1500: loss 143.473434, accuracy 0.196000\n",
      "iteration 246 / 1500: loss 142.684596, accuracy 0.180000\n",
      "iteration 247 / 1500: loss 141.840634, accuracy 0.197000\n",
      "iteration 248 / 1500: loss 140.979736, accuracy 0.192000\n",
      "iteration 249 / 1500: loss 140.103645, accuracy 0.208000\n",
      "iteration 250 / 1500: loss 139.211946, accuracy 0.204000\n",
      "iteration 251 / 1500: loss 138.477398, accuracy 0.183000\n",
      "iteration 252 / 1500: loss 137.673429, accuracy 0.197000\n",
      "iteration 253 / 1500: loss 136.933854, accuracy 0.177000\n",
      "iteration 254 / 1500: loss 136.068860, accuracy 0.184000\n",
      "iteration 255 / 1500: loss 135.200994, accuracy 0.205000\n",
      "iteration 256 / 1500: loss 134.401929, accuracy 0.193000\n",
      "iteration 257 / 1500: loss 133.653384, accuracy 0.192000\n",
      "iteration 258 / 1500: loss 132.842946, accuracy 0.196000\n",
      "iteration 259 / 1500: loss 132.141790, accuracy 0.179000\n",
      "iteration 260 / 1500: loss 131.192935, accuracy 0.225000\n",
      "iteration 261 / 1500: loss 130.454728, accuracy 0.197000\n",
      "iteration 262 / 1500: loss 129.708732, accuracy 0.189000\n",
      "iteration 263 / 1500: loss 128.932086, accuracy 0.194000\n",
      "iteration 264 / 1500: loss 128.226243, accuracy 0.181000\n",
      "iteration 265 / 1500: loss 127.457221, accuracy 0.189000\n",
      "iteration 266 / 1500: loss 126.613134, accuracy 0.204000\n",
      "iteration 267 / 1500: loss 125.965779, accuracy 0.183000\n",
      "iteration 268 / 1500: loss 125.169189, accuracy 0.180000\n",
      "iteration 269 / 1500: loss 124.375192, accuracy 0.207000\n",
      "iteration 270 / 1500: loss 123.714707, accuracy 0.189000\n",
      "iteration 271 / 1500: loss 122.964182, accuracy 0.202000\n",
      "iteration 272 / 1500: loss 122.236517, accuracy 0.187000\n",
      "iteration 273 / 1500: loss 121.524794, accuracy 0.203000\n",
      "iteration 274 / 1500: loss 120.786873, accuracy 0.193000\n",
      "iteration 275 / 1500: loss 120.071040, accuracy 0.182000\n",
      "iteration 276 / 1500: loss 119.411211, accuracy 0.184000\n",
      "iteration 277 / 1500: loss 118.661555, accuracy 0.205000\n",
      "iteration 278 / 1500: loss 117.977658, accuracy 0.183000\n",
      "iteration 279 / 1500: loss 117.308787, accuracy 0.206000\n",
      "iteration 280 / 1500: loss 116.564964, accuracy 0.204000\n",
      "iteration 281 / 1500: loss 115.869720, accuracy 0.203000\n",
      "iteration 282 / 1500: loss 115.218313, accuracy 0.204000\n",
      "iteration 283 / 1500: loss 114.572525, accuracy 0.180000\n",
      "iteration 284 / 1500: loss 113.804137, accuracy 0.217000\n",
      "iteration 285 / 1500: loss 113.179374, accuracy 0.204000\n",
      "iteration 286 / 1500: loss 112.481879, accuracy 0.218000\n",
      "iteration 287 / 1500: loss 111.845442, accuracy 0.195000\n",
      "iteration 288 / 1500: loss 111.215732, accuracy 0.209000\n",
      "iteration 289 / 1500: loss 110.479145, accuracy 0.206000\n",
      "iteration 290 / 1500: loss 109.892634, accuracy 0.194000\n",
      "iteration 291 / 1500: loss 109.216108, accuracy 0.201000\n",
      "iteration 292 / 1500: loss 108.604843, accuracy 0.196000\n",
      "iteration 293 / 1500: loss 107.964413, accuracy 0.213000\n",
      "iteration 294 / 1500: loss 107.342099, accuracy 0.205000\n",
      "iteration 295 / 1500: loss 106.653794, accuracy 0.201000\n",
      "iteration 296 / 1500: loss 106.086465, accuracy 0.194000\n",
      "iteration 297 / 1500: loss 105.451482, accuracy 0.207000\n",
      "iteration 298 / 1500: loss 104.794586, accuracy 0.203000\n",
      "iteration 299 / 1500: loss 104.181170, accuracy 0.218000\n",
      "iteration 300 / 1500: loss 103.577605, accuracy 0.195000\n",
      "iteration 301 / 1500: loss 103.031401, accuracy 0.202000\n",
      "iteration 302 / 1500: loss 102.347029, accuracy 0.206000\n",
      "iteration 303 / 1500: loss 101.801200, accuracy 0.202000\n",
      "iteration 304 / 1500: loss 101.141527, accuracy 0.206000\n",
      "iteration 305 / 1500: loss 100.574543, accuracy 0.213000\n",
      "iteration 306 / 1500: loss 99.938390, accuracy 0.224000\n",
      "iteration 307 / 1500: loss 99.369427, accuracy 0.211000\n",
      "iteration 308 / 1500: loss 98.795198, accuracy 0.208000\n",
      "iteration 309 / 1500: loss 98.233663, accuracy 0.198000\n",
      "iteration 310 / 1500: loss 97.593362, accuracy 0.231000\n",
      "iteration 311 / 1500: loss 97.093024, accuracy 0.196000\n",
      "iteration 312 / 1500: loss 96.449065, accuracy 0.230000\n",
      "iteration 313 / 1500: loss 95.981608, accuracy 0.181000\n",
      "iteration 314 / 1500: loss 95.402952, accuracy 0.195000\n",
      "iteration 315 / 1500: loss 94.775739, accuracy 0.224000\n",
      "iteration 316 / 1500: loss 94.262067, accuracy 0.179000\n",
      "iteration 317 / 1500: loss 93.733356, accuracy 0.190000\n",
      "iteration 318 / 1500: loss 93.133872, accuracy 0.218000\n",
      "iteration 319 / 1500: loss 92.614838, accuracy 0.190000\n",
      "iteration 320 / 1500: loss 92.062930, accuracy 0.204000\n",
      "iteration 321 / 1500: loss 91.544362, accuracy 0.191000\n",
      "iteration 322 / 1500: loss 90.928391, accuracy 0.204000\n",
      "iteration 323 / 1500: loss 90.409660, accuracy 0.212000\n",
      "iteration 324 / 1500: loss 89.912717, accuracy 0.203000\n",
      "iteration 325 / 1500: loss 89.338825, accuracy 0.216000\n",
      "iteration 326 / 1500: loss 88.855293, accuracy 0.217000\n",
      "iteration 327 / 1500: loss 88.325968, accuracy 0.205000\n",
      "iteration 328 / 1500: loss 87.825527, accuracy 0.217000\n",
      "iteration 329 / 1500: loss 87.287962, accuracy 0.204000\n",
      "iteration 330 / 1500: loss 86.773601, accuracy 0.207000\n",
      "iteration 331 / 1500: loss 86.283868, accuracy 0.220000\n",
      "iteration 332 / 1500: loss 85.737891, accuracy 0.215000\n",
      "iteration 333 / 1500: loss 85.202582, accuracy 0.224000\n",
      "iteration 334 / 1500: loss 84.724545, accuracy 0.219000\n",
      "iteration 335 / 1500: loss 84.342478, accuracy 0.184000\n",
      "iteration 336 / 1500: loss 83.800342, accuracy 0.206000\n",
      "iteration 337 / 1500: loss 83.314069, accuracy 0.221000\n",
      "iteration 338 / 1500: loss 82.790168, accuracy 0.206000\n",
      "iteration 339 / 1500: loss 82.370051, accuracy 0.193000\n",
      "iteration 340 / 1500: loss 81.765348, accuracy 0.233000\n",
      "iteration 341 / 1500: loss 81.351725, accuracy 0.194000\n",
      "iteration 342 / 1500: loss 80.891800, accuracy 0.199000\n",
      "iteration 343 / 1500: loss 80.367064, accuracy 0.235000\n",
      "iteration 344 / 1500: loss 79.947386, accuracy 0.204000\n",
      "iteration 345 / 1500: loss 79.487162, accuracy 0.193000\n",
      "iteration 346 / 1500: loss 78.981106, accuracy 0.201000\n",
      "iteration 347 / 1500: loss 78.527444, accuracy 0.228000\n",
      "iteration 348 / 1500: loss 78.036569, accuracy 0.217000\n",
      "iteration 349 / 1500: loss 77.601685, accuracy 0.230000\n",
      "iteration 350 / 1500: loss 77.163063, accuracy 0.220000\n",
      "iteration 351 / 1500: loss 76.674276, accuracy 0.244000\n",
      "iteration 352 / 1500: loss 76.251873, accuracy 0.206000\n",
      "iteration 353 / 1500: loss 75.790649, accuracy 0.206000\n",
      "iteration 354 / 1500: loss 75.361397, accuracy 0.210000\n",
      "iteration 355 / 1500: loss 74.936539, accuracy 0.216000\n",
      "iteration 356 / 1500: loss 74.459250, accuracy 0.220000\n",
      "iteration 357 / 1500: loss 74.067309, accuracy 0.219000\n",
      "iteration 358 / 1500: loss 73.616219, accuracy 0.211000\n",
      "iteration 359 / 1500: loss 73.176228, accuracy 0.241000\n",
      "iteration 360 / 1500: loss 72.786146, accuracy 0.210000\n",
      "iteration 361 / 1500: loss 72.298735, accuracy 0.241000\n",
      "iteration 362 / 1500: loss 71.900747, accuracy 0.232000\n",
      "iteration 363 / 1500: loss 71.474083, accuracy 0.238000\n",
      "iteration 364 / 1500: loss 71.078012, accuracy 0.240000\n",
      "iteration 365 / 1500: loss 70.631877, accuracy 0.231000\n",
      "iteration 366 / 1500: loss 70.261654, accuracy 0.214000\n",
      "iteration 367 / 1500: loss 69.850404, accuracy 0.219000\n",
      "iteration 368 / 1500: loss 69.429244, accuracy 0.219000\n",
      "iteration 369 / 1500: loss 68.977479, accuracy 0.250000\n",
      "iteration 370 / 1500: loss 68.640831, accuracy 0.209000\n",
      "iteration 371 / 1500: loss 68.216573, accuracy 0.228000\n",
      "iteration 372 / 1500: loss 67.859027, accuracy 0.217000\n",
      "iteration 373 / 1500: loss 67.417326, accuracy 0.208000\n",
      "iteration 374 / 1500: loss 66.979224, accuracy 0.252000\n",
      "iteration 375 / 1500: loss 66.659584, accuracy 0.220000\n",
      "iteration 376 / 1500: loss 66.250950, accuracy 0.234000\n",
      "iteration 377 / 1500: loss 65.829490, accuracy 0.244000\n",
      "iteration 378 / 1500: loss 65.491650, accuracy 0.218000\n",
      "iteration 379 / 1500: loss 65.070764, accuracy 0.256000\n",
      "iteration 380 / 1500: loss 64.694884, accuracy 0.236000\n",
      "iteration 381 / 1500: loss 64.358014, accuracy 0.221000\n",
      "iteration 382 / 1500: loss 63.930653, accuracy 0.231000\n",
      "iteration 383 / 1500: loss 63.613796, accuracy 0.224000\n",
      "iteration 384 / 1500: loss 63.275691, accuracy 0.227000\n",
      "iteration 385 / 1500: loss 62.867830, accuracy 0.240000\n",
      "iteration 386 / 1500: loss 62.542413, accuracy 0.215000\n",
      "iteration 387 / 1500: loss 62.129748, accuracy 0.229000\n",
      "iteration 388 / 1500: loss 61.738309, accuracy 0.260000\n",
      "iteration 389 / 1500: loss 61.371210, accuracy 0.248000\n",
      "iteration 390 / 1500: loss 61.039185, accuracy 0.251000\n",
      "iteration 391 / 1500: loss 60.685420, accuracy 0.252000\n",
      "iteration 392 / 1500: loss 60.364736, accuracy 0.225000\n",
      "iteration 393 / 1500: loss 60.045842, accuracy 0.206000\n",
      "iteration 394 / 1500: loss 59.691677, accuracy 0.205000\n",
      "iteration 395 / 1500: loss 59.279375, accuracy 0.245000\n",
      "iteration 396 / 1500: loss 58.982746, accuracy 0.228000\n",
      "iteration 397 / 1500: loss 58.612862, accuracy 0.244000\n",
      "iteration 398 / 1500: loss 58.223142, accuracy 0.231000\n",
      "iteration 399 / 1500: loss 57.936838, accuracy 0.239000\n",
      "iteration 400 / 1500: loss 57.605727, accuracy 0.233000\n",
      "iteration 401 / 1500: loss 57.302233, accuracy 0.223000\n",
      "iteration 402 / 1500: loss 56.943046, accuracy 0.228000\n",
      "iteration 403 / 1500: loss 56.590108, accuracy 0.242000\n",
      "iteration 404 / 1500: loss 56.286254, accuracy 0.238000\n",
      "iteration 405 / 1500: loss 55.969748, accuracy 0.237000\n",
      "iteration 406 / 1500: loss 55.703520, accuracy 0.219000\n",
      "iteration 407 / 1500: loss 55.317640, accuracy 0.252000\n",
      "iteration 408 / 1500: loss 54.974690, accuracy 0.237000\n",
      "iteration 409 / 1500: loss 54.667892, accuracy 0.248000\n",
      "iteration 410 / 1500: loss 54.312302, accuracy 0.258000\n",
      "iteration 411 / 1500: loss 54.024423, accuracy 0.253000\n",
      "iteration 412 / 1500: loss 53.714308, accuracy 0.243000\n",
      "iteration 413 / 1500: loss 53.443184, accuracy 0.215000\n",
      "iteration 414 / 1500: loss 53.111689, accuracy 0.242000\n",
      "iteration 415 / 1500: loss 52.768336, accuracy 0.258000\n",
      "iteration 416 / 1500: loss 52.470613, accuracy 0.252000\n",
      "iteration 417 / 1500: loss 52.132550, accuracy 0.253000\n",
      "iteration 418 / 1500: loss 51.921929, accuracy 0.226000\n",
      "iteration 419 / 1500: loss 51.585411, accuracy 0.245000\n",
      "iteration 420 / 1500: loss 51.325928, accuracy 0.228000\n",
      "iteration 421 / 1500: loss 51.020022, accuracy 0.243000\n",
      "iteration 422 / 1500: loss 50.703738, accuracy 0.230000\n",
      "iteration 423 / 1500: loss 50.468624, accuracy 0.229000\n",
      "iteration 424 / 1500: loss 50.080010, accuracy 0.261000\n",
      "iteration 425 / 1500: loss 49.853261, accuracy 0.225000\n",
      "iteration 426 / 1500: loss 49.540454, accuracy 0.241000\n",
      "iteration 427 / 1500: loss 49.264636, accuracy 0.236000\n",
      "iteration 428 / 1500: loss 48.986826, accuracy 0.218000\n",
      "iteration 429 / 1500: loss 48.677306, accuracy 0.246000\n",
      "iteration 430 / 1500: loss 48.425960, accuracy 0.226000\n",
      "iteration 431 / 1500: loss 48.136766, accuracy 0.251000\n",
      "iteration 432 / 1500: loss 47.830760, accuracy 0.252000\n",
      "iteration 433 / 1500: loss 47.598094, accuracy 0.238000\n",
      "iteration 434 / 1500: loss 47.314983, accuracy 0.246000\n",
      "iteration 435 / 1500: loss 47.055042, accuracy 0.252000\n",
      "iteration 436 / 1500: loss 46.764110, accuracy 0.249000\n",
      "iteration 437 / 1500: loss 46.493690, accuracy 0.254000\n",
      "iteration 438 / 1500: loss 46.213261, accuracy 0.257000\n",
      "iteration 439 / 1500: loss 45.956645, accuracy 0.258000\n",
      "iteration 440 / 1500: loss 45.694655, accuracy 0.244000\n",
      "iteration 441 / 1500: loss 45.442633, accuracy 0.234000\n",
      "iteration 442 / 1500: loss 45.154639, accuracy 0.261000\n",
      "iteration 443 / 1500: loss 44.956692, accuracy 0.239000\n",
      "iteration 444 / 1500: loss 44.651806, accuracy 0.251000\n",
      "iteration 445 / 1500: loss 44.420452, accuracy 0.257000\n",
      "iteration 446 / 1500: loss 44.160187, accuracy 0.252000\n",
      "iteration 447 / 1500: loss 43.868376, accuracy 0.240000\n",
      "iteration 448 / 1500: loss 43.679993, accuracy 0.235000\n",
      "iteration 449 / 1500: loss 43.412110, accuracy 0.267000\n",
      "iteration 450 / 1500: loss 43.139658, accuracy 0.264000\n",
      "iteration 451 / 1500: loss 42.907639, accuracy 0.253000\n",
      "iteration 452 / 1500: loss 42.683528, accuracy 0.240000\n",
      "iteration 453 / 1500: loss 42.428893, accuracy 0.233000\n",
      "iteration 454 / 1500: loss 42.167690, accuracy 0.252000\n",
      "iteration 455 / 1500: loss 41.922268, accuracy 0.265000\n",
      "iteration 456 / 1500: loss 41.699000, accuracy 0.245000\n",
      "iteration 457 / 1500: loss 41.450429, accuracy 0.238000\n",
      "iteration 458 / 1500: loss 41.182911, accuracy 0.270000\n",
      "iteration 459 / 1500: loss 41.016763, accuracy 0.240000\n",
      "iteration 460 / 1500: loss 40.780847, accuracy 0.248000\n",
      "iteration 461 / 1500: loss 40.506030, accuracy 0.264000\n",
      "iteration 462 / 1500: loss 40.320106, accuracy 0.246000\n",
      "iteration 463 / 1500: loss 40.062263, accuracy 0.243000\n",
      "iteration 464 / 1500: loss 39.840225, accuracy 0.238000\n",
      "iteration 465 / 1500: loss 39.647611, accuracy 0.225000\n",
      "iteration 466 / 1500: loss 39.360486, accuracy 0.254000\n",
      "iteration 467 / 1500: loss 39.180364, accuracy 0.236000\n",
      "iteration 468 / 1500: loss 38.907930, accuracy 0.253000\n",
      "iteration 469 / 1500: loss 38.704854, accuracy 0.260000\n",
      "iteration 470 / 1500: loss 38.493545, accuracy 0.260000\n",
      "iteration 471 / 1500: loss 38.272507, accuracy 0.254000\n",
      "iteration 472 / 1500: loss 38.029817, accuracy 0.268000\n",
      "iteration 473 / 1500: loss 37.835341, accuracy 0.250000\n",
      "iteration 474 / 1500: loss 37.614416, accuracy 0.262000\n",
      "iteration 475 / 1500: loss 37.399721, accuracy 0.251000\n",
      "iteration 476 / 1500: loss 37.133506, accuracy 0.291000\n",
      "iteration 477 / 1500: loss 37.008693, accuracy 0.238000\n",
      "iteration 478 / 1500: loss 36.767873, accuracy 0.286000\n",
      "iteration 479 / 1500: loss 36.549937, accuracy 0.267000\n",
      "iteration 480 / 1500: loss 36.368006, accuracy 0.256000\n",
      "iteration 481 / 1500: loss 36.182021, accuracy 0.249000\n",
      "iteration 482 / 1500: loss 35.976978, accuracy 0.268000\n",
      "iteration 483 / 1500: loss 35.700020, accuracy 0.259000\n",
      "iteration 484 / 1500: loss 35.537488, accuracy 0.276000\n",
      "iteration 485 / 1500: loss 35.350981, accuracy 0.260000\n",
      "iteration 486 / 1500: loss 35.135204, accuracy 0.252000\n",
      "iteration 487 / 1500: loss 34.984294, accuracy 0.249000\n",
      "iteration 488 / 1500: loss 34.774938, accuracy 0.254000\n",
      "iteration 489 / 1500: loss 34.539524, accuracy 0.262000\n",
      "iteration 490 / 1500: loss 34.351238, accuracy 0.254000\n",
      "iteration 491 / 1500: loss 34.132857, accuracy 0.268000\n",
      "iteration 492 / 1500: loss 33.975832, accuracy 0.259000\n",
      "iteration 493 / 1500: loss 33.760613, accuracy 0.281000\n",
      "iteration 494 / 1500: loss 33.586629, accuracy 0.247000\n",
      "iteration 495 / 1500: loss 33.435053, accuracy 0.249000\n",
      "iteration 496 / 1500: loss 33.183144, accuracy 0.278000\n",
      "iteration 497 / 1500: loss 32.992905, accuracy 0.259000\n",
      "iteration 498 / 1500: loss 32.826952, accuracy 0.272000\n",
      "iteration 499 / 1500: loss 32.648981, accuracy 0.239000\n",
      "iteration 500 / 1500: loss 32.472119, accuracy 0.260000\n",
      "iteration 501 / 1500: loss 32.262596, accuracy 0.279000\n",
      "iteration 502 / 1500: loss 32.132975, accuracy 0.256000\n",
      "iteration 503 / 1500: loss 31.917983, accuracy 0.264000\n",
      "iteration 504 / 1500: loss 31.761310, accuracy 0.254000\n",
      "iteration 505 / 1500: loss 31.599204, accuracy 0.251000\n",
      "iteration 506 / 1500: loss 31.353531, accuracy 0.284000\n",
      "iteration 507 / 1500: loss 31.229594, accuracy 0.248000\n",
      "iteration 508 / 1500: loss 30.996283, accuracy 0.295000\n",
      "iteration 509 / 1500: loss 30.860653, accuracy 0.252000\n",
      "iteration 510 / 1500: loss 30.692882, accuracy 0.266000\n",
      "iteration 511 / 1500: loss 30.516325, accuracy 0.258000\n",
      "iteration 512 / 1500: loss 30.341759, accuracy 0.275000\n",
      "iteration 513 / 1500: loss 30.139372, accuracy 0.289000\n",
      "iteration 514 / 1500: loss 30.006995, accuracy 0.248000\n",
      "iteration 515 / 1500: loss 29.850977, accuracy 0.256000\n",
      "iteration 516 / 1500: loss 29.664435, accuracy 0.245000\n",
      "iteration 517 / 1500: loss 29.498376, accuracy 0.276000\n",
      "iteration 518 / 1500: loss 29.339430, accuracy 0.263000\n",
      "iteration 519 / 1500: loss 29.208038, accuracy 0.252000\n",
      "iteration 520 / 1500: loss 29.024001, accuracy 0.276000\n",
      "iteration 521 / 1500: loss 28.852327, accuracy 0.263000\n",
      "iteration 522 / 1500: loss 28.724971, accuracy 0.246000\n",
      "iteration 523 / 1500: loss 28.541752, accuracy 0.251000\n",
      "iteration 524 / 1500: loss 28.355880, accuracy 0.292000\n",
      "iteration 525 / 1500: loss 28.229513, accuracy 0.254000\n",
      "iteration 526 / 1500: loss 28.061789, accuracy 0.262000\n",
      "iteration 527 / 1500: loss 27.900478, accuracy 0.240000\n",
      "iteration 528 / 1500: loss 27.718194, accuracy 0.276000\n",
      "iteration 529 / 1500: loss 27.592849, accuracy 0.268000\n",
      "iteration 530 / 1500: loss 27.435390, accuracy 0.265000\n",
      "iteration 531 / 1500: loss 27.295469, accuracy 0.254000\n",
      "iteration 532 / 1500: loss 27.117832, accuracy 0.272000\n",
      "iteration 533 / 1500: loss 26.975799, accuracy 0.267000\n",
      "iteration 534 / 1500: loss 26.834033, accuracy 0.271000\n",
      "iteration 535 / 1500: loss 26.685490, accuracy 0.266000\n",
      "iteration 536 / 1500: loss 26.580622, accuracy 0.249000\n",
      "iteration 537 / 1500: loss 26.382415, accuracy 0.268000\n",
      "iteration 538 / 1500: loss 26.240343, accuracy 0.275000\n",
      "iteration 539 / 1500: loss 26.141475, accuracy 0.255000\n",
      "iteration 540 / 1500: loss 25.933187, accuracy 0.275000\n",
      "iteration 541 / 1500: loss 25.822330, accuracy 0.254000\n",
      "iteration 542 / 1500: loss 25.643803, accuracy 0.303000\n",
      "iteration 543 / 1500: loss 25.557844, accuracy 0.267000\n",
      "iteration 544 / 1500: loss 25.390530, accuracy 0.282000\n",
      "iteration 545 / 1500: loss 25.262559, accuracy 0.272000\n",
      "iteration 546 / 1500: loss 25.117064, accuracy 0.287000\n",
      "iteration 547 / 1500: loss 24.978162, accuracy 0.281000\n",
      "iteration 548 / 1500: loss 24.828758, accuracy 0.272000\n",
      "iteration 549 / 1500: loss 24.695861, accuracy 0.265000\n",
      "iteration 550 / 1500: loss 24.520529, accuracy 0.290000\n",
      "iteration 551 / 1500: loss 24.423173, accuracy 0.279000\n",
      "iteration 552 / 1500: loss 24.286511, accuracy 0.253000\n",
      "iteration 553 / 1500: loss 24.150793, accuracy 0.286000\n",
      "iteration 554 / 1500: loss 24.031040, accuracy 0.268000\n",
      "iteration 555 / 1500: loss 23.864010, accuracy 0.294000\n",
      "iteration 556 / 1500: loss 23.719551, accuracy 0.293000\n",
      "iteration 557 / 1500: loss 23.643625, accuracy 0.258000\n",
      "iteration 558 / 1500: loss 23.485971, accuracy 0.295000\n",
      "iteration 559 / 1500: loss 23.359648, accuracy 0.285000\n",
      "iteration 560 / 1500: loss 23.264843, accuracy 0.255000\n",
      "iteration 561 / 1500: loss 23.082646, accuracy 0.303000\n",
      "iteration 562 / 1500: loss 22.970466, accuracy 0.279000\n",
      "iteration 563 / 1500: loss 22.870257, accuracy 0.264000\n",
      "iteration 564 / 1500: loss 22.729208, accuracy 0.275000\n",
      "iteration 565 / 1500: loss 22.598239, accuracy 0.295000\n",
      "iteration 566 / 1500: loss 22.468269, accuracy 0.289000\n",
      "iteration 567 / 1500: loss 22.366029, accuracy 0.282000\n",
      "iteration 568 / 1500: loss 22.250783, accuracy 0.280000\n",
      "iteration 569 / 1500: loss 22.131954, accuracy 0.272000\n",
      "iteration 570 / 1500: loss 22.042853, accuracy 0.270000\n",
      "iteration 571 / 1500: loss 21.886138, accuracy 0.281000\n",
      "iteration 572 / 1500: loss 21.764004, accuracy 0.271000\n",
      "iteration 573 / 1500: loss 21.624568, accuracy 0.294000\n",
      "iteration 574 / 1500: loss 21.510055, accuracy 0.290000\n",
      "iteration 575 / 1500: loss 21.431481, accuracy 0.260000\n",
      "iteration 576 / 1500: loss 21.265676, accuracy 0.284000\n",
      "iteration 577 / 1500: loss 21.171952, accuracy 0.287000\n",
      "iteration 578 / 1500: loss 21.076108, accuracy 0.287000\n",
      "iteration 579 / 1500: loss 20.929436, accuracy 0.301000\n",
      "iteration 580 / 1500: loss 20.807262, accuracy 0.282000\n",
      "iteration 581 / 1500: loss 20.738926, accuracy 0.267000\n",
      "iteration 582 / 1500: loss 20.631294, accuracy 0.263000\n",
      "iteration 583 / 1500: loss 20.496091, accuracy 0.273000\n",
      "iteration 584 / 1500: loss 20.394379, accuracy 0.292000\n",
      "iteration 585 / 1500: loss 20.298060, accuracy 0.277000\n",
      "iteration 586 / 1500: loss 20.172760, accuracy 0.290000\n",
      "iteration 587 / 1500: loss 20.035765, accuracy 0.294000\n",
      "iteration 588 / 1500: loss 19.954152, accuracy 0.294000\n",
      "iteration 589 / 1500: loss 19.860155, accuracy 0.261000\n",
      "iteration 590 / 1500: loss 19.707833, accuracy 0.303000\n",
      "iteration 591 / 1500: loss 19.635631, accuracy 0.259000\n",
      "iteration 592 / 1500: loss 19.521148, accuracy 0.288000\n",
      "iteration 593 / 1500: loss 19.429351, accuracy 0.270000\n",
      "iteration 594 / 1500: loss 19.303524, accuracy 0.309000\n",
      "iteration 595 / 1500: loss 19.239883, accuracy 0.272000\n",
      "iteration 596 / 1500: loss 19.131122, accuracy 0.272000\n",
      "iteration 597 / 1500: loss 19.000881, accuracy 0.299000\n",
      "iteration 598 / 1500: loss 18.925076, accuracy 0.264000\n",
      "iteration 599 / 1500: loss 18.805221, accuracy 0.274000\n",
      "iteration 600 / 1500: loss 18.725210, accuracy 0.294000\n",
      "iteration 601 / 1500: loss 18.603641, accuracy 0.269000\n",
      "iteration 602 / 1500: loss 18.498594, accuracy 0.286000\n",
      "iteration 603 / 1500: loss 18.409626, accuracy 0.274000\n",
      "iteration 604 / 1500: loss 18.332581, accuracy 0.276000\n",
      "iteration 605 / 1500: loss 18.231037, accuracy 0.263000\n",
      "iteration 606 / 1500: loss 18.101612, accuracy 0.303000\n",
      "iteration 607 / 1500: loss 17.995730, accuracy 0.307000\n",
      "iteration 608 / 1500: loss 17.937971, accuracy 0.282000\n",
      "iteration 609 / 1500: loss 17.816490, accuracy 0.289000\n",
      "iteration 610 / 1500: loss 17.718899, accuracy 0.286000\n",
      "iteration 611 / 1500: loss 17.666771, accuracy 0.274000\n",
      "iteration 612 / 1500: loss 17.519213, accuracy 0.288000\n",
      "iteration 613 / 1500: loss 17.441660, accuracy 0.304000\n",
      "iteration 614 / 1500: loss 17.335202, accuracy 0.308000\n",
      "iteration 615 / 1500: loss 17.263663, accuracy 0.287000\n",
      "iteration 616 / 1500: loss 17.175544, accuracy 0.271000\n",
      "iteration 617 / 1500: loss 17.108778, accuracy 0.270000\n",
      "iteration 618 / 1500: loss 16.989301, accuracy 0.284000\n",
      "iteration 619 / 1500: loss 16.889827, accuracy 0.292000\n",
      "iteration 620 / 1500: loss 16.817189, accuracy 0.297000\n",
      "iteration 621 / 1500: loss 16.736797, accuracy 0.288000\n",
      "iteration 622 / 1500: loss 16.644359, accuracy 0.298000\n",
      "iteration 623 / 1500: loss 16.548965, accuracy 0.286000\n",
      "iteration 624 / 1500: loss 16.484712, accuracy 0.289000\n",
      "iteration 625 / 1500: loss 16.367857, accuracy 0.309000\n",
      "iteration 626 / 1500: loss 16.293086, accuracy 0.295000\n",
      "iteration 627 / 1500: loss 16.200670, accuracy 0.280000\n",
      "iteration 628 / 1500: loss 16.100427, accuracy 0.295000\n",
      "iteration 629 / 1500: loss 16.047930, accuracy 0.282000\n",
      "iteration 630 / 1500: loss 15.937690, accuracy 0.296000\n",
      "iteration 631 / 1500: loss 15.875680, accuracy 0.287000\n",
      "iteration 632 / 1500: loss 15.784863, accuracy 0.303000\n",
      "iteration 633 / 1500: loss 15.728651, accuracy 0.271000\n",
      "iteration 634 / 1500: loss 15.605141, accuracy 0.304000\n",
      "iteration 635 / 1500: loss 15.545047, accuracy 0.291000\n",
      "iteration 636 / 1500: loss 15.435959, accuracy 0.313000\n",
      "iteration 637 / 1500: loss 15.394998, accuracy 0.285000\n",
      "iteration 638 / 1500: loss 15.332448, accuracy 0.262000\n",
      "iteration 639 / 1500: loss 15.206889, accuracy 0.310000\n",
      "iteration 640 / 1500: loss 15.121830, accuracy 0.298000\n",
      "iteration 641 / 1500: loss 15.057643, accuracy 0.312000\n",
      "iteration 642 / 1500: loss 14.995893, accuracy 0.285000\n",
      "iteration 643 / 1500: loss 14.906271, accuracy 0.309000\n",
      "iteration 644 / 1500: loss 14.829332, accuracy 0.281000\n",
      "iteration 645 / 1500: loss 14.810636, accuracy 0.265000\n",
      "iteration 646 / 1500: loss 14.678441, accuracy 0.291000\n",
      "iteration 647 / 1500: loss 14.577983, accuracy 0.302000\n",
      "iteration 648 / 1500: loss 14.542303, accuracy 0.285000\n",
      "iteration 649 / 1500: loss 14.455758, accuracy 0.294000\n",
      "iteration 650 / 1500: loss 14.365473, accuracy 0.304000\n",
      "iteration 651 / 1500: loss 14.287487, accuracy 0.290000\n",
      "iteration 652 / 1500: loss 14.253981, accuracy 0.287000\n",
      "iteration 653 / 1500: loss 14.142185, accuracy 0.305000\n",
      "iteration 654 / 1500: loss 14.054122, accuracy 0.309000\n",
      "iteration 655 / 1500: loss 14.008218, accuracy 0.296000\n",
      "iteration 656 / 1500: loss 13.920972, accuracy 0.316000\n",
      "iteration 657 / 1500: loss 13.850479, accuracy 0.293000\n",
      "iteration 658 / 1500: loss 13.795488, accuracy 0.291000\n",
      "iteration 659 / 1500: loss 13.710563, accuracy 0.303000\n",
      "iteration 660 / 1500: loss 13.631313, accuracy 0.322000\n",
      "iteration 661 / 1500: loss 13.574444, accuracy 0.312000\n",
      "iteration 662 / 1500: loss 13.510417, accuracy 0.294000\n",
      "iteration 663 / 1500: loss 13.445488, accuracy 0.302000\n",
      "iteration 664 / 1500: loss 13.364089, accuracy 0.300000\n",
      "iteration 665 / 1500: loss 13.299146, accuracy 0.317000\n",
      "iteration 666 / 1500: loss 13.229381, accuracy 0.291000\n",
      "iteration 667 / 1500: loss 13.217141, accuracy 0.275000\n",
      "iteration 668 / 1500: loss 13.116485, accuracy 0.298000\n",
      "iteration 669 / 1500: loss 13.069759, accuracy 0.289000\n",
      "iteration 670 / 1500: loss 12.948551, accuracy 0.305000\n",
      "iteration 671 / 1500: loss 12.933015, accuracy 0.284000\n",
      "iteration 672 / 1500: loss 12.891827, accuracy 0.269000\n",
      "iteration 673 / 1500: loss 12.769679, accuracy 0.304000\n",
      "iteration 674 / 1500: loss 12.702152, accuracy 0.315000\n",
      "iteration 675 / 1500: loss 12.650437, accuracy 0.285000\n",
      "iteration 676 / 1500: loss 12.603663, accuracy 0.296000\n",
      "iteration 677 / 1500: loss 12.513995, accuracy 0.317000\n",
      "iteration 678 / 1500: loss 12.494636, accuracy 0.285000\n",
      "iteration 679 / 1500: loss 12.403243, accuracy 0.299000\n",
      "iteration 680 / 1500: loss 12.353113, accuracy 0.289000\n",
      "iteration 681 / 1500: loss 12.277136, accuracy 0.314000\n",
      "iteration 682 / 1500: loss 12.222658, accuracy 0.293000\n",
      "iteration 683 / 1500: loss 12.170019, accuracy 0.313000\n",
      "iteration 684 / 1500: loss 12.091379, accuracy 0.322000\n",
      "iteration 685 / 1500: loss 12.007289, accuracy 0.311000\n",
      "iteration 686 / 1500: loss 12.003382, accuracy 0.263000\n",
      "iteration 687 / 1500: loss 11.927993, accuracy 0.290000\n",
      "iteration 688 / 1500: loss 11.834384, accuracy 0.296000\n",
      "iteration 689 / 1500: loss 11.837840, accuracy 0.288000\n",
      "iteration 690 / 1500: loss 11.730608, accuracy 0.300000\n",
      "iteration 691 / 1500: loss 11.703215, accuracy 0.295000\n",
      "iteration 692 / 1500: loss 11.593094, accuracy 0.328000\n",
      "iteration 693 / 1500: loss 11.553700, accuracy 0.306000\n",
      "iteration 694 / 1500: loss 11.485774, accuracy 0.290000\n",
      "iteration 695 / 1500: loss 11.410462, accuracy 0.331000\n",
      "iteration 696 / 1500: loss 11.407797, accuracy 0.291000\n",
      "iteration 697 / 1500: loss 11.325401, accuracy 0.311000\n",
      "iteration 698 / 1500: loss 11.284180, accuracy 0.293000\n",
      "iteration 699 / 1500: loss 11.193237, accuracy 0.326000\n",
      "iteration 700 / 1500: loss 11.172934, accuracy 0.322000\n",
      "iteration 701 / 1500: loss 11.125400, accuracy 0.314000\n",
      "iteration 702 / 1500: loss 11.063322, accuracy 0.300000\n",
      "iteration 703 / 1500: loss 11.011734, accuracy 0.311000\n",
      "iteration 704 / 1500: loss 10.973552, accuracy 0.307000\n",
      "iteration 705 / 1500: loss 10.884253, accuracy 0.307000\n",
      "iteration 706 / 1500: loss 10.829268, accuracy 0.325000\n",
      "iteration 707 / 1500: loss 10.781965, accuracy 0.284000\n",
      "iteration 708 / 1500: loss 10.723874, accuracy 0.303000\n",
      "iteration 709 / 1500: loss 10.702513, accuracy 0.290000\n",
      "iteration 710 / 1500: loss 10.614430, accuracy 0.302000\n",
      "iteration 711 / 1500: loss 10.566460, accuracy 0.321000\n",
      "iteration 712 / 1500: loss 10.526750, accuracy 0.313000\n",
      "iteration 713 / 1500: loss 10.499350, accuracy 0.305000\n",
      "iteration 714 / 1500: loss 10.427110, accuracy 0.315000\n",
      "iteration 715 / 1500: loss 10.385326, accuracy 0.317000\n",
      "iteration 716 / 1500: loss 10.336748, accuracy 0.318000\n",
      "iteration 717 / 1500: loss 10.319077, accuracy 0.287000\n",
      "iteration 718 / 1500: loss 10.243389, accuracy 0.300000\n",
      "iteration 719 / 1500: loss 10.204619, accuracy 0.301000\n",
      "iteration 720 / 1500: loss 10.147203, accuracy 0.300000\n",
      "iteration 721 / 1500: loss 10.087518, accuracy 0.320000\n",
      "iteration 722 / 1500: loss 10.053479, accuracy 0.318000\n",
      "iteration 723 / 1500: loss 10.007079, accuracy 0.295000\n",
      "iteration 724 / 1500: loss 9.929738, accuracy 0.328000\n",
      "iteration 725 / 1500: loss 9.897443, accuracy 0.319000\n",
      "iteration 726 / 1500: loss 9.893571, accuracy 0.271000\n",
      "iteration 727 / 1500: loss 9.798382, accuracy 0.300000\n",
      "iteration 728 / 1500: loss 9.748375, accuracy 0.293000\n",
      "iteration 729 / 1500: loss 9.687137, accuracy 0.330000\n",
      "iteration 730 / 1500: loss 9.667939, accuracy 0.301000\n",
      "iteration 731 / 1500: loss 9.605297, accuracy 0.321000\n",
      "iteration 732 / 1500: loss 9.584471, accuracy 0.319000\n",
      "iteration 733 / 1500: loss 9.521836, accuracy 0.323000\n",
      "iteration 734 / 1500: loss 9.448897, accuracy 0.329000\n",
      "iteration 735 / 1500: loss 9.437420, accuracy 0.305000\n",
      "iteration 736 / 1500: loss 9.406424, accuracy 0.316000\n",
      "iteration 737 / 1500: loss 9.367873, accuracy 0.293000\n",
      "iteration 738 / 1500: loss 9.298460, accuracy 0.326000\n",
      "iteration 739 / 1500: loss 9.272885, accuracy 0.305000\n",
      "iteration 740 / 1500: loss 9.181568, accuracy 0.332000\n",
      "iteration 741 / 1500: loss 9.181183, accuracy 0.309000\n",
      "iteration 742 / 1500: loss 9.104472, accuracy 0.327000\n",
      "iteration 743 / 1500: loss 9.098856, accuracy 0.305000\n",
      "iteration 744 / 1500: loss 9.053313, accuracy 0.291000\n",
      "iteration 745 / 1500: loss 9.011177, accuracy 0.305000\n",
      "iteration 746 / 1500: loss 8.978727, accuracy 0.299000\n",
      "iteration 747 / 1500: loss 8.923391, accuracy 0.310000\n",
      "iteration 748 / 1500: loss 8.903659, accuracy 0.291000\n",
      "iteration 749 / 1500: loss 8.863349, accuracy 0.298000\n",
      "iteration 750 / 1500: loss 8.787376, accuracy 0.329000\n",
      "iteration 751 / 1500: loss 8.793736, accuracy 0.289000\n",
      "iteration 752 / 1500: loss 8.718481, accuracy 0.311000\n",
      "iteration 753 / 1500: loss 8.673228, accuracy 0.322000\n",
      "iteration 754 / 1500: loss 8.662515, accuracy 0.280000\n",
      "iteration 755 / 1500: loss 8.604086, accuracy 0.292000\n",
      "iteration 756 / 1500: loss 8.543734, accuracy 0.334000\n",
      "iteration 757 / 1500: loss 8.540940, accuracy 0.316000\n",
      "iteration 758 / 1500: loss 8.465198, accuracy 0.323000\n",
      "iteration 759 / 1500: loss 8.438421, accuracy 0.305000\n",
      "iteration 760 / 1500: loss 8.419013, accuracy 0.311000\n",
      "iteration 761 / 1500: loss 8.361838, accuracy 0.307000\n",
      "iteration 762 / 1500: loss 8.346063, accuracy 0.299000\n",
      "iteration 763 / 1500: loss 8.272036, accuracy 0.313000\n",
      "iteration 764 / 1500: loss 8.252405, accuracy 0.307000\n",
      "iteration 765 / 1500: loss 8.208667, accuracy 0.331000\n",
      "iteration 766 / 1500: loss 8.190302, accuracy 0.284000\n",
      "iteration 767 / 1500: loss 8.171143, accuracy 0.283000\n",
      "iteration 768 / 1500: loss 8.131181, accuracy 0.284000\n",
      "iteration 769 / 1500: loss 8.070128, accuracy 0.313000\n",
      "iteration 770 / 1500: loss 8.007520, accuracy 0.322000\n",
      "iteration 771 / 1500: loss 7.975303, accuracy 0.341000\n",
      "iteration 772 / 1500: loss 7.983533, accuracy 0.294000\n",
      "iteration 773 / 1500: loss 7.955548, accuracy 0.313000\n",
      "iteration 774 / 1500: loss 7.870086, accuracy 0.363000\n",
      "iteration 775 / 1500: loss 7.830510, accuracy 0.346000\n",
      "iteration 776 / 1500: loss 7.820036, accuracy 0.315000\n",
      "iteration 777 / 1500: loss 7.786859, accuracy 0.311000\n",
      "iteration 778 / 1500: loss 7.782171, accuracy 0.309000\n",
      "iteration 779 / 1500: loss 7.705604, accuracy 0.334000\n",
      "iteration 780 / 1500: loss 7.652372, accuracy 0.331000\n",
      "iteration 781 / 1500: loss 7.673144, accuracy 0.285000\n",
      "iteration 782 / 1500: loss 7.654021, accuracy 0.304000\n",
      "iteration 783 / 1500: loss 7.580835, accuracy 0.306000\n",
      "iteration 784 / 1500: loss 7.560222, accuracy 0.313000\n",
      "iteration 785 / 1500: loss 7.516400, accuracy 0.310000\n",
      "iteration 786 / 1500: loss 7.489702, accuracy 0.315000\n",
      "iteration 787 / 1500: loss 7.470189, accuracy 0.320000\n",
      "iteration 788 / 1500: loss 7.437059, accuracy 0.306000\n",
      "iteration 789 / 1500: loss 7.385269, accuracy 0.303000\n",
      "iteration 790 / 1500: loss 7.337335, accuracy 0.340000\n",
      "iteration 791 / 1500: loss 7.324165, accuracy 0.311000\n",
      "iteration 792 / 1500: loss 7.327314, accuracy 0.294000\n",
      "iteration 793 / 1500: loss 7.266568, accuracy 0.320000\n",
      "iteration 794 / 1500: loss 7.220214, accuracy 0.342000\n",
      "iteration 795 / 1500: loss 7.201500, accuracy 0.316000\n",
      "iteration 796 / 1500: loss 7.164789, accuracy 0.300000\n",
      "iteration 797 / 1500: loss 7.160004, accuracy 0.303000\n",
      "iteration 798 / 1500: loss 7.101536, accuracy 0.311000\n",
      "iteration 799 / 1500: loss 7.096690, accuracy 0.313000\n",
      "iteration 800 / 1500: loss 7.080283, accuracy 0.290000\n",
      "iteration 801 / 1500: loss 7.032899, accuracy 0.305000\n",
      "iteration 802 / 1500: loss 7.000314, accuracy 0.319000\n",
      "iteration 803 / 1500: loss 6.945201, accuracy 0.341000\n",
      "iteration 804 / 1500: loss 6.908486, accuracy 0.336000\n",
      "iteration 805 / 1500: loss 6.907896, accuracy 0.332000\n",
      "iteration 806 / 1500: loss 6.892184, accuracy 0.336000\n",
      "iteration 807 / 1500: loss 6.840299, accuracy 0.327000\n",
      "iteration 808 / 1500: loss 6.823763, accuracy 0.301000\n",
      "iteration 809 / 1500: loss 6.790027, accuracy 0.326000\n",
      "iteration 810 / 1500: loss 6.761681, accuracy 0.294000\n",
      "iteration 811 / 1500: loss 6.741494, accuracy 0.296000\n",
      "iteration 812 / 1500: loss 6.703479, accuracy 0.311000\n",
      "iteration 813 / 1500: loss 6.666767, accuracy 0.317000\n",
      "iteration 814 / 1500: loss 6.646291, accuracy 0.307000\n",
      "iteration 815 / 1500: loss 6.618984, accuracy 0.294000\n",
      "iteration 816 / 1500: loss 6.599637, accuracy 0.317000\n",
      "iteration 817 / 1500: loss 6.567089, accuracy 0.309000\n",
      "iteration 818 / 1500: loss 6.529539, accuracy 0.319000\n",
      "iteration 819 / 1500: loss 6.527173, accuracy 0.317000\n",
      "iteration 820 / 1500: loss 6.473574, accuracy 0.323000\n",
      "iteration 821 / 1500: loss 6.450946, accuracy 0.315000\n",
      "iteration 822 / 1500: loss 6.434822, accuracy 0.306000\n",
      "iteration 823 / 1500: loss 6.396626, accuracy 0.323000\n",
      "iteration 824 / 1500: loss 6.381570, accuracy 0.295000\n",
      "iteration 825 / 1500: loss 6.344303, accuracy 0.316000\n",
      "iteration 826 / 1500: loss 6.329165, accuracy 0.319000\n",
      "iteration 827 / 1500: loss 6.300932, accuracy 0.311000\n",
      "iteration 828 / 1500: loss 6.250815, accuracy 0.343000\n",
      "iteration 829 / 1500: loss 6.259910, accuracy 0.315000\n",
      "iteration 830 / 1500: loss 6.240851, accuracy 0.310000\n",
      "iteration 831 / 1500: loss 6.233269, accuracy 0.301000\n",
      "iteration 832 / 1500: loss 6.157348, accuracy 0.321000\n",
      "iteration 833 / 1500: loss 6.158115, accuracy 0.320000\n",
      "iteration 834 / 1500: loss 6.150680, accuracy 0.305000\n",
      "iteration 835 / 1500: loss 6.073407, accuracy 0.336000\n",
      "iteration 836 / 1500: loss 6.079363, accuracy 0.313000\n",
      "iteration 837 / 1500: loss 6.078199, accuracy 0.315000\n",
      "iteration 838 / 1500: loss 6.041822, accuracy 0.308000\n",
      "iteration 839 / 1500: loss 6.002152, accuracy 0.313000\n",
      "iteration 840 / 1500: loss 5.984284, accuracy 0.321000\n",
      "iteration 841 / 1500: loss 5.958055, accuracy 0.321000\n",
      "iteration 842 / 1500: loss 5.910540, accuracy 0.338000\n",
      "iteration 843 / 1500: loss 5.879044, accuracy 0.327000\n",
      "iteration 844 / 1500: loss 5.878582, accuracy 0.303000\n",
      "iteration 845 / 1500: loss 5.884894, accuracy 0.315000\n",
      "iteration 846 / 1500: loss 5.857587, accuracy 0.298000\n",
      "iteration 847 / 1500: loss 5.814759, accuracy 0.317000\n",
      "iteration 848 / 1500: loss 5.790723, accuracy 0.333000\n",
      "iteration 849 / 1500: loss 5.783095, accuracy 0.311000\n",
      "iteration 850 / 1500: loss 5.747787, accuracy 0.325000\n",
      "iteration 851 / 1500: loss 5.701673, accuracy 0.350000\n",
      "iteration 852 / 1500: loss 5.696137, accuracy 0.342000\n",
      "iteration 853 / 1500: loss 5.685040, accuracy 0.336000\n",
      "iteration 854 / 1500: loss 5.662237, accuracy 0.309000\n",
      "iteration 855 / 1500: loss 5.629792, accuracy 0.352000\n",
      "iteration 856 / 1500: loss 5.598330, accuracy 0.341000\n",
      "iteration 857 / 1500: loss 5.600964, accuracy 0.315000\n",
      "iteration 858 / 1500: loss 5.596470, accuracy 0.327000\n",
      "iteration 859 / 1500: loss 5.573281, accuracy 0.306000\n",
      "iteration 860 / 1500: loss 5.542316, accuracy 0.315000\n",
      "iteration 861 / 1500: loss 5.520413, accuracy 0.330000\n",
      "iteration 862 / 1500: loss 5.507507, accuracy 0.316000\n",
      "iteration 863 / 1500: loss 5.501140, accuracy 0.322000\n",
      "iteration 864 / 1500: loss 5.460869, accuracy 0.339000\n",
      "iteration 865 / 1500: loss 5.443964, accuracy 0.314000\n",
      "iteration 866 / 1500: loss 5.425306, accuracy 0.308000\n",
      "iteration 867 / 1500: loss 5.411139, accuracy 0.321000\n",
      "iteration 868 / 1500: loss 5.363558, accuracy 0.330000\n",
      "iteration 869 / 1500: loss 5.371101, accuracy 0.310000\n",
      "iteration 870 / 1500: loss 5.338749, accuracy 0.318000\n",
      "iteration 871 / 1500: loss 5.318379, accuracy 0.302000\n",
      "iteration 872 / 1500: loss 5.290962, accuracy 0.311000\n",
      "iteration 873 / 1500: loss 5.248795, accuracy 0.351000\n",
      "iteration 874 / 1500: loss 5.251220, accuracy 0.331000\n",
      "iteration 875 / 1500: loss 5.230983, accuracy 0.334000\n",
      "iteration 876 / 1500: loss 5.237426, accuracy 0.311000\n",
      "iteration 877 / 1500: loss 5.222547, accuracy 0.315000\n",
      "iteration 878 / 1500: loss 5.186063, accuracy 0.323000\n",
      "iteration 879 / 1500: loss 5.162613, accuracy 0.341000\n",
      "iteration 880 / 1500: loss 5.111275, accuracy 0.351000\n",
      "iteration 881 / 1500: loss 5.121896, accuracy 0.328000\n",
      "iteration 882 / 1500: loss 5.105061, accuracy 0.309000\n",
      "iteration 883 / 1500: loss 5.113090, accuracy 0.291000\n",
      "iteration 884 / 1500: loss 5.043309, accuracy 0.323000\n",
      "iteration 885 / 1500: loss 5.043385, accuracy 0.337000\n",
      "iteration 886 / 1500: loss 5.023599, accuracy 0.338000\n",
      "iteration 887 / 1500: loss 5.014267, accuracy 0.317000\n",
      "iteration 888 / 1500: loss 4.996767, accuracy 0.332000\n",
      "iteration 889 / 1500: loss 4.979335, accuracy 0.334000\n",
      "iteration 890 / 1500: loss 4.992086, accuracy 0.308000\n",
      "iteration 891 / 1500: loss 4.944756, accuracy 0.325000\n",
      "iteration 892 / 1500: loss 4.931770, accuracy 0.314000\n",
      "iteration 893 / 1500: loss 4.906833, accuracy 0.322000\n",
      "iteration 894 / 1500: loss 4.871909, accuracy 0.346000\n",
      "iteration 895 / 1500: loss 4.855237, accuracy 0.341000\n",
      "iteration 896 / 1500: loss 4.871164, accuracy 0.336000\n",
      "iteration 897 / 1500: loss 4.822531, accuracy 0.347000\n",
      "iteration 898 / 1500: loss 4.850777, accuracy 0.300000\n",
      "iteration 899 / 1500: loss 4.831941, accuracy 0.313000\n",
      "iteration 900 / 1500: loss 4.759355, accuracy 0.337000\n",
      "iteration 901 / 1500: loss 4.774150, accuracy 0.313000\n",
      "iteration 902 / 1500: loss 4.763607, accuracy 0.306000\n",
      "iteration 903 / 1500: loss 4.745944, accuracy 0.341000\n",
      "iteration 904 / 1500: loss 4.734294, accuracy 0.329000\n",
      "iteration 905 / 1500: loss 4.716623, accuracy 0.338000\n",
      "iteration 906 / 1500: loss 4.675556, accuracy 0.319000\n",
      "iteration 907 / 1500: loss 4.685500, accuracy 0.330000\n",
      "iteration 908 / 1500: loss 4.667304, accuracy 0.313000\n",
      "iteration 909 / 1500: loss 4.659160, accuracy 0.317000\n",
      "iteration 910 / 1500: loss 4.622764, accuracy 0.322000\n",
      "iteration 911 / 1500: loss 4.610921, accuracy 0.333000\n",
      "iteration 912 / 1500: loss 4.593170, accuracy 0.321000\n",
      "iteration 913 / 1500: loss 4.557958, accuracy 0.353000\n",
      "iteration 914 / 1500: loss 4.576479, accuracy 0.342000\n",
      "iteration 915 / 1500: loss 4.560523, accuracy 0.337000\n",
      "iteration 916 / 1500: loss 4.535946, accuracy 0.317000\n",
      "iteration 917 / 1500: loss 4.514833, accuracy 0.317000\n",
      "iteration 918 / 1500: loss 4.526577, accuracy 0.322000\n",
      "iteration 919 / 1500: loss 4.480321, accuracy 0.364000\n",
      "iteration 920 / 1500: loss 4.476625, accuracy 0.333000\n",
      "iteration 921 / 1500: loss 4.491599, accuracy 0.286000\n",
      "iteration 922 / 1500: loss 4.413051, accuracy 0.346000\n",
      "iteration 923 / 1500: loss 4.461687, accuracy 0.286000\n",
      "iteration 924 / 1500: loss 4.454157, accuracy 0.290000\n",
      "iteration 925 / 1500: loss 4.427854, accuracy 0.312000\n",
      "iteration 926 / 1500: loss 4.413712, accuracy 0.300000\n",
      "iteration 927 / 1500: loss 4.386973, accuracy 0.311000\n",
      "iteration 928 / 1500: loss 4.375309, accuracy 0.328000\n",
      "iteration 929 / 1500: loss 4.353894, accuracy 0.343000\n",
      "iteration 930 / 1500: loss 4.338399, accuracy 0.342000\n",
      "iteration 931 / 1500: loss 4.324749, accuracy 0.321000\n",
      "iteration 932 / 1500: loss 4.328170, accuracy 0.322000\n",
      "iteration 933 / 1500: loss 4.346228, accuracy 0.299000\n",
      "iteration 934 / 1500: loss 4.315466, accuracy 0.309000\n",
      "iteration 935 / 1500: loss 4.264812, accuracy 0.330000\n",
      "iteration 936 / 1500: loss 4.231108, accuracy 0.348000\n",
      "iteration 937 / 1500: loss 4.271575, accuracy 0.299000\n",
      "iteration 938 / 1500: loss 4.250137, accuracy 0.332000\n",
      "iteration 939 / 1500: loss 4.245421, accuracy 0.327000\n",
      "iteration 940 / 1500: loss 4.224902, accuracy 0.330000\n",
      "iteration 941 / 1500: loss 4.197173, accuracy 0.324000\n",
      "iteration 942 / 1500: loss 4.196099, accuracy 0.319000\n",
      "iteration 943 / 1500: loss 4.195344, accuracy 0.313000\n",
      "iteration 944 / 1500: loss 4.165976, accuracy 0.332000\n",
      "iteration 945 / 1500: loss 4.165870, accuracy 0.309000\n",
      "iteration 946 / 1500: loss 4.111931, accuracy 0.346000\n",
      "iteration 947 / 1500: loss 4.144594, accuracy 0.310000\n",
      "iteration 948 / 1500: loss 4.118708, accuracy 0.326000\n",
      "iteration 949 / 1500: loss 4.105204, accuracy 0.323000\n",
      "iteration 950 / 1500: loss 4.098743, accuracy 0.313000\n",
      "iteration 951 / 1500: loss 4.059259, accuracy 0.349000\n",
      "iteration 952 / 1500: loss 4.064896, accuracy 0.331000\n",
      "iteration 953 / 1500: loss 4.062330, accuracy 0.303000\n",
      "iteration 954 / 1500: loss 4.016087, accuracy 0.368000\n",
      "iteration 955 / 1500: loss 4.030089, accuracy 0.332000\n",
      "iteration 956 / 1500: loss 4.054585, accuracy 0.291000\n",
      "iteration 957 / 1500: loss 3.990273, accuracy 0.324000\n",
      "iteration 958 / 1500: loss 3.991653, accuracy 0.323000\n",
      "iteration 959 / 1500: loss 3.970043, accuracy 0.338000\n",
      "iteration 960 / 1500: loss 3.948364, accuracy 0.334000\n",
      "iteration 961 / 1500: loss 3.949239, accuracy 0.319000\n",
      "iteration 962 / 1500: loss 3.941033, accuracy 0.344000\n",
      "iteration 963 / 1500: loss 3.926616, accuracy 0.337000\n",
      "iteration 964 / 1500: loss 3.906241, accuracy 0.339000\n",
      "iteration 965 / 1500: loss 3.921659, accuracy 0.333000\n",
      "iteration 966 / 1500: loss 3.878735, accuracy 0.356000\n",
      "iteration 967 / 1500: loss 3.877591, accuracy 0.330000\n",
      "iteration 968 / 1500: loss 3.896925, accuracy 0.320000\n",
      "iteration 969 / 1500: loss 3.894484, accuracy 0.304000\n",
      "iteration 970 / 1500: loss 3.871482, accuracy 0.303000\n",
      "iteration 971 / 1500: loss 3.828115, accuracy 0.318000\n",
      "iteration 972 / 1500: loss 3.862858, accuracy 0.323000\n",
      "iteration 973 / 1500: loss 3.831617, accuracy 0.333000\n",
      "iteration 974 / 1500: loss 3.805551, accuracy 0.322000\n",
      "iteration 975 / 1500: loss 3.808358, accuracy 0.331000\n",
      "iteration 976 / 1500: loss 3.800136, accuracy 0.347000\n",
      "iteration 977 / 1500: loss 3.792286, accuracy 0.324000\n",
      "iteration 978 / 1500: loss 3.775144, accuracy 0.320000\n",
      "iteration 979 / 1500: loss 3.777990, accuracy 0.332000\n",
      "iteration 980 / 1500: loss 3.745069, accuracy 0.311000\n",
      "iteration 981 / 1500: loss 3.707617, accuracy 0.353000\n",
      "iteration 982 / 1500: loss 3.725933, accuracy 0.336000\n",
      "iteration 983 / 1500: loss 3.730981, accuracy 0.298000\n",
      "iteration 984 / 1500: loss 3.706021, accuracy 0.315000\n",
      "iteration 985 / 1500: loss 3.732899, accuracy 0.305000\n",
      "iteration 986 / 1500: loss 3.698741, accuracy 0.317000\n",
      "iteration 987 / 1500: loss 3.685257, accuracy 0.325000\n",
      "iteration 988 / 1500: loss 3.667932, accuracy 0.331000\n",
      "iteration 989 / 1500: loss 3.629723, accuracy 0.342000\n",
      "iteration 990 / 1500: loss 3.690980, accuracy 0.313000\n",
      "iteration 991 / 1500: loss 3.639714, accuracy 0.314000\n",
      "iteration 992 / 1500: loss 3.620231, accuracy 0.347000\n",
      "iteration 993 / 1500: loss 3.599483, accuracy 0.330000\n",
      "iteration 994 / 1500: loss 3.617999, accuracy 0.322000\n",
      "iteration 995 / 1500: loss 3.599931, accuracy 0.333000\n",
      "iteration 996 / 1500: loss 3.630021, accuracy 0.330000\n",
      "iteration 997 / 1500: loss 3.613461, accuracy 0.350000\n",
      "iteration 998 / 1500: loss 3.589088, accuracy 0.303000\n",
      "iteration 999 / 1500: loss 3.527678, accuracy 0.356000\n",
      "iteration 1000 / 1500: loss 3.577035, accuracy 0.331000\n",
      "iteration 1001 / 1500: loss 3.532888, accuracy 0.335000\n",
      "iteration 1002 / 1500: loss 3.533458, accuracy 0.332000\n",
      "iteration 1003 / 1500: loss 3.544384, accuracy 0.345000\n",
      "iteration 1004 / 1500: loss 3.499203, accuracy 0.330000\n",
      "iteration 1005 / 1500: loss 3.503628, accuracy 0.346000\n",
      "iteration 1006 / 1500: loss 3.498635, accuracy 0.345000\n",
      "iteration 1007 / 1500: loss 3.504152, accuracy 0.321000\n",
      "iteration 1008 / 1500: loss 3.506216, accuracy 0.323000\n",
      "iteration 1009 / 1500: loss 3.505935, accuracy 0.310000\n",
      "iteration 1010 / 1500: loss 3.447088, accuracy 0.341000\n",
      "iteration 1011 / 1500: loss 3.456358, accuracy 0.328000\n",
      "iteration 1012 / 1500: loss 3.445061, accuracy 0.336000\n",
      "iteration 1013 / 1500: loss 3.457456, accuracy 0.306000\n",
      "iteration 1014 / 1500: loss 3.428882, accuracy 0.345000\n",
      "iteration 1015 / 1500: loss 3.432331, accuracy 0.328000\n",
      "iteration 1016 / 1500: loss 3.422613, accuracy 0.332000\n",
      "iteration 1017 / 1500: loss 3.449073, accuracy 0.304000\n",
      "iteration 1018 / 1500: loss 3.420800, accuracy 0.327000\n",
      "iteration 1019 / 1500: loss 3.412254, accuracy 0.302000\n",
      "iteration 1020 / 1500: loss 3.382715, accuracy 0.335000\n",
      "iteration 1021 / 1500: loss 3.373725, accuracy 0.341000\n",
      "iteration 1022 / 1500: loss 3.353505, accuracy 0.345000\n",
      "iteration 1023 / 1500: loss 3.332628, accuracy 0.346000\n",
      "iteration 1024 / 1500: loss 3.351794, accuracy 0.334000\n",
      "iteration 1025 / 1500: loss 3.352829, accuracy 0.326000\n",
      "iteration 1026 / 1500: loss 3.341908, accuracy 0.333000\n",
      "iteration 1027 / 1500: loss 3.353815, accuracy 0.336000\n",
      "iteration 1028 / 1500: loss 3.306748, accuracy 0.336000\n",
      "iteration 1029 / 1500: loss 3.310702, accuracy 0.340000\n",
      "iteration 1030 / 1500: loss 3.321192, accuracy 0.319000\n",
      "iteration 1031 / 1500: loss 3.316144, accuracy 0.304000\n",
      "iteration 1032 / 1500: loss 3.301168, accuracy 0.335000\n",
      "iteration 1033 / 1500: loss 3.285158, accuracy 0.334000\n",
      "iteration 1034 / 1500: loss 3.256781, accuracy 0.335000\n",
      "iteration 1035 / 1500: loss 3.270491, accuracy 0.320000\n",
      "iteration 1036 / 1500: loss 3.266686, accuracy 0.326000\n",
      "iteration 1037 / 1500: loss 3.264361, accuracy 0.354000\n",
      "iteration 1038 / 1500: loss 3.259486, accuracy 0.317000\n",
      "iteration 1039 / 1500: loss 3.233515, accuracy 0.334000\n",
      "iteration 1040 / 1500: loss 3.236914, accuracy 0.335000\n",
      "iteration 1041 / 1500: loss 3.228725, accuracy 0.342000\n",
      "iteration 1042 / 1500: loss 3.229805, accuracy 0.336000\n",
      "iteration 1043 / 1500: loss 3.220006, accuracy 0.355000\n",
      "iteration 1044 / 1500: loss 3.223978, accuracy 0.311000\n",
      "iteration 1045 / 1500: loss 3.183371, accuracy 0.346000\n",
      "iteration 1046 / 1500: loss 3.195066, accuracy 0.340000\n",
      "iteration 1047 / 1500: loss 3.193936, accuracy 0.320000\n",
      "iteration 1048 / 1500: loss 3.193058, accuracy 0.329000\n",
      "iteration 1049 / 1500: loss 3.161571, accuracy 0.330000\n",
      "iteration 1050 / 1500: loss 3.148073, accuracy 0.361000\n",
      "iteration 1051 / 1500: loss 3.150371, accuracy 0.336000\n",
      "iteration 1052 / 1500: loss 3.174728, accuracy 0.346000\n",
      "iteration 1053 / 1500: loss 3.131033, accuracy 0.328000\n",
      "iteration 1054 / 1500: loss 3.165224, accuracy 0.310000\n",
      "iteration 1055 / 1500: loss 3.140311, accuracy 0.332000\n",
      "iteration 1056 / 1500: loss 3.132186, accuracy 0.335000\n",
      "iteration 1057 / 1500: loss 3.121405, accuracy 0.319000\n",
      "iteration 1058 / 1500: loss 3.133307, accuracy 0.324000\n",
      "iteration 1059 / 1500: loss 3.110977, accuracy 0.338000\n",
      "iteration 1060 / 1500: loss 3.110362, accuracy 0.347000\n",
      "iteration 1061 / 1500: loss 3.099471, accuracy 0.335000\n",
      "iteration 1062 / 1500: loss 3.111481, accuracy 0.287000\n",
      "iteration 1063 / 1500: loss 3.070069, accuracy 0.354000\n",
      "iteration 1064 / 1500: loss 3.083258, accuracy 0.330000\n",
      "iteration 1065 / 1500: loss 3.065399, accuracy 0.335000\n",
      "iteration 1066 / 1500: loss 3.056149, accuracy 0.332000\n",
      "iteration 1067 / 1500: loss 3.074784, accuracy 0.319000\n",
      "iteration 1068 / 1500: loss 3.070396, accuracy 0.318000\n",
      "iteration 1069 / 1500: loss 3.055097, accuracy 0.309000\n",
      "iteration 1070 / 1500: loss 3.030722, accuracy 0.343000\n",
      "iteration 1071 / 1500: loss 3.013983, accuracy 0.350000\n",
      "iteration 1072 / 1500: loss 3.068478, accuracy 0.281000\n",
      "iteration 1073 / 1500: loss 3.023341, accuracy 0.339000\n",
      "iteration 1074 / 1500: loss 3.048642, accuracy 0.331000\n",
      "iteration 1075 / 1500: loss 3.020119, accuracy 0.322000\n",
      "iteration 1076 / 1500: loss 3.006912, accuracy 0.304000\n",
      "iteration 1077 / 1500: loss 2.994191, accuracy 0.337000\n",
      "iteration 1078 / 1500: loss 2.970336, accuracy 0.346000\n",
      "iteration 1079 / 1500: loss 2.989068, accuracy 0.356000\n",
      "iteration 1080 / 1500: loss 2.977578, accuracy 0.333000\n",
      "iteration 1081 / 1500: loss 2.976646, accuracy 0.329000\n",
      "iteration 1082 / 1500: loss 2.959507, accuracy 0.318000\n",
      "iteration 1083 / 1500: loss 2.923820, accuracy 0.352000\n",
      "iteration 1084 / 1500: loss 2.959900, accuracy 0.336000\n",
      "iteration 1085 / 1500: loss 2.947626, accuracy 0.363000\n",
      "iteration 1086 / 1500: loss 2.944401, accuracy 0.352000\n",
      "iteration 1087 / 1500: loss 2.948894, accuracy 0.335000\n",
      "iteration 1088 / 1500: loss 2.951396, accuracy 0.335000\n",
      "iteration 1089 / 1500: loss 2.937818, accuracy 0.322000\n",
      "iteration 1090 / 1500: loss 2.957282, accuracy 0.322000\n",
      "iteration 1091 / 1500: loss 2.893525, accuracy 0.356000\n",
      "iteration 1092 / 1500: loss 2.918072, accuracy 0.340000\n",
      "iteration 1093 / 1500: loss 2.919602, accuracy 0.335000\n",
      "iteration 1094 / 1500: loss 2.938101, accuracy 0.318000\n",
      "iteration 1095 / 1500: loss 2.916641, accuracy 0.318000\n",
      "iteration 1096 / 1500: loss 2.907065, accuracy 0.330000\n",
      "iteration 1097 / 1500: loss 2.898549, accuracy 0.334000\n",
      "iteration 1098 / 1500: loss 2.884242, accuracy 0.338000\n",
      "iteration 1099 / 1500: loss 2.885248, accuracy 0.340000\n",
      "iteration 1100 / 1500: loss 2.876248, accuracy 0.328000\n",
      "iteration 1101 / 1500: loss 2.859429, accuracy 0.330000\n",
      "iteration 1102 / 1500: loss 2.862401, accuracy 0.342000\n",
      "iteration 1103 / 1500: loss 2.882379, accuracy 0.317000\n",
      "iteration 1104 / 1500: loss 2.873595, accuracy 0.341000\n",
      "iteration 1105 / 1500: loss 2.867454, accuracy 0.330000\n",
      "iteration 1106 / 1500: loss 2.860479, accuracy 0.312000\n",
      "iteration 1107 / 1500: loss 2.858040, accuracy 0.323000\n",
      "iteration 1108 / 1500: loss 2.852833, accuracy 0.330000\n",
      "iteration 1109 / 1500: loss 2.851741, accuracy 0.322000\n",
      "iteration 1110 / 1500: loss 2.789272, accuracy 0.365000\n",
      "iteration 1111 / 1500: loss 2.838706, accuracy 0.324000\n",
      "iteration 1112 / 1500: loss 2.821477, accuracy 0.331000\n",
      "iteration 1113 / 1500: loss 2.831948, accuracy 0.318000\n",
      "iteration 1114 / 1500: loss 2.833304, accuracy 0.333000\n",
      "iteration 1115 / 1500: loss 2.793009, accuracy 0.337000\n",
      "iteration 1116 / 1500: loss 2.786609, accuracy 0.343000\n",
      "iteration 1117 / 1500: loss 2.793035, accuracy 0.340000\n",
      "iteration 1118 / 1500: loss 2.795234, accuracy 0.331000\n",
      "iteration 1119 / 1500: loss 2.801289, accuracy 0.329000\n",
      "iteration 1120 / 1500: loss 2.766693, accuracy 0.361000\n",
      "iteration 1121 / 1500: loss 2.787519, accuracy 0.317000\n",
      "iteration 1122 / 1500: loss 2.788850, accuracy 0.344000\n",
      "iteration 1123 / 1500: loss 2.791190, accuracy 0.327000\n",
      "iteration 1124 / 1500: loss 2.762265, accuracy 0.346000\n",
      "iteration 1125 / 1500: loss 2.761747, accuracy 0.337000\n",
      "iteration 1126 / 1500: loss 2.773651, accuracy 0.316000\n",
      "iteration 1127 / 1500: loss 2.777741, accuracy 0.330000\n",
      "iteration 1128 / 1500: loss 2.758403, accuracy 0.342000\n",
      "iteration 1129 / 1500: loss 2.706413, accuracy 0.380000\n",
      "iteration 1130 / 1500: loss 2.749676, accuracy 0.361000\n",
      "iteration 1131 / 1500: loss 2.734234, accuracy 0.343000\n",
      "iteration 1132 / 1500: loss 2.758166, accuracy 0.333000\n",
      "iteration 1133 / 1500: loss 2.739129, accuracy 0.333000\n",
      "iteration 1134 / 1500: loss 2.738843, accuracy 0.340000\n",
      "iteration 1135 / 1500: loss 2.745819, accuracy 0.314000\n",
      "iteration 1136 / 1500: loss 2.728265, accuracy 0.316000\n",
      "iteration 1137 / 1500: loss 2.703133, accuracy 0.338000\n",
      "iteration 1138 / 1500: loss 2.703620, accuracy 0.366000\n",
      "iteration 1139 / 1500: loss 2.719090, accuracy 0.324000\n",
      "iteration 1140 / 1500: loss 2.695016, accuracy 0.351000\n",
      "iteration 1141 / 1500: loss 2.688223, accuracy 0.322000\n",
      "iteration 1142 / 1500: loss 2.693028, accuracy 0.347000\n",
      "iteration 1143 / 1500: loss 2.698182, accuracy 0.322000\n",
      "iteration 1144 / 1500: loss 2.676470, accuracy 0.332000\n",
      "iteration 1145 / 1500: loss 2.711247, accuracy 0.336000\n",
      "iteration 1146 / 1500: loss 2.684223, accuracy 0.347000\n",
      "iteration 1147 / 1500: loss 2.653400, accuracy 0.356000\n",
      "iteration 1148 / 1500: loss 2.706497, accuracy 0.312000\n",
      "iteration 1149 / 1500: loss 2.669218, accuracy 0.326000\n",
      "iteration 1150 / 1500: loss 2.696902, accuracy 0.340000\n",
      "iteration 1151 / 1500: loss 2.650293, accuracy 0.347000\n",
      "iteration 1152 / 1500: loss 2.700400, accuracy 0.315000\n",
      "iteration 1153 / 1500: loss 2.679279, accuracy 0.321000\n",
      "iteration 1154 / 1500: loss 2.638792, accuracy 0.357000\n",
      "iteration 1155 / 1500: loss 2.670558, accuracy 0.315000\n",
      "iteration 1156 / 1500: loss 2.634002, accuracy 0.324000\n",
      "iteration 1157 / 1500: loss 2.629166, accuracy 0.333000\n",
      "iteration 1158 / 1500: loss 2.621746, accuracy 0.329000\n",
      "iteration 1159 / 1500: loss 2.668762, accuracy 0.321000\n",
      "iteration 1160 / 1500: loss 2.637458, accuracy 0.341000\n",
      "iteration 1161 / 1500: loss 2.657907, accuracy 0.309000\n",
      "iteration 1162 / 1500: loss 2.632345, accuracy 0.337000\n",
      "iteration 1163 / 1500: loss 2.638772, accuracy 0.327000\n",
      "iteration 1164 / 1500: loss 2.628942, accuracy 0.324000\n",
      "iteration 1165 / 1500: loss 2.614995, accuracy 0.336000\n",
      "iteration 1166 / 1500: loss 2.607509, accuracy 0.335000\n",
      "iteration 1167 / 1500: loss 2.612803, accuracy 0.322000\n",
      "iteration 1168 / 1500: loss 2.605169, accuracy 0.330000\n",
      "iteration 1169 / 1500: loss 2.569313, accuracy 0.372000\n",
      "iteration 1170 / 1500: loss 2.604626, accuracy 0.342000\n",
      "iteration 1171 / 1500: loss 2.587208, accuracy 0.337000\n",
      "iteration 1172 / 1500: loss 2.594764, accuracy 0.331000\n",
      "iteration 1173 / 1500: loss 2.617164, accuracy 0.318000\n",
      "iteration 1174 / 1500: loss 2.566554, accuracy 0.375000\n",
      "iteration 1175 / 1500: loss 2.579618, accuracy 0.351000\n",
      "iteration 1176 / 1500: loss 2.591803, accuracy 0.314000\n",
      "iteration 1177 / 1500: loss 2.580623, accuracy 0.351000\n",
      "iteration 1178 / 1500: loss 2.581527, accuracy 0.318000\n",
      "iteration 1179 / 1500: loss 2.576808, accuracy 0.309000\n",
      "iteration 1180 / 1500: loss 2.580639, accuracy 0.337000\n",
      "iteration 1181 / 1500: loss 2.573186, accuracy 0.322000\n",
      "iteration 1182 / 1500: loss 2.557480, accuracy 0.342000\n",
      "iteration 1183 / 1500: loss 2.555050, accuracy 0.350000\n",
      "iteration 1184 / 1500: loss 2.574759, accuracy 0.314000\n",
      "iteration 1185 / 1500: loss 2.543790, accuracy 0.349000\n",
      "iteration 1186 / 1500: loss 2.554748, accuracy 0.329000\n",
      "iteration 1187 / 1500: loss 2.536686, accuracy 0.341000\n",
      "iteration 1188 / 1500: loss 2.570824, accuracy 0.329000\n",
      "iteration 1189 / 1500: loss 2.527289, accuracy 0.347000\n",
      "iteration 1190 / 1500: loss 2.513783, accuracy 0.358000\n",
      "iteration 1191 / 1500: loss 2.533165, accuracy 0.349000\n",
      "iteration 1192 / 1500: loss 2.551123, accuracy 0.326000\n",
      "iteration 1193 / 1500: loss 2.544007, accuracy 0.345000\n",
      "iteration 1194 / 1500: loss 2.512924, accuracy 0.345000\n",
      "iteration 1195 / 1500: loss 2.531313, accuracy 0.326000\n",
      "iteration 1196 / 1500: loss 2.509871, accuracy 0.357000\n",
      "iteration 1197 / 1500: loss 2.510123, accuracy 0.351000\n",
      "iteration 1198 / 1500: loss 2.525987, accuracy 0.346000\n",
      "iteration 1199 / 1500: loss 2.519544, accuracy 0.323000\n",
      "iteration 1200 / 1500: loss 2.513479, accuracy 0.334000\n",
      "iteration 1201 / 1500: loss 2.516766, accuracy 0.332000\n",
      "iteration 1202 / 1500: loss 2.512393, accuracy 0.338000\n",
      "iteration 1203 / 1500: loss 2.516588, accuracy 0.335000\n",
      "iteration 1204 / 1500: loss 2.512683, accuracy 0.327000\n",
      "iteration 1205 / 1500: loss 2.492301, accuracy 0.353000\n",
      "iteration 1206 / 1500: loss 2.490153, accuracy 0.334000\n",
      "iteration 1207 / 1500: loss 2.489478, accuracy 0.329000\n",
      "iteration 1208 / 1500: loss 2.487772, accuracy 0.353000\n",
      "iteration 1209 / 1500: loss 2.515795, accuracy 0.320000\n",
      "iteration 1210 / 1500: loss 2.499026, accuracy 0.332000\n",
      "iteration 1211 / 1500: loss 2.482447, accuracy 0.334000\n",
      "iteration 1212 / 1500: loss 2.505000, accuracy 0.337000\n",
      "iteration 1213 / 1500: loss 2.478921, accuracy 0.334000\n",
      "iteration 1214 / 1500: loss 2.452626, accuracy 0.366000\n",
      "iteration 1215 / 1500: loss 2.456091, accuracy 0.372000\n",
      "iteration 1216 / 1500: loss 2.461524, accuracy 0.344000\n",
      "iteration 1217 / 1500: loss 2.469976, accuracy 0.314000\n",
      "iteration 1218 / 1500: loss 2.489846, accuracy 0.321000\n",
      "iteration 1219 / 1500: loss 2.478674, accuracy 0.321000\n",
      "iteration 1220 / 1500: loss 2.452380, accuracy 0.332000\n",
      "iteration 1221 / 1500: loss 2.469749, accuracy 0.310000\n",
      "iteration 1222 / 1500: loss 2.457261, accuracy 0.342000\n",
      "iteration 1223 / 1500: loss 2.495328, accuracy 0.312000\n",
      "iteration 1224 / 1500: loss 2.461739, accuracy 0.314000\n",
      "iteration 1225 / 1500: loss 2.447669, accuracy 0.338000\n",
      "iteration 1226 / 1500: loss 2.442877, accuracy 0.314000\n",
      "iteration 1227 / 1500: loss 2.442869, accuracy 0.346000\n",
      "iteration 1228 / 1500: loss 2.425338, accuracy 0.350000\n",
      "iteration 1229 / 1500: loss 2.452749, accuracy 0.309000\n",
      "iteration 1230 / 1500: loss 2.433636, accuracy 0.349000\n",
      "iteration 1231 / 1500: loss 2.416093, accuracy 0.359000\n",
      "iteration 1232 / 1500: loss 2.411919, accuracy 0.337000\n",
      "iteration 1233 / 1500: loss 2.440535, accuracy 0.330000\n",
      "iteration 1234 / 1500: loss 2.440848, accuracy 0.321000\n",
      "iteration 1235 / 1500: loss 2.415567, accuracy 0.331000\n",
      "iteration 1236 / 1500: loss 2.425642, accuracy 0.361000\n",
      "iteration 1237 / 1500: loss 2.410559, accuracy 0.346000\n",
      "iteration 1238 / 1500: loss 2.425722, accuracy 0.328000\n",
      "iteration 1239 / 1500: loss 2.433859, accuracy 0.328000\n",
      "iteration 1240 / 1500: loss 2.433247, accuracy 0.334000\n",
      "iteration 1241 / 1500: loss 2.414386, accuracy 0.332000\n",
      "iteration 1242 / 1500: loss 2.434199, accuracy 0.325000\n",
      "iteration 1243 / 1500: loss 2.408465, accuracy 0.355000\n",
      "iteration 1244 / 1500: loss 2.389804, accuracy 0.329000\n",
      "iteration 1245 / 1500: loss 2.393220, accuracy 0.328000\n",
      "iteration 1246 / 1500: loss 2.399236, accuracy 0.332000\n",
      "iteration 1247 / 1500: loss 2.382491, accuracy 0.343000\n",
      "iteration 1248 / 1500: loss 2.396893, accuracy 0.345000\n",
      "iteration 1249 / 1500: loss 2.389436, accuracy 0.342000\n",
      "iteration 1250 / 1500: loss 2.384160, accuracy 0.346000\n",
      "iteration 1251 / 1500: loss 2.392448, accuracy 0.344000\n",
      "iteration 1252 / 1500: loss 2.414492, accuracy 0.334000\n",
      "iteration 1253 / 1500: loss 2.403151, accuracy 0.338000\n",
      "iteration 1254 / 1500: loss 2.377210, accuracy 0.326000\n",
      "iteration 1255 / 1500: loss 2.374366, accuracy 0.348000\n",
      "iteration 1256 / 1500: loss 2.381615, accuracy 0.320000\n",
      "iteration 1257 / 1500: loss 2.353479, accuracy 0.365000\n",
      "iteration 1258 / 1500: loss 2.367651, accuracy 0.352000\n",
      "iteration 1259 / 1500: loss 2.372595, accuracy 0.340000\n",
      "iteration 1260 / 1500: loss 2.349215, accuracy 0.343000\n",
      "iteration 1261 / 1500: loss 2.379446, accuracy 0.322000\n",
      "iteration 1262 / 1500: loss 2.374686, accuracy 0.330000\n",
      "iteration 1263 / 1500: loss 2.374874, accuracy 0.331000\n",
      "iteration 1264 / 1500: loss 2.354131, accuracy 0.351000\n",
      "iteration 1265 / 1500: loss 2.347608, accuracy 0.345000\n",
      "iteration 1266 / 1500: loss 2.356550, accuracy 0.337000\n",
      "iteration 1267 / 1500: loss 2.388113, accuracy 0.313000\n",
      "iteration 1268 / 1500: loss 2.376040, accuracy 0.325000\n",
      "iteration 1269 / 1500: loss 2.333575, accuracy 0.334000\n",
      "iteration 1270 / 1500: loss 2.342378, accuracy 0.349000\n",
      "iteration 1271 / 1500: loss 2.353985, accuracy 0.328000\n",
      "iteration 1272 / 1500: loss 2.368405, accuracy 0.338000\n",
      "iteration 1273 / 1500: loss 2.346483, accuracy 0.349000\n",
      "iteration 1274 / 1500: loss 2.356161, accuracy 0.324000\n",
      "iteration 1275 / 1500: loss 2.336209, accuracy 0.339000\n",
      "iteration 1276 / 1500: loss 2.370787, accuracy 0.321000\n",
      "iteration 1277 / 1500: loss 2.373267, accuracy 0.321000\n",
      "iteration 1278 / 1500: loss 2.356806, accuracy 0.324000\n",
      "iteration 1279 / 1500: loss 2.387527, accuracy 0.315000\n",
      "iteration 1280 / 1500: loss 2.332860, accuracy 0.343000\n",
      "iteration 1281 / 1500: loss 2.341767, accuracy 0.315000\n",
      "iteration 1282 / 1500: loss 2.321660, accuracy 0.347000\n",
      "iteration 1283 / 1500: loss 2.330761, accuracy 0.332000\n",
      "iteration 1284 / 1500: loss 2.335374, accuracy 0.326000\n",
      "iteration 1285 / 1500: loss 2.324784, accuracy 0.323000\n",
      "iteration 1286 / 1500: loss 2.315525, accuracy 0.337000\n",
      "iteration 1287 / 1500: loss 2.341690, accuracy 0.320000\n",
      "iteration 1288 / 1500: loss 2.319346, accuracy 0.342000\n",
      "iteration 1289 / 1500: loss 2.326986, accuracy 0.342000\n",
      "iteration 1290 / 1500: loss 2.326736, accuracy 0.328000\n",
      "iteration 1291 / 1500: loss 2.333746, accuracy 0.344000\n",
      "iteration 1292 / 1500: loss 2.304107, accuracy 0.339000\n",
      "iteration 1293 / 1500: loss 2.308969, accuracy 0.341000\n",
      "iteration 1294 / 1500: loss 2.324735, accuracy 0.330000\n",
      "iteration 1295 / 1500: loss 2.317271, accuracy 0.344000\n",
      "iteration 1296 / 1500: loss 2.306711, accuracy 0.342000\n",
      "iteration 1297 / 1500: loss 2.328739, accuracy 0.332000\n",
      "iteration 1298 / 1500: loss 2.329117, accuracy 0.344000\n",
      "iteration 1299 / 1500: loss 2.311860, accuracy 0.308000\n",
      "iteration 1300 / 1500: loss 2.292547, accuracy 0.342000\n",
      "iteration 1301 / 1500: loss 2.327717, accuracy 0.324000\n",
      "iteration 1302 / 1500: loss 2.310302, accuracy 0.341000\n",
      "iteration 1303 / 1500: loss 2.291170, accuracy 0.350000\n",
      "iteration 1304 / 1500: loss 2.290658, accuracy 0.354000\n",
      "iteration 1305 / 1500: loss 2.292503, accuracy 0.350000\n",
      "iteration 1306 / 1500: loss 2.319793, accuracy 0.326000\n",
      "iteration 1307 / 1500: loss 2.292554, accuracy 0.368000\n",
      "iteration 1308 / 1500: loss 2.286403, accuracy 0.347000\n",
      "iteration 1309 / 1500: loss 2.283463, accuracy 0.340000\n",
      "iteration 1310 / 1500: loss 2.301161, accuracy 0.326000\n",
      "iteration 1311 / 1500: loss 2.276403, accuracy 0.369000\n",
      "iteration 1312 / 1500: loss 2.305252, accuracy 0.313000\n",
      "iteration 1313 / 1500: loss 2.299585, accuracy 0.349000\n",
      "iteration 1314 / 1500: loss 2.275440, accuracy 0.350000\n",
      "iteration 1315 / 1500: loss 2.280930, accuracy 0.327000\n",
      "iteration 1316 / 1500: loss 2.298924, accuracy 0.338000\n",
      "iteration 1317 / 1500: loss 2.293044, accuracy 0.335000\n",
      "iteration 1318 / 1500: loss 2.284066, accuracy 0.324000\n",
      "iteration 1319 / 1500: loss 2.283863, accuracy 0.323000\n",
      "iteration 1320 / 1500: loss 2.311419, accuracy 0.323000\n",
      "iteration 1321 / 1500: loss 2.286528, accuracy 0.329000\n",
      "iteration 1322 / 1500: loss 2.275882, accuracy 0.329000\n",
      "iteration 1323 / 1500: loss 2.286125, accuracy 0.327000\n",
      "iteration 1324 / 1500: loss 2.250531, accuracy 0.355000\n",
      "iteration 1325 / 1500: loss 2.297954, accuracy 0.329000\n",
      "iteration 1326 / 1500: loss 2.258146, accuracy 0.327000\n",
      "iteration 1327 / 1500: loss 2.287219, accuracy 0.320000\n",
      "iteration 1328 / 1500: loss 2.250732, accuracy 0.339000\n",
      "iteration 1329 / 1500: loss 2.284939, accuracy 0.331000\n",
      "iteration 1330 / 1500: loss 2.289785, accuracy 0.326000\n",
      "iteration 1331 / 1500: loss 2.251109, accuracy 0.373000\n",
      "iteration 1332 / 1500: loss 2.286143, accuracy 0.333000\n",
      "iteration 1333 / 1500: loss 2.279836, accuracy 0.327000\n",
      "iteration 1334 / 1500: loss 2.277665, accuracy 0.321000\n",
      "iteration 1335 / 1500: loss 2.260772, accuracy 0.336000\n",
      "iteration 1336 / 1500: loss 2.235751, accuracy 0.334000\n",
      "iteration 1337 / 1500: loss 2.257943, accuracy 0.334000\n",
      "iteration 1338 / 1500: loss 2.244832, accuracy 0.349000\n",
      "iteration 1339 / 1500: loss 2.288350, accuracy 0.340000\n",
      "iteration 1340 / 1500: loss 2.268238, accuracy 0.336000\n",
      "iteration 1341 / 1500: loss 2.268256, accuracy 0.312000\n",
      "iteration 1342 / 1500: loss 2.249662, accuracy 0.356000\n",
      "iteration 1343 / 1500: loss 2.244079, accuracy 0.342000\n",
      "iteration 1344 / 1500: loss 2.235458, accuracy 0.343000\n",
      "iteration 1345 / 1500: loss 2.233600, accuracy 0.342000\n",
      "iteration 1346 / 1500: loss 2.233232, accuracy 0.349000\n",
      "iteration 1347 / 1500: loss 2.256222, accuracy 0.336000\n",
      "iteration 1348 / 1500: loss 2.247567, accuracy 0.319000\n",
      "iteration 1349 / 1500: loss 2.239825, accuracy 0.337000\n",
      "iteration 1350 / 1500: loss 2.253526, accuracy 0.333000\n",
      "iteration 1351 / 1500: loss 2.249526, accuracy 0.340000\n",
      "iteration 1352 / 1500: loss 2.258779, accuracy 0.342000\n",
      "iteration 1353 / 1500: loss 2.253527, accuracy 0.323000\n",
      "iteration 1354 / 1500: loss 2.214092, accuracy 0.378000\n",
      "iteration 1355 / 1500: loss 2.215362, accuracy 0.346000\n",
      "iteration 1356 / 1500: loss 2.246768, accuracy 0.321000\n",
      "iteration 1357 / 1500: loss 2.219930, accuracy 0.353000\n",
      "iteration 1358 / 1500: loss 2.245941, accuracy 0.345000\n",
      "iteration 1359 / 1500: loss 2.238768, accuracy 0.336000\n",
      "iteration 1360 / 1500: loss 2.244426, accuracy 0.323000\n",
      "iteration 1361 / 1500: loss 2.214802, accuracy 0.332000\n",
      "iteration 1362 / 1500: loss 2.245778, accuracy 0.318000\n",
      "iteration 1363 / 1500: loss 2.216217, accuracy 0.332000\n",
      "iteration 1364 / 1500: loss 2.249685, accuracy 0.316000\n",
      "iteration 1365 / 1500: loss 2.259254, accuracy 0.298000\n",
      "iteration 1366 / 1500: loss 2.217102, accuracy 0.331000\n",
      "iteration 1367 / 1500: loss 2.198913, accuracy 0.345000\n",
      "iteration 1368 / 1500: loss 2.231488, accuracy 0.311000\n",
      "iteration 1369 / 1500: loss 2.243714, accuracy 0.322000\n",
      "iteration 1370 / 1500: loss 2.228876, accuracy 0.326000\n",
      "iteration 1371 / 1500: loss 2.243959, accuracy 0.322000\n",
      "iteration 1372 / 1500: loss 2.216362, accuracy 0.336000\n",
      "iteration 1373 / 1500: loss 2.201145, accuracy 0.337000\n",
      "iteration 1374 / 1500: loss 2.237691, accuracy 0.314000\n",
      "iteration 1375 / 1500: loss 2.216754, accuracy 0.349000\n",
      "iteration 1376 / 1500: loss 2.214538, accuracy 0.337000\n",
      "iteration 1377 / 1500: loss 2.221606, accuracy 0.347000\n",
      "iteration 1378 / 1500: loss 2.217508, accuracy 0.343000\n",
      "iteration 1379 / 1500: loss 2.220480, accuracy 0.342000\n",
      "iteration 1380 / 1500: loss 2.202748, accuracy 0.345000\n",
      "iteration 1381 / 1500: loss 2.205854, accuracy 0.339000\n",
      "iteration 1382 / 1500: loss 2.235723, accuracy 0.301000\n",
      "iteration 1383 / 1500: loss 2.201361, accuracy 0.348000\n",
      "iteration 1384 / 1500: loss 2.214011, accuracy 0.319000\n",
      "iteration 1385 / 1500: loss 2.207708, accuracy 0.336000\n",
      "iteration 1386 / 1500: loss 2.222348, accuracy 0.332000\n",
      "iteration 1387 / 1500: loss 2.220304, accuracy 0.327000\n",
      "iteration 1388 / 1500: loss 2.202679, accuracy 0.341000\n",
      "iteration 1389 / 1500: loss 2.208108, accuracy 0.312000\n",
      "iteration 1390 / 1500: loss 2.223817, accuracy 0.318000\n",
      "iteration 1391 / 1500: loss 2.213787, accuracy 0.333000\n",
      "iteration 1392 / 1500: loss 2.206647, accuracy 0.349000\n",
      "iteration 1393 / 1500: loss 2.197724, accuracy 0.314000\n",
      "iteration 1394 / 1500: loss 2.216388, accuracy 0.320000\n",
      "iteration 1395 / 1500: loss 2.210025, accuracy 0.334000\n",
      "iteration 1396 / 1500: loss 2.215782, accuracy 0.331000\n",
      "iteration 1397 / 1500: loss 2.199326, accuracy 0.331000\n",
      "iteration 1398 / 1500: loss 2.222462, accuracy 0.302000\n",
      "iteration 1399 / 1500: loss 2.193201, accuracy 0.346000\n",
      "iteration 1400 / 1500: loss 2.212207, accuracy 0.343000\n",
      "iteration 1401 / 1500: loss 2.177092, accuracy 0.368000\n",
      "iteration 1402 / 1500: loss 2.200689, accuracy 0.351000\n",
      "iteration 1403 / 1500: loss 2.186690, accuracy 0.343000\n",
      "iteration 1404 / 1500: loss 2.183921, accuracy 0.332000\n",
      "iteration 1405 / 1500: loss 2.187089, accuracy 0.329000\n",
      "iteration 1406 / 1500: loss 2.178507, accuracy 0.360000\n",
      "iteration 1407 / 1500: loss 2.179577, accuracy 0.357000\n",
      "iteration 1408 / 1500: loss 2.187555, accuracy 0.345000\n",
      "iteration 1409 / 1500: loss 2.205880, accuracy 0.312000\n",
      "iteration 1410 / 1500: loss 2.192463, accuracy 0.332000\n",
      "iteration 1411 / 1500: loss 2.204435, accuracy 0.330000\n",
      "iteration 1412 / 1500: loss 2.181817, accuracy 0.344000\n",
      "iteration 1413 / 1500: loss 2.183640, accuracy 0.326000\n",
      "iteration 1414 / 1500: loss 2.180309, accuracy 0.339000\n",
      "iteration 1415 / 1500: loss 2.188950, accuracy 0.319000\n",
      "iteration 1416 / 1500: loss 2.176944, accuracy 0.344000\n",
      "iteration 1417 / 1500: loss 2.179354, accuracy 0.332000\n",
      "iteration 1418 / 1500: loss 2.166583, accuracy 0.354000\n",
      "iteration 1419 / 1500: loss 2.198619, accuracy 0.350000\n",
      "iteration 1420 / 1500: loss 2.176445, accuracy 0.326000\n",
      "iteration 1421 / 1500: loss 2.185750, accuracy 0.327000\n",
      "iteration 1422 / 1500: loss 2.201677, accuracy 0.322000\n",
      "iteration 1423 / 1500: loss 2.173660, accuracy 0.362000\n",
      "iteration 1424 / 1500: loss 2.220655, accuracy 0.331000\n",
      "iteration 1425 / 1500: loss 2.166743, accuracy 0.345000\n",
      "iteration 1426 / 1500: loss 2.194478, accuracy 0.328000\n",
      "iteration 1427 / 1500: loss 2.168376, accuracy 0.345000\n",
      "iteration 1428 / 1500: loss 2.179716, accuracy 0.336000\n",
      "iteration 1429 / 1500: loss 2.173323, accuracy 0.322000\n",
      "iteration 1430 / 1500: loss 2.157909, accuracy 0.356000\n",
      "iteration 1431 / 1500: loss 2.160955, accuracy 0.343000\n",
      "iteration 1432 / 1500: loss 2.151464, accuracy 0.362000\n",
      "iteration 1433 / 1500: loss 2.165428, accuracy 0.360000\n",
      "iteration 1434 / 1500: loss 2.189108, accuracy 0.334000\n",
      "iteration 1435 / 1500: loss 2.173062, accuracy 0.320000\n",
      "iteration 1436 / 1500: loss 2.162990, accuracy 0.333000\n",
      "iteration 1437 / 1500: loss 2.168557, accuracy 0.327000\n",
      "iteration 1438 / 1500: loss 2.185583, accuracy 0.326000\n",
      "iteration 1439 / 1500: loss 2.167431, accuracy 0.361000\n",
      "iteration 1440 / 1500: loss 2.198811, accuracy 0.324000\n",
      "iteration 1441 / 1500: loss 2.185037, accuracy 0.326000\n",
      "iteration 1442 / 1500: loss 2.148005, accuracy 0.344000\n",
      "iteration 1443 / 1500: loss 2.152152, accuracy 0.348000\n",
      "iteration 1444 / 1500: loss 2.175402, accuracy 0.323000\n",
      "iteration 1445 / 1500: loss 2.171811, accuracy 0.322000\n",
      "iteration 1446 / 1500: loss 2.166308, accuracy 0.313000\n",
      "iteration 1447 / 1500: loss 2.159480, accuracy 0.347000\n",
      "iteration 1448 / 1500: loss 2.169115, accuracy 0.324000\n",
      "iteration 1449 / 1500: loss 2.164364, accuracy 0.348000\n",
      "iteration 1450 / 1500: loss 2.221322, accuracy 0.288000\n",
      "iteration 1451 / 1500: loss 2.167652, accuracy 0.343000\n",
      "iteration 1452 / 1500: loss 2.149482, accuracy 0.337000\n",
      "iteration 1453 / 1500: loss 2.135691, accuracy 0.340000\n",
      "iteration 1454 / 1500: loss 2.148841, accuracy 0.354000\n",
      "iteration 1455 / 1500: loss 2.167409, accuracy 0.343000\n",
      "iteration 1456 / 1500: loss 2.160893, accuracy 0.335000\n",
      "iteration 1457 / 1500: loss 2.168754, accuracy 0.352000\n",
      "iteration 1458 / 1500: loss 2.172444, accuracy 0.312000\n",
      "iteration 1459 / 1500: loss 2.170633, accuracy 0.319000\n",
      "iteration 1460 / 1500: loss 2.151871, accuracy 0.344000\n",
      "iteration 1461 / 1500: loss 2.148374, accuracy 0.336000\n",
      "iteration 1462 / 1500: loss 2.145762, accuracy 0.333000\n",
      "iteration 1463 / 1500: loss 2.167659, accuracy 0.351000\n",
      "iteration 1464 / 1500: loss 2.157224, accuracy 0.323000\n",
      "iteration 1465 / 1500: loss 2.132132, accuracy 0.358000\n",
      "iteration 1466 / 1500: loss 2.138746, accuracy 0.336000\n",
      "iteration 1467 / 1500: loss 2.174216, accuracy 0.331000\n",
      "iteration 1468 / 1500: loss 2.133208, accuracy 0.323000\n",
      "iteration 1469 / 1500: loss 2.160106, accuracy 0.335000\n",
      "iteration 1470 / 1500: loss 2.126329, accuracy 0.359000\n",
      "iteration 1471 / 1500: loss 2.175049, accuracy 0.326000\n",
      "iteration 1472 / 1500: loss 2.153110, accuracy 0.328000\n",
      "iteration 1473 / 1500: loss 2.183745, accuracy 0.329000\n",
      "iteration 1474 / 1500: loss 2.129636, accuracy 0.350000\n",
      "iteration 1475 / 1500: loss 2.157500, accuracy 0.323000\n",
      "iteration 1476 / 1500: loss 2.156621, accuracy 0.321000\n",
      "iteration 1477 / 1500: loss 2.137765, accuracy 0.334000\n",
      "iteration 1478 / 1500: loss 2.119989, accuracy 0.347000\n",
      "iteration 1479 / 1500: loss 2.156470, accuracy 0.326000\n",
      "iteration 1480 / 1500: loss 2.134744, accuracy 0.345000\n",
      "iteration 1481 / 1500: loss 2.130813, accuracy 0.355000\n",
      "iteration 1482 / 1500: loss 2.138823, accuracy 0.349000\n",
      "iteration 1483 / 1500: loss 2.163073, accuracy 0.314000\n",
      "iteration 1484 / 1500: loss 2.141309, accuracy 0.323000\n",
      "iteration 1485 / 1500: loss 2.143996, accuracy 0.319000\n",
      "iteration 1486 / 1500: loss 2.149646, accuracy 0.327000\n",
      "iteration 1487 / 1500: loss 2.145291, accuracy 0.335000\n",
      "iteration 1488 / 1500: loss 2.108686, accuracy 0.358000\n",
      "iteration 1489 / 1500: loss 2.126629, accuracy 0.353000\n",
      "iteration 1490 / 1500: loss 2.154753, accuracy 0.333000\n",
      "iteration 1491 / 1500: loss 2.144856, accuracy 0.310000\n",
      "iteration 1492 / 1500: loss 2.153113, accuracy 0.324000\n",
      "iteration 1493 / 1500: loss 2.095873, accuracy 0.363000\n",
      "iteration 1494 / 1500: loss 2.145400, accuracy 0.339000\n",
      "iteration 1495 / 1500: loss 2.140091, accuracy 0.325000\n",
      "iteration 1496 / 1500: loss 2.135492, accuracy 0.331000\n",
      "iteration 1497 / 1500: loss 2.166853, accuracy 0.309000\n",
      "iteration 1498 / 1500: loss 2.146793, accuracy 0.345000\n",
      "iteration 1499 / 1500: loss 2.137842, accuracy 0.344000\n",
      "lr 7.500000e-08 reg 3.250000e+04 train accuracy: 0.340469 val accuracy: 0.353000\n",
      "lr 7.500000e-08 reg 4.000000e+04 train accuracy: 0.335571 val accuracy: 0.357000\n",
      "lr 1.250000e-07 reg 3.750000e+04 train accuracy: 0.339571 val accuracy: 0.354000\n",
      "lr 1.500000e-07 reg 3.500000e+04 train accuracy: 0.340265 val accuracy: 0.353000\n",
      "lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.347510 val accuracy: 0.356000\n",
      "best validation accuracy achieved during cross-validation: 0.357000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [2e-7, 0.75e-7,1.5e-7, 1.25e-7, 0.75e-7]\n",
    "regularization_strengths = [3e4, 3.25e4, 3.5e4, 3.75e4, 4e4,4.25e4, 4.5e4,4.75e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "for (learning_rate,regularization_strength) in zip(learning_rates,regularization_strengths):\n",
    "    softmax = Softmax()\n",
    "    loss_history=softmax.train(X_train, y_train, learning_rate=learning_rate, reg=regularization_strength, num_iters=1500, verbose=True)\n",
    "    y_train_pred=softmax.predict(X_train)\n",
    "    y_val_pred=softmax.predict(X_val)\n",
    "    train_accuracy=np.mean(y_train==y_train_pred)\n",
    "    val_accuracy=np.mean(y_val==y_val_pred)\n",
    "    results[(learning_rate,regularization_strength)]=(train_accuracy,val_accuracy)\n",
    "    if val_accuracy>best_val:\n",
    "        best_val=val_accuracy\n",
    "        best_softmax=softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f94a8f12",
   "metadata": {
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.352000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2112ee",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86d86120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24688f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
